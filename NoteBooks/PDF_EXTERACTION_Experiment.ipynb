{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc2f853f-d4d8-41f4-badf-3594cdd940c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6359c5d-bf25-4346-9bbf-e73b4e8dd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import base64\n",
    "import PyPDF2\n",
    "import requests\n",
    "import string\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e4bad9-24ee-4f4d-b743-2788da5dc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention( Q, K, V ) = softmax(QKT √dk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of1√dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by1√dk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and variance dk. 4output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO where head i= Attention( QWQ i, KWK i, V WV i) Where the projections are parameter matrices WQ i∈Rdmodel×dk,WK i∈Rdmodel×dk,WV i∈Rdmodel×dv andWO∈Rhdv×dmodel. In this work we employ h= 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: •In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. •The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. •Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN( x) = max(0 , xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel. 5\n",
      "11735\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import string\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the PDF\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "# Download the PDF content using requests\n",
    "response = requests.get(pdf_url)\n",
    "pdf_content = response.content\n",
    "\n",
    "# Create a PDF reader\n",
    "pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_content))\n",
    "\n",
    "# Extract text from pages 2 to 5 (index 1 to 4)\n",
    "start_page = 1  # Page index to start from\n",
    "end_page = min(start_page +4, len(pdf_reader.pages))  # End at page index 5 or last page, whichever comes first\n",
    "\n",
    "extracted_text = \"\"\n",
    "for page_number in range(start_page, end_page):\n",
    "    page = pdf_reader.pages[page_number]\n",
    "    extracted_text += page.extract_text()\n",
    "\n",
    "# Clean up the extracted text\n",
    "cleaned_text = \" \".join(extracted_text.split())  # Remove extra spaces\n",
    "\n",
    "# Print the cleaned extracted text\n",
    "print(cleaned_text)\n",
    "print(len(cleaned_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4eaa4a8-ad37-4d3a-84bd-9e6d982f95f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "#model and tokenizer loading\n",
    "checkpoint = \"LaMini-Flan-T5-248M\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint, device_map='auto', torch_dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73514d82-f6c0-4780-aba7-6b42215a6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_sum = pipeline(\n",
    "    'summarization',\n",
    "    model = base_model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 500, \n",
    "    min_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72ec3a1-4d02-4410-ac38-4f3bd410d798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2760 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Transformer is a model architecture based on stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The Transformer uses multi-head attention to draw global dependencies between input and output, allowing for more parallelization and reaching a new state of the art in translation quality. Multi-head Attention is used to generate a sequence of hidden states, reducing sequential computation and improving model performance. It is also used in conjunction with a recurrent network to reduce computational cost and improve model performance in various tasks. In addition, the Transformer employs h=8 parallel attention layers, where each layer produces outputs of dimension dmodel = 512, and the output is computed as a weighted sum 3Scaled Dot-Product Attention. This allows the model to jointly attend to information from different representation subspaces at different positions, resulting in large dot products.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "result = pipe_sum(cleaned_text)\n",
    "result = result[0]['summary_text']\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe287e-811f-4835-84fe-458d8095ac34",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5ae46-cb5d-4a77-9901-67ab9aed35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "# Download the PDF content using requests\n",
    "response = requests.get(pdf_url)\n",
    "pdf_content = response.content\n",
    "\n",
    "# Create a PDF reader\n",
    "pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_content))\n",
    "\n",
    "# Extract text from pages 2 to 5 (index 1 to 4)\n",
    "start_page = 1  # Page index to start from\n",
    "end_page = min(start_page +4, len(pdf_reader.pages))  # End at page index 5 or last page, whichever comes first\n",
    "\n",
    "extracted_text = \"\"\n",
    "for page_number in range(start_page, end_page):\n",
    "    page = pdf_reader.pages[page_number]\n",
    "    extracted_text += page.extract_text()\n",
    "\n",
    "# Clean up the extracted text\n",
    "cleaned_text = \" \".join(extracted_text.split())  # Remove extra spaces\n",
    "\n",
    "pipe_sum = pipeline('summarization',model = base_model,tokenizer = tokenizer,max_length = 1000, min_length = 50)\n",
    "result = pipe_sum(cleaned_text)\n",
    "result = result[0]['summary_text']\n",
    "print(result)\n",
    "print(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a5410-1e0b-4bad-9a75-5845e66513fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c15f0c96-e408-4dec-9892-38515e32035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_text\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and outputsequences. Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [ 21] and conditionalcomputation [ 32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions. In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an outputsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacksrespectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has twosub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each ofthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layeritself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack. Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3Scaled Dot-Product Attention\n",
      " Multi-Head AttentionMulti-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists ofqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on thevalues.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed togetherinto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention( Q, K, V ) = softmax(QKT\n",
      "√dk)V (1)Attention( Q, K, V ) = softmax(QKT\n",
      "√dk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factorof1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention ismuch faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperformsdot product attention without scaling for larger values of dk[3]. We suspect that for large values ofdk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=Pdki=1qiki, has mean 0and variance dk.\n",
      "4output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i, KWK\n",
      "i, V WV\n",
      "i)where head i= Attention( QWQ\n",
      "i, KWK\n",
      "i, V WV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we usedk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows everyposition in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].[38, 2, 9].\n",
      "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in theencoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.encoder.\n",
      "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parametersfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionalitydff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the inputtokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmaxlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\n",
      "5\n",
      "\n",
      "\\n\n",
      "\n",
      "result\n",
      "The Transformer is a neural network architecture that uses multi-head attention to generate outputs of different positions in the input sequence. It is the first transduction model that relies entirely on attention mechanisms to draw global dependencies between input and output. The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder layers, and a multi-head attention function to achieve more parallelization and improve model performance. Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions, reducing the computational cost and improving model performance in cases of sequential computation. The transformer uses learned embeddings and softmax functions to convert inputtokens and output tokens to vectors of dimension dmodel.\n",
      "<built-in function len>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Define the PDF URL\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "# Download the PDF content using requests\n",
    "response = requests.get(pdf_url)\n",
    "pdf_content = response.content\n",
    "\n",
    "# Create a PDF reader\n",
    "pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_content))\n",
    "\n",
    "# Extract text from pages 2 to 5 (index 1 to 4)\n",
    "start_page = 1  # Page index to start from\n",
    "end_page = min(start_page + 4, len(pdf_reader.pages))  # End at page index 5 or last page, whichever comes first\n",
    "\n",
    "extracted_text = \"\"\n",
    "for page_number in range(start_page, end_page):\n",
    "    page = pdf_reader.pages[page_number]\n",
    "    extracted_text += page.extract_text()\n",
    "\n",
    "# Split the extracted text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "text_chunks = text_splitter.split_text(extracted_text)\n",
    "\n",
    "# Combine the chunks into final text\n",
    "final_text = \"\".join(text_chunks)\n",
    "\n",
    "# Print or return the final text\n",
    "print(\"final_text\")\n",
    "print(final_text)\n",
    "print(\"\\n\\\\n\\n\")\n",
    "pipe_sum = pipeline('summarization',model = base_model,tokenizer = tokenizer,max_length = 1000, min_length = 50)\n",
    "result = pipe_sum(final_text)\n",
    "result = result[0]['summary_text']\n",
    "print(\"result\")\n",
    "print(result)\n",
    "print(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d77868-1b10-41fd-ab13-cbd99b8e0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878ccaa-8a3f-4671-961d-a6e8e63ab39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "260dc8b7-5d9e-44e5-91bb-4dec08137040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_text\n",
      "1 Introduction\n",
      "Recurrent neural networks long shortterm memory  13 and gated recurrent  7 neural networks\n",
      "in particular have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation  3525 Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoderdecoder\n",
      "architectures 38 24 15architectures 38 24 15\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences Aligning the positions to steps in computation time they generate a sequence of hidden\n",
      "states ht as a function of the previous hidden state ht1and the input for position t This inherently\n",
      "sequential nature precludes parallelization within training examples which becomes critical at longersequence lengths as memory constraints limit batching across examples Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks  21 and conditional\n",
      "computation  32 while also improving model performance in case of the latter The fundamental\n",
      "constraint of sequential computation however remains\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences  219 In all but a few cases  27 however such attention mechanisms\n",
      "are used in conjunction with a recurrent network\n",
      "In this work we propose the Transformer a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and outputThe Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "16 ByteNet  18 and ConvS2S  9 all of which use convolutional neural networks as basic building\n",
      "block computing hidden representations in parallel for all input and output positions In these modelsthe number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions linearly for ConvS2S and logarithmically for ByteNet This makes\n",
      "it more difficult to learn dependencies between distant positions  12 In the Transformer this is\n",
      "reduced to a constant number of operations albeit at the cost of reduced effective resolution due\n",
      "to averaging attentionweighted positions an effect we counteract with MultiHead Attention asdescribed in section 32\n",
      "Selfattention sometimes called intraattention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence Selfattention has been\n",
      "used successfully in a variety of tasks including reading comprehension abstractive summarization\n",
      "textual entailment and learning taskindependent sentence representations 4 27 28 22Endtoend memory networks are based on a recurrent attention mechanism instead of sequence\n",
      "aligned recurrence and have been shown to perform well on simplelanguage question answering and\n",
      "language modeling tasks 34\n",
      "To the best of our knowledge however the Transformer is the first transduction model relying\n",
      "entirely on selfattention to compute representations of its input and output without using sequencealigned RNNs or convolution In the following sections we will describe the Transformer motivate\n",
      "selfattention and discuss its advantages over models such as 17 18 and 9\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoderdecoder structure  5235\n",
      "Here the encoder maps an input sequence of symbol representations x1  x nto a sequence\n",
      "of continuous representations z z1  z n Given z the decoder then generates an outputsequence y1  y mof symbols one element at a time At each step the model is autoregressive\n",
      "10 consuming the previously generated symbols as additional input when generating the next\n",
      "2Figure 1 The Transformer  model architecture\n",
      "The Transformer follows this overall architecture using stacked selfattention and pointwise fully\n",
      "connected layers for both the encoder and decoder shown in the left and right halves of Figure 1\n",
      "respectively\n",
      "31 Encoder and Decoder Stacksrespectively\n",
      "31 Encoder and Decoder Stacks\n",
      "Encoder The encoder is composed of a stack of N 6 identical layers Each layer has two\n",
      "sublayers The first is a multihead selfattention mechanism and the second is a simple position\n",
      "wise fully connected feedforward network We employ a residual connection  11 around each of\n",
      "the two sublayers followed by layer normalization  1 That is the output of each sublayer isLayerNorm x Sublayer x where Sublayer xis the function implemented by the sublayer\n",
      "itself To facilitate these residual connections all sublayers in the model as well as the embedding\n",
      "layers produce outputs of dimension dmodel  512 \n",
      "Decoder The decoder is also composed of a stack of N 6identical layers In addition to the two\n",
      "sublayers in each encoder layer the decoder inserts a third sublayer which performs multiheadattention over the output of the encoder stack Similar to the encoder we employ residual connections\n",
      "around each of the sublayers followed by layer normalization We also modify the selfattention\n",
      "sublayer in the decoder stack to prevent positions from attending to subsequent positions This\n",
      "masking combined with fact that the output embeddings are offset by one position ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i\n",
      "32 Attention32 Attention\n",
      "An attention function can be described as mapping a query and a set of keyvalue pairs to an output\n",
      "where the query keys values and output are all vectors The output is computed as a weighted sum\n",
      "3Scaled DotProduct Attention\n",
      " MultiHead Attention\n",
      "Figure 2 left Scaled DotProduct Attention right MultiHead Attention consists of several\n",
      "attention layers running in parallel\n",
      "of the values where the weight assigned to each value is computed by a compatibility function of thequery with the corresponding key\n",
      "321 Scaled DotProduct Attention\n",
      "We call our particular attention Scaled DotProduct Attention Figure 2 The input consists of\n",
      "queries and keys of dimension dk and values of dimension dv We compute the dot products of the\n",
      "query with all keys divide each bydk and apply a softmax function to obtain the weights on the\n",
      "values\n",
      "In practice we compute the attention function on a set of queries simultaneously packed togetherinto a matrix Q The keys and values are also packed together into matrices KandV We compute\n",
      "the matrix of outputs as\n",
      "Attention Q K V   softmaxQKT\n",
      "dkV 1\n",
      "The two most commonly used attention functions are additive attention  2 and dotproduct multi\n",
      "plicative attention Dotproduct attention is identical to our algorithm except for the scaling factor\n",
      "of1dk Additive attention computes the compatibility function using a feedforward network witha single hidden layer While the two are similar in theoretical complexity dotproduct attention is\n",
      "much faster and more spaceefficient in practice since it can be implemented using highly optimized\n",
      "matrix multiplication code\n",
      "While for small values of dkthe two mechanisms perform similarly additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk3 We suspect that for large values ofdk the dot products grow large in magnitude pushing the softmax function into regions where it has\n",
      "extremely small gradients4 To counteract this effect we scale the dot products by1dk\n",
      "322 MultiHead Attention\n",
      "Instead of performing a single attention function with dmodeldimensional keys values and queries\n",
      "we found it beneficial to linearly project the queries keys and values htimes with different learnedlinear projections to dkdkanddvdimensions respectively On each of these projected versions of\n",
      "queries keys and values we then perform the attention function in parallel yielding dvdimensional\n",
      "4To illustrate why the dot products get large assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1 Then their dot product qkPdk\n",
      "i1qiki has mean 0and variance dk\n",
      "4output values These are concatenated and once again projected resulting in the final values asdepicted in Figure 2\n",
      "Multihead attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions With a single attention head averaging inhibits this\n",
      "MultiHead Q K V   Concathead 1 head hWO\n",
      "where head i Attention QWQ\n",
      "i KWK\n",
      "i V WV\n",
      "i\n",
      "Where the projections are parameter matrices WQ\n",
      "iRdmodeldkWK\n",
      "iRdmodeldkWV\n",
      "iRdmodeldv\n",
      "andWORhdvdmodeliRdmodeldkWV\n",
      "iRdmodeldv\n",
      "andWORhdvdmodel\n",
      "In this work we employ h 8 parallel attention layers or heads For each of these we use\n",
      "dkdvdmodelh 64  Due to the reduced dimension of each head the total computational cost\n",
      "is similar to that of singlehead attention with full dimensionality\n",
      "323 Applications of Attention in our Model\n",
      "The Transformer uses multihead attention in three different ways\n",
      "In encoderdecoder attention layers the queries come from the previous decoder layerand the memory keys and values come from the output of the encoder This allows every\n",
      "position in the decoder to attend over all positions in the input sequence This mimics the\n",
      "typical encoderdecoder attention mechanisms in sequencetosequence models such as\n",
      "38 2 9\n",
      "The encoder contains selfattention layers In a selfattention layer all of the keys values\n",
      "and queries come from the same place in this case the output of the previous layer in theencoder Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder\n",
      "Similarly selfattention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position We need to prevent leftward\n",
      "information flow in the decoder to preserve the autoregressive property We implement this\n",
      "inside of scaled dotproduct attention by masking out setting to  all values in the inputof the softmax which correspond to illegal connections See Figure 2\n",
      "33 Positionwise FeedForward Networks\n",
      "In addition to attention sublayers each of the layers in our encoder and decoder contains a fully\n",
      "connected feedforward network which is applied to each position separately and identically This\n",
      "consists of two linear transformations with a ReLU activation in between\n",
      "FFN x  max0  xW 1b1W2b2 2FFN x  max0  xW 1b1W2b2 2\n",
      "While the linear transformations are the same across different positions they use different parameters\n",
      "from layer to layer Another way of describing this is as two convolutions with kernel size 1\n",
      "The dimensionality of input and output is dmodel  512  and the innerlayer has dimensionality\n",
      "dff 2048 \n",
      "34 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models we use learned embeddings to convert the inputtokens and output tokens to vectors of dimension dmodel We also use the usual learned linear transfor\n",
      "mation and softmax function to convert the decoder output to predicted nexttoken probabilities In\n",
      "our model we share the same weight matrix between the two embedding layers and the presoftmax\n",
      "linear transformation similar to  30 In the embedding layers we multiply those weights bydmodel\n",
      "5\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 49.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Define the PDF URL\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "# Download the PDF content using requests\n",
    "response = requests.get(pdf_url)\n",
    "pdf_content = response.content\n",
    "\n",
    "# Create a PDF reader\n",
    "pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_content))\n",
    "\n",
    "# Extract text from pages 2 to 5 (index 1 to 4)\n",
    "start_page = 1  # Page index to start from\n",
    "end_page = min(start_page + 4, len(pdf_reader.pages))  # End at page index 5 or last page, whichever comes first\n",
    "\n",
    "extracted_text = \"\"\n",
    "for page_number in range(start_page, end_page):\n",
    "    page = pdf_reader.pages[page_number]\n",
    "    extracted_text += page.extract_text()\n",
    "\n",
    "# Split the extracted text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "text_chunks = text_splitter.split_text(extracted_text)\n",
    "\n",
    "# Combine the chunks into final text\n",
    "final_text = \"\".join(text_chunks)\n",
    "final_text = re.sub(r'[^a-zA-Z0-9\\s]', '', final_text)\n",
    "final_text   = re.sub(r'\\S*@\\S*\\s?', '', final_text)\n",
    "final_text= final_text.rstrip()\n",
    "# Print or return the final text\n",
    "print(\"final_text\")\n",
    "print(final_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2165f07b-98cf-415a-b716-d6b68df531c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "The Transformer is a neural network architecture that uses stacked selfattention and pointwise fully connected layers for both the encoder and decoder layers. It is the first transduction model relying entirely on attention mechanisms to draw global dependencies between input and output. The Transformer uses multihead attention and multi-head attention to reduce sequential computation and improve model performance. Multihead attention is used to generate a sequence of hidden states and outputs. The output is computed as a weighted sum 3Scaled DotProduct Attention and MultiHead Attention. The focus is on reducing the number of operations required to relate signals from two arbitrary input or output positions. The attention function can be described as mapping a query and a set of keyvalue pairs to an output where the query keys and values are all vectors. This allows the model to jointly attend to information from different representation subspaces at different positions. To counteract this effect, we scale the dot products by1dk 322 Multihead Attention. This involves dividing the input by the weight assigned to each value and applying a softmax function to obtain the weights on the values. This prevents leftward information flow and preserves the autoregressive property. We implement this inside of scaled dotproduct attention by masking out setting to all values in the input of the softmax which corresponds to illegal connections. We share the same weight matrix between the two embedding layers and the presoftmax linear transformation similar to 30. In the embeddding layers, we multiply those weights bydmodel 5.\n",
      "<built-in function len>\n",
      "CPU times: total: 19min 24s\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "pipe_sum = pipeline('summarization',model = base_model,tokenizer = tokenizer,max_length = 512, min_length = 50)\n",
    "result = pipe_sum(final_text)\n",
    "result = result[0]['summary_text']\n",
    "print(\"result\")\n",
    "print(result)\n",
    "print(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794af10-dd5e-44ee-88a0-ce6ebe0a77cc",
   "metadata": {},
   "source": [
    "'The Transformer is a model architecture based on stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The Transformer uses multi-head attention to draw global dependencies between input and output, allowing for more parallelization and reaching a new state of the art in translation quality. Multi-head Attention is used to generate a sequence of hidden states, reducing sequential computation and improving model performance. It is also used in conjunction with a recurrent network to reduce computational cost and improve model performance in various tasks. In addition, the Transformer employs h=8 parallel attention layers, where each layer produces outputs of dimension dmodel = 512, and the output is computed as a weighted sum 3Scaled Dot-Product Attention. This allows the model to jointly attend to information from different representation subspaces at different positions, resulting in large dot products.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9f025-9ff9-44a4-985c-692e1d2a1a4f",
   "metadata": {},
   "source": [
    "The Transformer is a neural network architecture that uses multi-head attention to generate outputs of different positions in the input sequence. It is the first transduction model that relies entirely on attention mechanisms to draw global dependencies between input and output. The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder layers, and a multi-head attention function to achieve more parallelization and improve model performance. Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions, reducing the computational cost and improving model performance in cases of sequential computation. The transformer uses learned embeddings and softmax functions to convert inputtokens and output tokens to vectors of dimension dmodel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d459e-5da9-489d-a390-d56efb10b505",
   "metadata": {},
   "source": [
    "The Transformer is a neural network architecture that uses stacked selfattention and pointwise fully connected layers for both the encoder and decoder layers. It is the first transduction model relying entirely on attention mechanisms to draw global dependencies between input and output. The Transformer uses multihead attention and multi-head attention to reduce sequential computation and improve model performance. Multihead attention is used to generate a sequence of hidden states and outputs. The output is computed as a weighted sum 3Scaled DotProduct Attention and MultiHead Attention. The focus is on reducing the number of operations required to relate signals from two arbitrary input or output positions. The attention function can be described as mapping a query and a set of keyvalue pairs to an output where the query keys and values are all vectors. This allows the model to jointly attend to information from different representation subspaces at different positions. To counteract this effect, we scale the dot products by1dk 322 Multihead Attention. This involves dividing the input by the weight assigned to each value and applying a softmax function to obtain the weights on the values. This prevents leftward information flow and preserves the autoregressive property. We implement this inside of scaled dotproduct attention by masking out setting to all values in the input of the softmax which corresponds to illegal connections. We share the same weight matrix between the two embedding layers and the presoftmax linear transformation similar to 30. In the embeddding layers, we multiply those weights bydmodel 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
