{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XIlWkPd8ego",
    "outputId": "723ab83c-fa60-4f19-d9d7-4251d05329b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 13 13:17:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   53C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUpxxPXSBwYa",
    "outputId": "885e4b80-dea2-4372-a062-c5ffe40bf230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nOoODRZBwbK",
    "outputId": "a64faf4e-b253-4930-f5de-78d152454853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.21.0\n",
      "Found existing installation: transformers 4.31.0\n",
      "Uninstalling transformers-4.31.0:\n",
      "  Successfully uninstalled transformers-4.31.0\n",
      "Found existing installation: accelerate 0.21.0\n",
      "Uninstalling accelerate-0.21.0:\n",
      "  Successfully uninstalled accelerate-0.21.0\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: transformers, accelerate\n",
      "Successfully installed accelerate-0.21.0 transformers-4.31.0\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade accelerate\n",
    "%pip uninstall -y transformers accelerate\n",
    "%pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FO-gd4MABwd6",
    "outputId": "597d3a65-0590-43ae-e0d6-691ed014f233"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "fxMJFM0hBwgq",
    "outputId": "9ffc67b1-a8e7-4545-9f6c-4fe774a07f9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLk92MtpBwjL",
    "outputId": "8b77555a-ea15-4240-a7f9-47a2565ad1fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylhAZN-mBwl0",
    "outputId": "c94a069e-48c6-4677-9e8c-639e1456c53c"
   },
   "outputs": [],
   "source": [
    "#dowload & unzip data\n",
    "\n",
    "# !wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
    "# !unzip summarizer-data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3C1FnPKEtfZ"
   },
   "source": [
    "# Arxiv dataset for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWhmIaDdEe70",
    "outputId": "600e6469-26af-4bb8-b0d6-3142a27fe41d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = Path(\"C:/Users/soulo/MACHINE_LEARNING/PaperMate/data/sumsum_dataset\")\n",
    "file_contents = []\n",
    "\n",
    "for file_path in dataset_path.glob(\"**/*.txt\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        file_contents.append(content)\n",
    "\n",
    "# Now file_contents contains the content of each text file in your dataset\n",
    "file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01qulRiiNEkX",
    "outputId": "80a6f9d3-470c-45f9-9242-d9cc65dbee35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Eric: MACHINE!\r\n",
      "Rob: That's so gr8!\r\n",
      "Eric: I know! And shows how Americans see Russian ;)\r\n",
      "Rob: And it's really funny!\r\n",
      "Eric: I know! I especially like the train part!\r\n",
      "Rob: Hahaha! No one talks to the machine like that!\r\n",
      "Eric: Is this his only stand-up?\r\n",
      "Rob: Idk. I'll check.\r\n",
      "Eric: Sure.\r\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\r\n",
      "Eric: Gr8! I'll watch them now!\r\n",
      "Rob: Me too!\r\n",
      "Eric: MACHINE!\r\n",
      "Rob: MACHINE!\r\n",
      "Eric: TTYL?\r\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HgV3tby1Bwol"
   },
   "outputs": [],
   "source": [
    "# dataset_samsum = load_from_disk('samsum_dataset')\n",
    "# dataset_samsum = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmexcE-sL-48",
    "outputId": "36592848-8745-4404-f093-e18838c2b3c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 203037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BF9y7AG9BwrT"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "de297ffe3a784fed9d29135393ceed65",
      "91aa5ad0699a435e815b9a522722fdab",
      "46cfab302bff434ab00f0129ebf6bd9a",
      "8d1f58fad61748729bb19e31f165d3b1",
      "eea4e9901cc14628940789a21931abcb",
      "26f780177576466692fa9adb33315d85",
      "3dbd980a30b34221b4edcf1ba336910c",
      "f7a1b419379846efa61e63aa085cff9b",
      "a0653a62c15842fa927db9b03eaefdd1",
      "37237174db9746ce936837e3e0497167",
      "7cef02372fad494ab9d983d57a748540"
     ]
    },
    "id": "IebCdCcHBww9",
    "outputId": "bdfa4d2f-8978-4550-d62b-0045c2225cba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de297ffe3a784fed9d29135393ceed65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wKQJ2nGBwzd",
    "outputId": "35e2f837-abfa-4ef3-bcdd-f4cc136daad3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyjSsaVMBw12"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrhTe1PjCeXJ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JjmM6X3CBw41"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XLK_zxalBw7t"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S9mQGx3gBw-N"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "t8-GH2TgBxAm",
    "outputId": "ac45c454-827e-4fcc-9e8c-38ba27e93bc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 47:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.629700</td>\n",
       "      <td>1.485818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=920, training_loss=1.819335088522538, metrics={'train_runtime': 2835.9935, 'train_samples_per_second': 5.195, 'train_steps_per_second': 0.324, 'total_flos': 5526698901602304.0, 'train_loss': 1.819335088522538, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85hFCEDICjS3"
   },
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SzMSJExEBxDG"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "5cdee988303d4c51943d5b5d2ecd9b34",
      "266cde849a2147c2b546c41b5bdc32e1",
      "2305bbf6353d400db7db64d522359ecc",
      "841fa1c9ffa548baabc6cf8d795380d4",
      "8444df609b094a3eab311992b679ecd7",
      "ba620322118c4583bfc296331ad66e5b",
      "47d33f4f0f2841d0892fc3dab185e7d1",
      "c2eab49022484b4a983bd35e7767a30f",
      "6391971bfa8a43c28e51af0a5c65bb17",
      "15271a050f064de48a950096b214dea6",
      "7dd8482301664be296dfdeecd0d5f417"
     ]
    },
    "id": "z1-VQMPvCLOZ",
    "outputId": "a9c00f78-26e4-49cb-98ed-6eb1c0d81fca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-5a43aadd1b0e>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric('rouge')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdee988303d4c51943d5b5d2ecd9b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "p1NjdJWQCLRb",
    "outputId": "b4af5e4b-291a-40c7-ba1e-e1420fe6c902"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "  <div id=\"df-f5a12560-260d-4d35-9fa9-7101cb6fe510\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.024009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024019</td>\n",
       "      <td>0.023973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5a12560-260d-4d35-9fa9-7101cb6fe510')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "\n",
       "\n",
       "\n",
       "    <div id=\"df-707ac36e-d101-475f-aaf5-37e4771cefa7\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-707ac36e-d101-475f-aaf5-37e4771cefa7')\"\n",
       "              title=\"Suggest charts.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "    </div>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "    <script>\n",
       "      async function quickchart(key) {\n",
       "        const containerElement = document.querySelector('#' + key);\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      }\n",
       "    </script>\n",
       "\n",
       "      <script>\n",
       "\n",
       "function displayQuickchartButton(domScope) {\n",
       "  let quickchartButtonEl =\n",
       "    domScope.querySelector('#df-707ac36e-d101-475f-aaf5-37e4771cefa7 button.colab-df-quickchart');\n",
       "  quickchartButtonEl.style.display =\n",
       "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "}\n",
       "\n",
       "        displayQuickchartButton(document);\n",
       "      </script>\n",
       "      <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f5a12560-260d-4d35-9fa9-7101cb6fe510 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f5a12560-260d-4d35-9fa9-7101cb6fe510');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           rouge1  rouge2    rougeL  rougeLsum\n",
       "pegasus  0.024009     0.0  0.024019   0.023973"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    ")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Vuwiy2PCmTr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfYlQAaOCoie"
   },
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WeKKVcCtCLUB"
   },
   "outputs": [],
   "source": [
    "## Save model\n",
    "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1KcTNKVCLWq",
    "outputId": "78f6eb2d-87fa-4150-acf1-327376425c57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/spiece.model',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3vQ9h_mCLZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJarIGwCCqkx"
   },
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ssTVsSl8CLbu"
   },
   "outputs": [],
   "source": [
    "#Load\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")\n",
    "model ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTW0gNEBCLgn",
    "outputId": "25f4575c-4bc5-4208-fdad-0f681ec58b3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Reference Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "Model Summary:\n",
      "Amanda can't find Betty's number. Larry called Betty last time they were at the park together. Hannah wants Amanda to text Larry. Amanda will text Larry.\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "\n",
    "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
    "\n",
    "\n",
    "\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
    "\n",
    "##\n",
    "print(\"Dialogue:\")\n",
    "print(sample_text)\n",
    "\n",
    "\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FxCgWqiDCLjN"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnhydviFcJKA",
    "outputId": "4c3b783f-cf7f-4194-a04c-187b7c891aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Summary: Recommender systems are widely used these days in e-commerce, for the purpose of personalized recommendation. Amazon.com is using recommender systems for books. Personalized recommendation can be applied to outside of commercial applications. We suggest developing the scholarly paper recommendation system for academic researchers. It will automatically detect their research topics they are interested in and recommend the related articles they may be interested in based on similarity of works. The most effective memory-based algorithms are item-based CF, matrix factorization, content-based methods and hybrid approach. Koren suggests combining rating information and user, item profiles for more accurate recommendation. Matsatsinis introduced scientific paper recommendation using decision theory. Sugiyama extended scholarly paper recommendation with citation and reference information. It is difficult to evaluate recommender systems due to lack of standard methods. We combined title, key words, and abstract to construct a set of words representing a paper. First, we removed stop words such as ”the” or ”of”. This process reduces the length of dictionary, resulting in reduced dimension of the clustering work. In English, same word can be used as different parts, usually in a slightly different form. It is better to deal with minor changes of forms as same words, as it can dramatically reduce the dimension. As a first step, we removed last ”ed”, ”ly”, and ”ing” from the word, whenever encountered. We are ready to proceed to our main goal: personalized recommendation of academic papers. In this paper, we discuss fundamental characteristics of our problem. We build a user-item matrix with each paper’s title, list of authors, key words, and abstract. We use 1-5 scale as it is widely used in recommendation systems in literature. According to Netflix Prize data, only 1% of cells of the user-item matrix are observed 3 values. It is possible to estimate missing data only using small amount of observed data. In our situation, one author writes only one or two papers in one conference proceeding. There are only at most two or three top-level conferences in each field, the maximum number of papers one author can publish a year is about 10. lem makes it difficult to use widely-used collaborative filtering algorithms. 3.3.2 Naive Recommender uses k-Nearest Neighbors (kNN) algorithm to select and recommend most similar n papers to the target user’s previous paper. Papers are assigned to only one of the most similar papers written by the target user. After assigned to a cluster, the score is calculated based on the distance between the candidate paper and its centroid. Using the calculated score as a distance metric for kNN, we select k papers for recommendation to the target user. It will be important to include the information about relevant researchers to users and recommend papers that they found interesting or they have written. For the perspective of machine learning, we may need to consider about scalability. TF-IDF model [10] can be a great candidate to implement. In this paper, we have presented a Personalized Academic Research Paper Recommendation System. We have developed a web crawler to retrieve a huge number of research papers from the web. Second, we define a similarity measure for research papers. Third, we have developed our recommender system using collaboration filtering methods. \n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "Recommender systems are widely used these days in e-commerce, for the purpose of personalized\n",
    "recommendation. Based on each user’s profile, previous purchase history, and online behavior, they\n",
    "suggest products which they are likely to prefer. For example, Amazon.com is using recommender\n",
    "systems for books. When a user logs-in to the system, it suggests books similar to previously bought\n",
    "ones by the user.\n",
    "Personalized recommendation can be applied to outside of commercial applications. These days,\n",
    "many academic papers are coming out from a lot of conferences and journals. Academic researchers\n",
    "should go through all the conferences and journals which are related to their field of research and\n",
    "find out if there is any new articles that may relate to their current works. Sometimes they search\n",
    "the articles from Google scholars or Citeseer with the key words that might show interesting articles\n",
    "to them. However, these two methods require users to commit their time to search articles, which is\n",
    "labor-intensive, and also do not guarantee that they will find the exact articles related to their field\n",
    "of research.\n",
    "In order to reduce their workload, we suggest developing the scholarly paper recommendation system for academic researchers, which will automatically detect their research topics they are interested in and recommend the related articles they may be interested in based on similarity of the\n",
    "works. We believe this system will save the researchers’ time to search the articles and increase the\n",
    "accuracy of finding the articles they are interested in.\n",
    "In this section we briefly present some of the research literature related to recommender systems in\n",
    "general, academic paper recommendation system, and evaluation of recommender systems.\n",
    "Recommender systems are broadly classified into three categories[7]: collaborative filtering,\n",
    "contents-based methods, and hybrid methods. First, collaborative filtering uses only user-item rating\n",
    "matrix for predicting unseen preference[21, 1]. It can be categorized into memory-based CF, which\n",
    "contains the whole matrix on memory, and model-based CF, building a model for estimation[2].\n",
    "The most effective memory-based algorithms known so far is item-based CF[19]. Recently, making use of matrix factorization, a kind of model-based approach[14, 16, 18, 3, 24], is known as\n",
    "the most efficient and accurate, especially after those approaches won the Netflix prize in 2009.\n",
    "Content-based methods, on the other hand, recommend items based on their characteristics as well\n",
    "as specific preferences of a user[7]. Pazzani[15] studied this approach in depth, including how to\n",
    "build user and item profiles. Last category, hybrid approach, tries to combine both collaborative and\n",
    "content-based recommendation. Koren[8] suggested effectively combining rating information and\n",
    "user, item profiles for more accurate recommendation.\n",
    "Recommender systems have concentrated on recommending media items such as movies, but recently they are extending to academy. Most popular application is citation recommendation[5, 12,\n",
    "23, 20]. Recently, Matsatsinis [11] introduced scientific paper recommendation using decision theory. Sugiyama[22] extended scholarly paper recommendation with citation and reference information.\n",
    "Although recommender systems are very popular in commercial applications these days, it is still\n",
    "difficult to evaluate them due to the lack of standard methods. Traditional recommender systems [4, 17, 6] were usually introduced in Human-Computer Interaction community, so they have\n",
    "been evaluated by user study. This approach is still used, especially for verifying improvement in\n",
    "terms of user experience.\n",
    "3.2 Data Model\n",
    "3.2.1 Bag-of-word model\n",
    "With the gathered data, we modeled them by a bag-of-word model. In this model, each word appeared in the whole document corpora becomes an attribute. Then, each document is represented by\n",
    "a bit vector, indicating whether each word appears or not. This model is based on two assumptions;\n",
    "1) word probabilities for one text position are independent of the words that occur in other positions\n",
    "(Naive Bayes Assumption) and 2) the probability of encountering a specific word is independent of\n",
    "its position. (Independent Identical Distribution Assumption) This assumption is incorrect, but it is\n",
    "known that this does not seriously affect classification or learning task. [13] We combined title, key\n",
    "words, and abstract to construct a set of words representing a paper.\n",
    "3.2.2 Heuristics\n",
    "For more efficient processing, we applied some heuristics. First, we removed stop words such as\n",
    "”the” or ”of”. These words appear in almost every document in English, so they are not useful for\n",
    "classifying or filtering some specific documents, but just slow down computation speed by increasing\n",
    "the text length. We removed about 140 words which were selected manually. This process reduces\n",
    "the length of dictionary, resulting in reduced dimension of the clustering work, so we expect speed\n",
    "improvement.\n",
    "The other heuristic applied is stemming. In English, same word can be used as different parts, usually in a slightly different form. For example, ”clear”, ”clearly”, and ”cleared” have same meaning,\n",
    "but used in different forms for its position or role in the sentence. It is much better to deal with these\n",
    "minor changes of forms as same words, as it can dramatically reduce the dimension. However, this\n",
    "work is not straightforward. As a first step, we just removed last ”ed”, ”ly”, and ”ing” from the\n",
    "word, whenever encountered.\n",
    "3.3 Learner (Recommender)\n",
    "Using the crawled documents and data model discussed so far, we are ready to proceed to our main\n",
    "goal: personalized recommendation of academic papers. As a perspective of recommendation system, we can consider authors as users and papers as items. We will use these terms interchangeably\n",
    "henceforth. We can think of recommendation system as a task to fill out missing preference data on\n",
    "a user-item matrix, based on observed values. There can be lots of schemes to decide proper values\n",
    "for missing preference. Filling with the user’s average or item’s average can be a simple baseline. In\n",
    "this section, we discuss fundamental characteristics of our problem, and then describe our algorithm.\n",
    "3.3.1 Inherent Characteristics of Problem\n",
    "The information we gather contains each paper’s title, list of authors, key words, and abstract. In\n",
    "order to build a user-item matrix with this data, we assume that users are interested in their own\n",
    "papers. Thus, we set high score (in this paper, 5) to every <author, paper> pair that the paper is\n",
    "written by the author. We use 1-5 scale as it is widely used in recommendation systems in literature.\n",
    "We claim that this user-item matrix we use is extremely sparse, which means most of values are\n",
    "missing while only small portion of them are observed. This situation is common in recommendation, though. According to Netflix Prize data, only 1% of cells of the user-item matrix are observed\n",
    "3\n",
    "values. Nonetheless, it has been shown that it is possible to accurately estimate missing data only\n",
    "using small amount of observed data. In our situation, however, the sparsity can be worse. Regularly, one author writes only one or two papers in one conference proceeding. There are only at\n",
    "most two or three top-level conferences in each field, the maximum number of papers one author\n",
    "can publish a year is about 10. This is an ideal case, and most researchers may have only one or two\n",
    "papers. Thus, our matrix have only a few number of preference data.\n",
    "More serious problem is that we do not have ”dislike” information. When we request users to\n",
    "explicitly rate items in a common recommendation system, we can get both positive and negative\n",
    "feedback from the user. For example, we can get ”very like” feedback for the movie ”Titanic” as\n",
    "well as ”very hate” one for the ”Shrek 2.” Based on this variety, we can infer that the user may\n",
    "prefer romantic movies to animations. In our data, however, we do not have negative feedback. This\n",
    "problem makes difficult us to use widely-used collaborative filtering algorithms.\n",
    "3.3.2 Naive Recommender\n",
    "We basically assume that authors will like papers similar to ones they wrote before. In this context,\n",
    "we note that similar papers mean ones dealing with similar topic. In our Naive Recommender,\n",
    "we just apply this assumption. When we try to recommend a set of papers to a specific user, we\n",
    "first calculate similarity between every paper and the user’s own papers. Then, we take the highest\n",
    "similarity as the score of that paper. This process is similar to k-Nearest Neighbors (kNN) algorithm.\n",
    "That is, we can easily select and recommend most similar n papers to the target user’s previous paper.\n",
    "We used vector cosine of our data model (bit vector) as the similarity measure.\n",
    "However, the real situation is a little bit more complicated, as the user may have written more than\n",
    "one paper. It is still kNN, but we can have more than one queried point. Thus, we applied clustering\n",
    "first. All candidate papers are assigned to only one of the most similar paper written by the target\n",
    "user. This process is similar to K-means, but the centroids are also papers, so their geometric location\n",
    "in the space cannot change. Thus, we do not need to iterate in our case. After assigned to a cluster,\n",
    "the score is calculated based on the distance between the candidate paper and its centroid. For\n",
    "example, as shown in Figure 2, each big circle represents a centroid of a cluster and small circles\n",
    "connected to the centroid are members of its cluster. Using the calculated score as a distance metric\n",
    "for kNN, we select k papers for recommendation to the target user.\n",
    "interested in more accurately. Also, through the focus group interview we discovered the interesting\n",
    "fact that even though the topics are not as much as relevant to their research topic, they showed great\n",
    "interest to the papers that their peer researchers, i.e., their former students or the researchers they\n",
    "have done research together before, wrote. In this way, it will be important to include the information about relevant researchers to users and recommend papers that they found interesting or they\n",
    "have wrote. Also, the subjects replied, if we provide information about which researcher liked this\n",
    "papers, it would also give them great reason and motivation to read that paper.\n",
    "For the perspective of machine learning, we may need to consider about scalability. Although our\n",
    "current system runs within a few minutes, it may take more time when we crawl more data. First, we\n",
    "can improve accuracy of similarity measure by allowing counting the frequency of each word in a\n",
    "document, instead of bit vector model. TF-IDF model [10] can be a great candidate to implement. In\n",
    "this model, we give more weight for frequently used words in a specific document, but not in other\n",
    "ones. Also, we may need to speed up the calculation. For this, dimension reduction will be helpful.\n",
    "Specifically, it would be better to add more stemming logic because this can deal with more words\n",
    "as same ones, so we can successfully reduce dimension. We may use L-Distance algorithm [9] for\n",
    "calculating similarity of each word pair, and decide whether they are same or not.\n",
    "6 Conclusion\n",
    "In this paper, we have presented a Personalized Academic Research Paper Recommendation System, which recommends related articles for each researcher. Thanks to our system, researchers can\n",
    "get their related papers without searching keywords on Google or browsing top conferences’ proceedings. Our system makes three contributions. First, we have developed a web crawler to retrieve\n",
    "a huge number of research papers from the web. Second, we define a similarity measure for research\n",
    "papers. Third, we have developed our recommender system using collaboration filtering methods.\n",
    "Evaluation results show the usefulness of our system.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split the input text into chunks of a manageable length\n",
    "chunk_size = 1000  # Adjust as needed\n",
    "chunks = [input_text[i:i+chunk_size] for i in range(0, len(input_text), chunk_size)]\n",
    "\n",
    "# Generate summaries for each chunk and combine them\n",
    "combined_summary = \"\"\n",
    "for chunk in chunks:\n",
    "    summary = pipe(chunk, max_length=150, min_length=40, do_sample=False)\n",
    "    combined_summary += summary[0]['summary_text'] + \" \"\n",
    "\n",
    "print(\"Combined Summary:\", combined_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeg8NvnJfaLQ"
   },
   "source": [
    "Combined Summary:  recommender systems are widely used these days in e-commerce, for the purpose of personalized recommendations. Based on each user’s profile, previous purchase history, and online behavior, they suggest products they are likely to prefer. In order to reduce their workload, we suggest developing the scholarly paper recommendation system for academic researchers. We believe this system will save the researchers’ time to search the articles and increase the accuracy of finding the articles they are interested in. We briefly present some of the research literature related to recommender systems. The most effective memory-based algorithms known so far is item-based CF[19]. Recently, making use of matrix factorization, a kind of model-based approach, is known as the most efficient and accurate. hybrid approach, tries to combine both collaborative and content-based recommendation.  recommender systems are very popular in commercial applications. It is still difficult to evaluate them due to the lack of standard methods. With the gathered data, we modeled them by a bag-of-word model. In this model, each word appeared in the whole document corpora becomes an attribute. We combined title, keywords, and abstract to construct a set of words representing a paper. We removed stop words such as ‘the’ or ‘of’ These words appear in almost every document in English. This process reduces the length of dictionary, resulting in reduced dimension of clustering work. In English, same word can be used as different parts, usually in a slightly different form. We can think of recommendation system as a task to fill out missing preference data on a user-item matrix, based on observed values. We will use these terms interchangeably. The information we gather contains each paper’s title, list of authors, key words, and abstract. In order to build a user-item matrix with this data, we assume that users are interested in their own papers. We set high score (in this paper, 5) to every <author, paper> pair that the paper is written by. More serious problem is that we do not have ”dislike” information. When we request users toexplicitly rate items in a common recommendation system, we can get both positive and negative feedback from the user. This means that it is possible to accurately estimate missing data. We basically assume that authors will like papers similar to ones they wrote before. In our Naive Recommender, we just apply this assumption. When we try to recommend a set of papers to a specific user, we calculate similarity between every paper and the user’s own papers. Then, we take the highest similarity as the score of that paper. The process is similar to K-means, but the centroids are also papers. After assigned to a cluster, the score is calculated based on the distance between the candidate paper and its centroid. Using the calculated score as a distance metric, we select k papers for recommendation to the target user. It will be important to include the information about relevant researchers to users and recommend papers that they found interesting. If we provide information about which researcher liked this paper, it would also give them great reason and motivation to read that paper. In this paper, we have presented a Personalized Academic Research Paper Recommendation System, which recommends related articles for each researcher. Thanks to our system, researchers can get their related papers without searching keywords on Google or browsing top conferences’ proceedings.\n",
    "add Codeadd Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUq4uq8KfaSo"
   },
   "source": [
    "Combined Summary: Recommender systems are widely used these days in e-commerce, for the purpose of personalized recommendation. Amazon.com is using recommender systems for books. Personalized recommendation can be applied to outside of commercial applications. We suggest developing the scholarly paper recommendation system for academic researchers. It will automatically detect their research topics they are interested in and recommend the related articles they may be interested in based on similarity of works. The most effective memory-based algorithms are item-based CF, matrix factorization, content-based methods and hybrid approach. Koren suggests combining rating information and user, item profiles for more accurate recommendation. Matsatsinis introduced scientific paper recommendation using decision theory. Sugiyama extended scholarly paper recommendation with citation and reference information. It is difficult to evaluate recommender systems due to lack of standard methods. We combined title, key words, and abstract to construct a set of words representing a paper. First, we removed stop words such as ”the” or ”of”. This process reduces the length of dictionary, resulting in reduced dimension of the clustering work. In English, same word can be used as different parts, usually in a slightly different form. It is better to deal with minor changes of forms as same words, as it can dramatically reduce the dimension. As a first step, we removed last ”ed”, ”ly”, and ”ing” from the word, whenever encountered. We are ready to proceed to our main goal: personalized recommendation of academic papers. In this paper, we discuss fundamental characteristics of our problem. We build a user-item matrix with each paper’s title, list of authors, key words, and abstract. We use 1-5 scale as it is widely used in recommendation systems in literature. According to Netflix Prize data, only 1% of cells of the user-item matrix are observed 3 values. It is possible to estimate missing data only using small amount of observed data. In our situation, one author writes only one or two papers in one conference proceeding. There are only at most two or three top-level conferences in each field, the maximum number of papers one author can publish a year is about 10. lem makes it difficult to use widely-used collaborative filtering algorithms. 3.3.2 Naive Recommender uses k-Nearest Neighbors (kNN) algorithm to select and recommend most similar n papers to the target user’s previous paper. Papers are assigned to only one of the most similar papers written by the target user. After assigned to a cluster, the score is calculated based on the distance between the candidate paper and its centroid. Using the calculated score as a distance metric for kNN, we select k papers for recommendation to the target user. It will be important to include the information about relevant researchers to users and recommend papers that they found interesting or they have written. For the perspective of machine learning, we may need to consider about scalability. TF-IDF model [10] can be a great candidate to implement. In this paper, we have presented a Personalized Academic Research Paper Recommendation System. We have developed a web crawler to retrieve a huge number of research papers from the web. Second, we define a similarity measure for research papers. Third, we have developed our recommender system using collaboration filtering methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Jwmrux7feJV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9JGdclHfePb"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Paths to the directories you want to download\n",
    "model_dir = '/content/pegasus-samsum-model'\n",
    "# tokenizer_dir = '/content/tokenizer'\n",
    "# dataset_dir = '/content/samsum_dataset'\n",
    "\n",
    "# Path for the compressed archive\n",
    "archive_name = 'my_MODEL'\n",
    "\n",
    "# Create a compressed archive of the directories\n",
    "shutil.make_archive(archive_name, 'zip', model_dir)\n",
    "\n",
    "# Download the compressed archive\n",
    "files.download(archive_name + '.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "FYHNTIUAhouQ",
    "outputId": "1af019c7-6ad7-4c46-a445-8f6c75aa5986"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_b7235d90-0cf0-40a6-99f7-81335545bd7e\", \"my_MODEL_Tokenizer.zip\", 2428660)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Paths to the directories you want to download\n",
    "# model_dir = '/content/pegasus-samsum-model'\n",
    "tokenizer_dir = '/content/tokenizer'\n",
    "# dataset_dir = '/content/samsum_dataset'\n",
    "\n",
    "# Path for the compressed archive\n",
    "archive_name = 'my_MODEL_Tokenizer'\n",
    "\n",
    "# Create a compressed archive of the directories\n",
    "shutil.make_archive(archive_name, 'zip', tokenizer_dir)\n",
    "\n",
    "# Download the compressed archive\n",
    "files.download(archive_name + '.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB2n22zCgh1L"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Paths to the directories you want to download\n",
    "# model_dir = '/content/pegasus-samsum-model'\n",
    "# tokenizer_dir = '/content/tokenizer'\n",
    "dataset_dir = '/content/samsum_dataset'\n",
    "\n",
    "# Path for the compressed archive\n",
    "archive_name = 'samsum-dataset'\n",
    "\n",
    "# Create a compressed archive of the directories\n",
    "shutil.make_archive(archive_name, 'zip', tokenizer_dir)\n",
    "\n",
    "# Download the compressed archive\n",
    "files.download(archive_name + '.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "def clean_research_paper(paper_text):\n",
    "  \"\"\"Cleans a scientific research paper for text summarization.\n",
    "\n",
    "  Args:\n",
    "    paper_text: The text of the research paper.\n",
    "\n",
    "  Returns:\n",
    "    The cleaned text of the research paper.\n",
    "  \"\"\"\n",
    "\n",
    "  # Remove all formatting.\n",
    "  paper_text = re.sub(r\"\\[[^\\]]+\\]\", \"\", paper_text)\n",
    "  paper_text = re.sub(r\"<.*?>\", \"\", paper_text)\n",
    "\n",
    "  # Tokenize the text.\n",
    "  tokens = nltk.word_tokenize(paper_text)\n",
    "\n",
    "  # Remove stop words.\n",
    "  stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "  tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "  # Stem the words.\n",
    "  stemmer = nltk.stem.PorterStemmer()\n",
    "  tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "  # Remove punctuation.\n",
    "  tokens = [token for token in tokens if not token in string.punctuation]\n",
    "\n",
    "  # Calculate the frequency of each word.\n",
    "  word_counts = nltk.FreqDist(tokens)\n",
    "\n",
    "  # Identify the key sentences.\n",
    "  key_sentences = []\n",
    "  for sentence in paper_text.split(\".\"):\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sentence_score = 0\n",
    "    for token in sentence_tokens:\n",
    "      sentence_score += word_counts[token]\n",
    "    if sentence_score > 0:\n",
    "      key_sentences.append(sentence)\n",
    "\n",
    "  # Generate the summary.\n",
    "  summary = \" \".join(key_sentences)\n",
    "\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Large Language Models\n",
    "Michael R. Douglas\n",
    "CMSA, Harvard University\n",
    "Dept. of Physics, Stony Brook University\n",
    "mdouglas@cmsa.fas.harvard.edu\n",
    "July 2023\n",
    "Abstract\n",
    "Artificial intelligence is making spectacular progress, and one of the\n",
    "best examples is the development of large language models (LLMs) such\n",
    "as OpenAI’s GPT series. In these lectures, written for readers with a\n",
    "background in mathematics or physics, we give a brief history and survey\n",
    "of the state of the art, and describe the underlying transformer architec-\n",
    "ture in detail. We then explore some current ideas on how LLMs work\n",
    "and how models trained to predict the next word in a text are able to\n",
    "perform other tasks displaying intelligence.\n",
    "1arXiv:2307.05782v1  [cs.CL]  11 Jul 20231 Introduction\n",
    "At the end of November 2022, OpenAI released a system called ChatGPT which\n",
    "interacts with its users in natural language. It can answer questions, engage in\n",
    "dialogs, translate between languages and write computer code with a fluency\n",
    "and ability far exceeding all previous publically available systems. Although it\n",
    "falls well short of human abilities in many ways, still the large language model\n",
    "technology of which it is an example is widely considered to be a major advance\n",
    "in artificial intelligence.1\n",
    "Few developments in science and technology entered the popular conscious-\n",
    "ness as quickly as ChatGPT. There is no mystery about why. The ability to use\n",
    "language is a defining property of humanity, and for the first time a computer\n",
    "is doing this well enough to make a comparison with humans interesting. All\n",
    "of the hopes and fears which have developed around AI, robots and technology\n",
    "more generally are being brought into the discussion. In my opinion this is\n",
    "justified; the speed of recent progress makes it urgent to better understand AI,\n",
    "to forecast its capabilities and limitations, and to make wise decisions about\n",
    "its development and use. With great opportunities will come great challenges,\n",
    "which will concern all of us.\n",
    "In these lecture notes we give an introduction to this subject for mathe-\n",
    "maticians, physicists, and other scientists and readers who are mathematically\n",
    "knowledgeable but not necessarily expert in machine learning or artificial intel-\n",
    "ligence. We begin with a very brief overview of AI in §2 to explain some ideas\n",
    "we consider to be essential context, the basic principles of the symbolic and\n",
    "connectionist approaches. In §3 we define statistical language models and relate\n",
    "the history of transformer-based LLMs up through GPT-4. In §4 we discuss\n",
    "measures of what LLMs do and how well they do it. We then give a precise\n",
    "explanation of simpler language models in §5 and the transformer architecture\n",
    "in§6.\n",
    "It is amazing that a model defined by a few short equations, trained to go\n",
    "through a text and simply predict each next word as it appears – a task which\n",
    "seems only loosely related to any definition of intelligence – can do tasks which\n",
    "“obviously” require intelligence, such as solving word problems like the one in\n",
    "Figure 1 below. At present nobody really understands how this works. Even\n",
    "the interpretation of what LLMs are doing is controversial, ranging from the\n",
    "belief that they are “simply” rearranging the sentences they were trained on,\n",
    "all the way to the belief that the LLMs are learning sophisticated models of\n",
    "the world and that “simply” scaling up the computations will produce artificial\n",
    "general intelligence. Any forecast for progress must take into account the current\n",
    "models’ shortcomings – lack of long term memory and ability to plan, tendency\n",
    "to make up facts and “hallucinate,” unreliability in logical reasoning, etc.. Do\n",
    "these problems have technical solutions which will also look simple once we have\n",
    "them? Or are they more significant barriers?\n",
    "Most current work on LLMs takes an engineering and problem solving per-\n",
    "1A few of the many other milestones in LLM development are [22, 32, 39, 113, 114].\n",
    "2Question: Assume that the variance of the first nnatural numbers is 10, and\n",
    "the variance of the first meven natural numbers is 16. Compute m+n.\n",
    "Model output: Letσ2be the variance of the first meven natural numbers,\n",
    "andτ2be the variance of the first nnatural numbers. Then, we have τ2=n2−1\n",
    "12\n",
    "andσ2=m2−1\n",
    "3. We solve the system of equations:\n",
    "n2−1\n",
    "12= 10\n",
    "m2−1\n",
    "3= 16.\n",
    "This translates into the system\n",
    "n2= 121\n",
    "m2= 48 + 1 = 49 .\n",
    "Therefore, n= 11 andm= 7 , son+m= 18 .\n",
    "Figure 1: A question-answer pair solved by Minerva. From Lewkowycz et al\n",
    "[77, 1], “Solving quantitative reasoning problems with language models,” 2022.\n",
    "spective, but there are many interesting works which focus more on understand-\n",
    "ing how LLMs work. One would think this should be far easier than understand-\n",
    "ing how human brains work, as we have full knowledge of an LLM’s microscopic\n",
    "workings and can do a wide variety of experiments on it.2These efforts are in\n",
    "their early days, but in §7 we survey current approaches to understanding how\n",
    "LLMs do what they do. We conclude in §8 with more general discussion, some\n",
    "questions and potentially important developments to watch for.\n",
    "Before we start, let me say a little about my own background. I was trained\n",
    "as a theoretical physicist and most of my contributions to science are in string\n",
    "theory and its interface with mathematics, but I have followed AI fairly closely\n",
    "since the 80’s and in detail since 2016. In addition I spent eight years in quan-\n",
    "titative finance where I gained a good deal of “hands-on” experience with ma-\n",
    "chine learning. I have given many lectures telling computer scientists about\n",
    "physics and physicists about computational topics, and benefited from conver-\n",
    "sations with many people – more than I can name here, but let me thank\n",
    "Gerald Jay Sussman, David McAllester, Yann LeCun, Sanjeev Arora, Surya\n",
    "Ganguli, Jeremy Avigad, Vijay Balasubramanian, Dmitri Chklovskii, David\n",
    "Donoho, Steve Skiena, Christian Szegedy, Misha Tsodyks, Tony Wu and the\n",
    "2At least, this was the case before March 2023. Currently the weights and even the de-\n",
    "sign parameters of GPT-4, the most advanced LLM, are held in confidence by OpenAI as\n",
    "proprietary information.\n",
    "3many speakers in the CMSA New Technologies seminar series,3and Josef Urban\n",
    "and the AITP community.4Thanks to David McAllester and Sergiy Verstyuk\n",
    "for comments on the first draft. These notes would not have been possible with-\n",
    "out their input and advice, and I hope their signal to noise ratio approaches\n",
    "that of what they shared with me.\n",
    "2 Symbolic and connectionist AI\n",
    "The goal of artificial intelligence is to build computational systems which can\n",
    "perform tasks which require intelligence. Although intelligence is hard to define\n",
    "precisely, an operational definition suitable for LLMs is ability at tasks requiring\n",
    "language, reasoning, and planning, as judged by humans who interact with the\n",
    "system. Some famous and difficult “challenge tasks” include playing chess [122],\n",
    "proving mathematical theorems [104], and answering natural language questions\n",
    "using generally known facts and common sense.5\n",
    "These problems have been the subject of intense investigation since the mid-\n",
    "50’s, and a few textbooks and histories are [93, 105, 119, 121]. Essentially from\n",
    "the start, two broad approaches were set out, which would later be called sym-\n",
    "bolic and connectionist AI.6The symbolic approach originated in mathematical\n",
    "logic and generative linguistic theory, and tracked the development of computer\n",
    "technology (both hardware and software) as a tool for solving practical and\n",
    "scientific problems. Central topics in this approach are formal logic and lan-\n",
    "guage theory, search and heuristics, and engineering techniques for designing\n",
    "and building large and complex systems.\n",
    "Symbolic AI systems are designed, meaning that their creators develop a de-\n",
    "tailed understanding of the task, and encode this understanding into the system\n",
    "by programming, by hardware design and otherwise. As an example, consider\n",
    "the task of parsing: given an input string of words, determine its grammatical\n",
    "structure. Most of us learned to diagram sentences in elementary school, and\n",
    "although linguists have developed far more sophisticated notions of grammar,\n",
    "this simple notion gives the right idea. The grammar of a language is encoded\n",
    "in rules, which belong to a formal framework – see the appendix for the example\n",
    "of context-free grammars. Given such a framework, one can design a parsing\n",
    "algorithm which takes as input a rule set and an input string, and produces an\n",
    "output which states whether the string is a grammatical sentence and if so makes\n",
    "its structure explicit. This is a symbolic AI approach, not because the words\n",
    "“symbolize” anything (after all grammar does not have to come with mean-\n",
    "3https://live-hu-cmsa-222.pantheonsite.io/\n",
    "4http://aitp-conference.org/2023/\n",
    "5This challenge originates with Turing’s famous test, but the restriction to question answer-\n",
    "ing makes it better defined and testable using benchmarks, standardized question-answer sets.\n",
    "Discussion of the original test can be found at https://en.wikipedia.org/wiki/Turing test\n",
    "6Symbolic AI is sometimes called “GOFAI” for “good old-fashioned AI.” Related terms\n",
    "include “rule based,” “logic based,” “expert system” and “feature engineering.” The con-\n",
    "nectionist approach has many other names, reflecting its mixed ancestry: “neural,” “deep\n",
    "learning,” “parallel distributed processing,” “differentiable,” and “representation learning.”\n",
    "4ing), but because the grammatical rules and the parsing algorithm (including\n",
    "its internal data structures) have a clear meaning to their designers.\n",
    "Symbolic methods have had considerable success at many tasks requiring\n",
    "intelligence, famously including chess playing [60] and symbolic algebra7as well\n",
    "as more prosaic but very central tasks such as translating high level computer\n",
    "languages to machine code (compiling). And a great deal of work has been\n",
    "done to broaden their scope, for example to build question answering systems\n",
    "such as the well known IBM Watson. Many valuable techniques came out of\n",
    "this; ways to systematize rules into “knowledge bases” or “knowledge graphs,”\n",
    "methods for automated logical reasoning, and so on. But it was long ago realized\n",
    "that once one goes beyond “formal worlds” such as chess and algebra to the\n",
    "complex and messy situations of real life, although one can postulate rules which\n",
    "capture many truths and can be used for reasoning, rules which are valid in all\n",
    "cases are very rare. Furthermore, the sheer number of rules required to cover\n",
    "even the likely possibilities is very large. These difficulties were addressed by\n",
    "implementing probabilistic reasoning and by getting teams of humans to develop\n",
    "the requisite enormous rule sets, leading to the “expert system” approach which\n",
    "was applied (for example) to medical question answering. Cyc,8an early and\n",
    "well known expert system, is commercially available and has a database of\n",
    "commonly known facts with over 25 million assertions; however this is dwarfed\n",
    "by knowledge bases such as Wikidata (over one billion “facts”) but which do\n",
    "not have a systematic reasoning engine. It is clear that any approach which\n",
    "depends on careful human analysis of such large knowledge bases is impractical.\n",
    "Meanwhile, a very different “connectionist” approach to AI was being cham-\n",
    "pioned by other researchers. They drew their inspiration from hypotheses about\n",
    "how the brain works, from information theory and statistics, from physics and\n",
    "other natural sciences, and from applied mathematics and particularly opti-\n",
    "mization theory. These diverse points of view came together in the 1990’s and\n",
    "led to a great deal of interdisciplinary work,9of which the part most related to\n",
    "AI and which lies behind LLMs is called machine learning (ML).\n",
    "The usual starting point in modern treatments of ML is to rephrase a task\n",
    "as a statistical inference problem based on a large dataset. A canonical example\n",
    "is image recognition – say, given an array of pixels (light intensity values),\n",
    "estimate the probability that the image depicts a cat. Rather than design a\n",
    "system to do this, one starts with a large dataset of images with labels (cat and\n",
    "non-cat). One then designs a very general statistical model and “trains” it on\n",
    "this dataset to predict the label given the image. This is supervised learning,\n",
    "one can also do “self-supervised” learning in which the system predicts some\n",
    "part of the data given other parts (say, filling in part of an image). A third\n",
    "standard ML paradigm is reinforcement learning, which applies to tasks which\n",
    "involve choosing actions to fulfill a longer term goal. The classic example is\n",
    "game playing, as in AlphaGo and AlphaZero.\n",
    "In any case, since the problem is formulated statistically, it is possible to\n",
    "7https://www.sigsam.org/\n",
    "8https://cyc.com/\n",
    "9I first learned about this from [83, 101].\n",
    "5consider the training dataset one item at a time, and use it to incrementally\n",
    "improve the model. This is almost always done by formulating the task in terms\n",
    "of an “objective function” which measures the quality with which it is performed,\n",
    "for example the accuracy with which correct labels are assigned to images. One\n",
    "then takes a parameterized model and trains it by optimizing this function,\n",
    "evaluated on the training dataset, with respect to the model parameters. For\n",
    "the classic models of statistics this can be done analytically, as in a least squares\n",
    "fit. For more general models one uses numerical methods, such as gradient\n",
    "descent. Either way, a central question of statistics and machine learning is\n",
    "generalization, meaning the extent to which the model well describes data not\n",
    "in the training set but sampled from the same probability distribution. A well\n",
    "known principle which speaks to this question is “Occam’s razor,” that simpler\n",
    "models will generalize better. This is often simplified to the rule that a model\n",
    "should have the minimal number of parameters needed to fit the dataset.\n",
    "Not all machine learning systems are “deep learning” [76] or “connection-\n",
    "ist” [118]. These terms generally refer to the use of neural networks with large\n",
    "numbers of parameters which provide effective universal function approxima-\n",
    "tors. While the idea is very old [117], before 2012 it was widely believed to be\n",
    "impractical. One argument for this was the “dull side” of Occam’s razor – mod-\n",
    "els with so many parameters were destined to overfit and would not generalize.\n",
    "Evidently this is not the case, leading to concepts such as “benign overfitting.”\n",
    "[10, 15] Another argument was that the objective functions for these models\n",
    "are highly nonconvex and optimization would get stuck at poor quality local\n",
    "minima. This can be a problem, but turns out to be solvable for reasons that\n",
    "are partially understood [31, 49]. Finally, despite the effectiveness of the trained\n",
    "model in performing a task, the large number of parameters often makes it very\n",
    "hard to understand how such a model works, and why a given input produces\n",
    "a particular output. This “interpretability problem” remains a key issue with\n",
    "deep learning models, and is the subject of much research [52].\n",
    "There are many other variations and hybrid approaches in the story. Another\n",
    "important one is the “pattern recognition” approach [19, 102]. This is also based\n",
    "on statistical inference but – like the symbolic approach – it emphasizes the value\n",
    "of detailed understanding of the problem domain in designing the system. For\n",
    "example, one could hand-code the initial layers of an image recognition network\n",
    "to detect lines or other significant “features” of the image. But unlike a purely\n",
    "symbolic approach, these features would be used as input to a general statistical\n",
    "or neural model.\n",
    "Another concept which illustrates the relation between the two approaches\n",
    "is probabilistic reasoning, the use of rules such as “the chance of rain when it is\n",
    "cloudy is 50%”. One can state and use such rules in a symbolic approach (see\n",
    "for example [69]), the essential distinction with connectionism is not the use of\n",
    "probabilities but rather the representation of knowledge in terms of explicit and\n",
    "meaningful rules.\n",
    "As we suspect every reader has already heard, the symbolic approach was\n",
    "dominant from the early days until 2012, and (along with many other suc-\n",
    "cesses) led to a superhuman chess player, but seemed inadequate for our other\n",
    "6two challenge tasks (theorem proving and question answering). In 2012 the\n",
    "connectionist approach surpassed other approaches to computer vision [70], and\n",
    "ever since neural systems have gone from triumph to triumph. In 2016 the\n",
    "deep learning system AlphaZero surpassed the symbolic AI chess players (and\n",
    "of course humans). Over the last few years, transformer models trained on a\n",
    "large corpus of natural language to predict each next word as it appears, have\n",
    "revolutionized the field of natural language processing. As we write this the\n",
    "state of the art GPT-4 demonstrates truly remarkable performance at question\n",
    "answering, code generation and many other tasks [24].\n",
    "The simplest and arguably deepest explanation for this history is that it is\n",
    "a consequence of the exponential growth of computational power and training\n",
    "datasets, which continues to the present day. Given limited computing power\n",
    "and data, the ability of the symbolic and pattern recognition approaches to\n",
    "directly incorporate human understanding into a system is a significant advan-\n",
    "tage. On the other hand, given sufficiently large computing power and data,\n",
    "this advantage is nullified and may even become disadvantageous, as the human\n",
    "effort required to code the system becomes the limiting resource. This point,\n",
    "that the most significant advances in AI (and computation more generally) have\n",
    "come from hardware improvements and replacing human engineering with data-\n",
    "driven methods, is forcefully made by Sutton in his “bitter lesson” essay [128].\n",
    "In§3,§4 and §8 we will discuss scaling laws and evidence for and against the\n",
    "idea that by continuing along the current path, training ever-larger models on\n",
    "ever-larger datasets, we will achieve AGI (artificial general intelligence, whatever\n",
    "that means) and the realms beyond.\n",
    "Up to now the symbolic and connectionist approaches have generally been\n",
    "considered to be in tension.10There is another point of view which consid-\n",
    "ers them complementary, with a symbolic approach better suited for certain\n",
    "problems (for example logical reasoning) and connectionist for others (for ex-\n",
    "ample image recognition). Given this point of view one can seek a synthesis or\n",
    "“neurosymbolic” approach, advocated in many works [7, 47, 72].\n",
    "But are they in conflict at all? Another reconciliation is the hypothesis that\n",
    "problems which in the symbolic approach are solved using rules and algorithms,\n",
    "are also being solved that way by neural systems and in particular by LLMs.\n",
    "However, rather than the algorithms and rules being coded by humans, as the\n",
    "result of its training procedure the LLM has somehow learned them, encoded\n",
    "in its networks in some as yet mysterious way. This vague hypothesis can be\n",
    "sharpened in many ways, in part by proposing specific mechanisms by which\n",
    "algorithms and rules are encoded, in part by making general claims about the\n",
    "algorithms which are being learned. We discuss these ideas in §7 and §8.\n",
    "10To better discuss this point one should refine the symbolic-connectionist dichotomy into\n",
    "multiple axes: system design versus learning from data; meaningful rules versus uninterpreted\n",
    "models; combinatorial versus differentiable optimization; deterministic versus probabilistic.\n",
    "73 Language models\n",
    "Throughout the history of linguistics, languages have been described in terms\n",
    "of rules: rules of grammar, phonology, morphology, and so on, along with log-\n",
    "ical and other frameworks for describing meaning. This remains the case in\n",
    "Chomskyan linguistics and in much of theoretical linguistics.\n",
    "By contrast, LLMs are statistical language models, meaning that they encode\n",
    "a probability distribution on strings of words, call this P(w1. . . w L), which\n",
    "approximates the distribution realized by a large body (or “corpus”) of text in\n",
    "the language. The simplest example is the frequency or “1-gram” model defined\n",
    "by taking the words to be independently distributed, so\n",
    "P(w1. . . w L) =LY\n",
    "i=1P(wi);P(w) =number of occurrences of win the corpus\n",
    "total number of words in the corpus.\n",
    "(1)\n",
    "Of course, this model captures very little of the structure of language, which\n",
    "involves dependencies between the word choices.\n",
    "LLMs are generative models,11by which we will mean that there is a practi-\n",
    "cal method for sampling from the distribution. To explain this, consider a word\n",
    "prediction task in which some words in a string are given (the “input”) and\n",
    "others left blank (the “output”). Given a probability distribution P(w1. . . w L),\n",
    "there is a corresponding conditional probability distribution for the output given\n",
    "the input. As an example, suppose we are given the string “The cat [BLANK]\n",
    "outside,” where “[BLANK]” is a “token” which marks the position of the missing\n",
    "word. The relevant conditional probabilities might be\n",
    "P(the cat went outside |the cat [BLANK] outside) = 0 .5\n",
    "P(the cat sat outside |the cat [BLANK] outside) = 0 .2\n",
    "and so on, summing to total probability 1. In the masked word prediction task,\n",
    "the model must determine (or sample from) this distribution.\n",
    "A particularly convenient case is to give the conditional probability of the\n",
    "word which follows a given string, which we denote as\n",
    "P(wn+1|w1w2. . . w n−1wn). (2)\n",
    "By sampling this distribution to get a new word wn+1and appending it to the\n",
    "end, the string can be extended one word at a time. Repeating this process\n",
    "gives an arbitrarily long string, which by the laws of probability is a sample\n",
    "from the original probability distribution P(w1. . . w L), for example\n",
    "P(the cat went outside) = P(the) P(cat|the)P(went |the cat) P(outside |the cat went) .\n",
    "This factorization of the probability into successive conditional probabilities\n",
    "defines the class of autoregressive models. One could furthermore require that\n",
    "11Many different definitions of this term can be found in the literature.\n",
    "8the conditional probability Eq. 2 depends only on the kmost recent words, in\n",
    "which case one would have a Markov model whose state is a string of kwords.\n",
    "To evaluate how good a language model is, we want to quantify how well its\n",
    "probability distribution approximates that of the corpus (the empirical distribu-\n",
    "tion). The standard measure of this is the cross entropy. For an autoregressive\n",
    "model this is a sum of terms, one for each word in the corpus,12\n",
    "L=−1\n",
    "NN−nX\n",
    "i=1logP(wi+n|wiwi+1. . . w i+n−1) (3)\n",
    "One also refers to exp −Las “perplexity.” In a machine learning approach, we\n",
    "can use Eq. 3 as an objective function and minimize it as a function of the\n",
    "network parameters to train the network. We can then apply the many tools of\n",
    "ML: backpropagation, splitting the sum into batches, varying the learning rate\n",
    "and so on, to get an efficient and effective model. While the details are an art\n",
    "which depends on the particular domain and model architecture,13conceptually\n",
    "these are much the same for LLMs as for other machine learning models.\n",
    "This statistical approach to modeling language has been pursued since the\n",
    "late 80’s [21, 64, 87] and many models were developed, such as the recurrent neu-\n",
    "ral network (RNN) which we will describe in §5. Following the general machine\n",
    "learning experience that supervised tasks (learning from input-output pairs)\n",
    "are easier than unsupervised tasks, many of these works addressed machine\n",
    "translation and parsing, for which there are good labeled datasets (documents\n",
    "with their translations; sentences with their grammatical structure). However\n",
    "unlabeled datasets are much larger and by 2015 or so there was a sense that self-\n",
    "supervised learning was the next frontier [74], leading to more focus on masked\n",
    "word prediction.\n",
    "The history of transformer models starts with the 2017 proposal of Vaswani\n",
    "et al [132]. Their model was designed for a translation task and was more com-\n",
    "plicated than what we will explain in §6, but the essential idea to use attention\n",
    "and positional encoding to represent all the relations between the words in a\n",
    "text originated here and is fully present.\n",
    "The transformer architecture was taken up by many groups, and particularly\n",
    "influential 2018 works include BERT [39] and GPT [112]. BERT was trained by\n",
    "masking arbitrary words in a sentence (not just the next word), which allows\n",
    "the model to look backward and forward for context and leads to better results.\n",
    "However it is not straightforward to sample from such a model, and eventually\n",
    "the simpler next word prediction approach followed by GPT won out.\n",
    "Both of these models, and most work of this period, followed the paradigm\n",
    "of pretraining followed by fine tuning. The idea was to first train for word\n",
    "prediction on a very large corpus, to get a general purpose model. This would\n",
    "then be adapted to specific tasks such as question answering by fine tuning.\n",
    "This means doing a second pass of supervised learning on a much smaller labeled\n",
    "12With the sign, L ≥0 and better models have smaller L. The term “loss function” is often\n",
    "used for an objective function with these properties.\n",
    "13In CS this term generally refers to the large scale arrangement of components of a system.\n",
    "9dataset, replacing next word prediction by the objective function for the specific\n",
    "task. Say we are doing question answering, this could be the accuracy of the\n",
    "answers. This two step procedure was justified by the notion of transfer learning,\n",
    "meaning that the capabilities of the general purpose model “transfer” to related\n",
    "but different tasks. This approach led to SOTA14results on many benchmarks\n",
    "and motivated much further work.\n",
    "Most importantly, a great deal of ingenuity and hard work was put into\n",
    "solving the engineering problems of training larger and larger models on larger\n",
    "and larger datasets. As for the data, a lot of text is available on the web, with\n",
    "one much used archive of this data provided by Common Crawl.15Training can\n",
    "largely be done in parallel by dividing up this data, and the availability of large\n",
    "clusters of GPU-enabled servers at industrial labs and through cloud computing\n",
    "meant that sufficient computing resources were available in principle. However,\n",
    "the overall cost of training scales as (at least) the product of model size and\n",
    "dataset size, and this was becoming expensive. While the precise cost figures\n",
    "for the GPT series are not public, it is estimated that a single training run\n",
    "of the largest GPT-3 models cost tens of millions of dollars. To motivate and\n",
    "efficiently carry out such costly experiments, one needs some ability to predict in\n",
    "advance how changes in model and dataset size will affect the training methods\n",
    "(for example the optimal choice of learning rate) and performance.\n",
    "An important advance in this direction was the observation of power law\n",
    "scaling in language model performance [67]. Figure 2 plots the test loss16against\n",
    "the logarithms of the sizes and compute resources used, and these straight lines\n",
    "correspond to a power law relation between size and perplexity. This scaling\n",
    "holds over many decades in model size and, while the exponents α∼ −0.076 to\n",
    "−0.095 are rather small, this is a strong argument that larger models will have\n",
    "better performance. These ideas were also used to determine optimal model-\n",
    "dataset size tradeoff [58] and the scaling of hyperparameters [140]. These results\n",
    "were a significant input into the decision to do this very expensive research.\n",
    "Year Model Number of Parameters Dataset size (tokens)\n",
    "2018 GPT 110M 1B\n",
    "2018 BERT 340M 3B\n",
    "2019 GPT-2 1.5B 10B\n",
    "2020 GPT-3 175B 500B\n",
    "2022 PaLM 540B 780B\n",
    "2023 GPT-4 1.4T (?) ?\n",
    "Table 1: Large Language Models (M/B/T = million/billion/trillion). In many\n",
    "cases several model sizes were considered; we quote the largest.\n",
    "14State of the art, in other words an improvement over all previously evaluated models.\n",
    "15https://commoncrawl.org/\n",
    "16This is Eq. 3 (minus log perplexity) evaluated on texts which were removed or “held out”\n",
    "of the training set, to get a measure of generalization ability.\n",
    "10Dataset Size tokensParameters non-embeddingCompute PF-days, non-embeddingTest LossFigure 2: Language modeling performance as a function of model size, dataset\n",
    "size, and amount of compute used for training. From Kaplan et al, “Scaling\n",
    "Laws for Neural Language Models,” 2020 [67].\n",
    "Now it should be realized that, while the measure being improved here is\n",
    "fairly objective, still there was no strong reason to think that improving it would\n",
    "lead to models with qualitatively new “emergent” capabilities. But it appears\n",
    "that this is what happened: GPT-3 and its fine-tuned cousins (such as Codex)\n",
    "were able to do tasks, such as write computer code from a natural language\n",
    "description, for which smaller models were almost worthless.17We will discuss\n",
    "more of this progress shortly, and speculate a bit in the conclusions.\n",
    "One of the most interesting LLM phenomena is in-context learning, first\n",
    "discussed in the original GPT-3 paper [22]. This refers to the ability of an\n",
    "LLM to carry out tasks different from its original objective without modify-\n",
    "ing its parameters, indeed without any need for additional training on the new\n",
    "task (fine tuning). Rather, after being given (as input text) a few examples\n",
    "of input-output pairs, the LLM can be given another input and will generate\n",
    "a suitable output. Say the new task is question answering, then after a few\n",
    "question-answer examples the LLM will answer the next question it is given.\n",
    "While intuition based on human abilities might find this unremarkable, it is\n",
    "actually quite unusual for an ML model and this is why the pretraining-fine\n",
    "tuning paradigm was the usual approach in previous work. Of course the train-\n",
    "ing set already contains many examples of QA pairs. More striking are tasks\n",
    "which are not much represented in the training set, such as finding anagrams\n",
    "or rearranging letters in words. One can even do in-context “meta-learning” of\n",
    "machine learning tasks such as linear regression (see §4).\n",
    "Once it is established that the model can generalize from a few examples, a\n",
    "further step towards human capabilities is to try zero examples, instead simply\n",
    "explaining the task in natural language. At this point it becomes difficult to\n",
    "classify the tasks – should we consider the task of writing code from a natural\n",
    "language specification to be a form of translation, or an example of explaining\n",
    "the task, or something else? The relation between the input text or “prompt”\n",
    "17A quantitative version of this claim is that performance for the “emergent” capability\n",
    "improves rapidly at some threshold value of the word prediction loss. This claim is disputed,\n",
    "see [120, 133] for discussion.\n",
    "11and the output has many surprising features. For example, a standard tech-\n",
    "nique in LLM question answering which measurably improves performance is to\n",
    "precede the question with a prompt such as “I will answer this question help-\n",
    "fully and truthfully.” Is this somehow biasing the network towards certain texts\n",
    "and away from others (after all the internet corpus is hardly a reliable source of\n",
    "truth) ? Suppose we have a theory of how this works, how can we test it? Does\n",
    "the model “know” anything about the truth of statements? [25, 79]\n",
    "As has been much reported, one of the major difficulties in using LLMs\n",
    "for practical tasks is their propensity to invent facts (especially citations) and\n",
    "their limited ability to do logical reasoning, algebra and other symbolic tasks.\n",
    "A device for improving this, called “chain of thought prompting,” is to give\n",
    "examples (say of question answer task for definiteness) with some intermediate\n",
    "reasoning steps spelled out. This was used in the Minerva QA system [77]\n",
    "which produced the example in Figure 1. Still the fraction of problems it solved\n",
    "correctly is around 50% (the later GPT-4 is similar). Even for simpler questions,\n",
    "the reliability of GPT-4 is more like 90%. Much current research is devoted to\n",
    "this problem of reliable reasoning, as we discuss in §8.\n",
    "4 Phenomenology of language models\n",
    "In this section we discuss general claims, “non-invasive” experiments, and the-\n",
    "oretical arguments which do not depend on “microscopic details” of the models\n",
    "such as the trained weights.18This includes evaluation of model capabiliities,\n",
    "qualitative observations and scaling laws.\n",
    "What can LLMs do? There is a huge body of work on this question, and any\n",
    "attempt to review it would rapidly go out of date, but let us review the primary\n",
    "method for studying it. This is benchmarking, the development of standardized\n",
    "sets of test items for which model accuracy can be evaluated in a reproducible\n",
    "way. This is in principle straightforward if the input corresponds to a single\n",
    "correct output, as in multiple choice question answering.19If the answer is free-\n",
    "form text, one can use text comparison metrics such as the ROUGE score. One\n",
    "current standard for evaluating LLMs, BIG-bench [127], combines 204 language\n",
    "tasks (at first publication; they accept new tasks) including translation, QA,\n",
    "puzzle solving, text classification and summarization, and tests of common sense\n",
    "reasoning. A leaderboard listing the current best LLMs is at20. Another is the\n",
    "EleutherAI “Language Model Evaluation Harness”21and leaderboard.22The\n",
    "benchmark suite HELM [80] measures additional metrics such as tendency to\n",
    "repeat copyrighted material, bias, toxicity and the like.\n",
    "18We are calling this “phenomenology” following the physics use of the term, not its use in\n",
    "psychology and philosophy to describe the study of subjective experience.\n",
    "19A potential pitfall is that after a benchmark is published, the test items can find their\n",
    "way into future training data and then be solved by memorization. Methods to detect and\n",
    "prevent this are discussed in the references.\n",
    "20https://paperswithcode.com/dataset/big-bench\n",
    "21https://github.com/EleutherAI/lm-evaluation-harness\n",
    "22https://huggingface.co/spaces/HuggingFaceH4/open llmleaderboard\n",
    "12Reasoning ability is of particular interest for mathematical and scientific ap-\n",
    "plications – of course we all look forward to the day when computers will help us\n",
    "grade assignments, referee papers and do our research. There are many bench-\n",
    "marks for solving logical problems expressed in natural language. Benchmarks\n",
    "for mathematical theorem proving include NaturalProofs [135], MiniF2F [144]\n",
    "and ProofNet [6]; as of mid-2023 LLMs (and the best other systems) can find\n",
    "many proofs (20–80%) but still fail on some seemingly easy cases. Simpler as-\n",
    "pects of reasoning which have benchmarks are the ability to deal with negation\n",
    "[142], consistency (between different phrasings of the same question) [61], and\n",
    "compositionality (the ability to analyze statements and problems into simpler\n",
    "parts, solve these and combine the results) [111].\n",
    "Natural language tasks are very complex, and benchmarks constructed from\n",
    "real world data cannot be used directly in theoretical considerations. For this\n",
    "purpose one generally defines “toy worlds” and generates synthetic data. The\n",
    "possibilities are endless, but some which have been used are arithmetic prob-\n",
    "lems (decimal arithmetic; modular arithmetic), game play, solving systems of\n",
    "equations, and parsing formal languages. A particularly interesting task is lin-\n",
    "ear regression [48]; since this is the prototypical case of statistical inference, a\n",
    "system which learns to do it can be said to be “learning how to learn.”\n",
    "Coming to scaling laws, denote the model size (number of parameters) as\n",
    "Pand the dataset size (number of tokens in the corpus) as D, then there are\n",
    "two general regimes. If we hold one of these (say P) fixed and take the other\n",
    "(sayD) to infinity, then a law of large numbers applies and L ∼ 1/D. On the\n",
    "other hand, if we take one parameter very large and study the dependence on\n",
    "the other, nontrivial power law scaling can emerge. In principle one can get\n",
    "different exponents for DandP, suggesting the ansatz\n",
    "L(P, D) =\"\u0012Pc\n",
    "P\u0013αP/αD\n",
    "+Dc\n",
    "D#αD\n",
    ". (4)\n",
    "where Lis test loss Eq. 3 computed in an optimally regularized model.23This\n",
    "is a good fit to Figure 2.\n",
    "While in Figure 2 the two exponents appear to differ, there is not really\n",
    "convincing evidence that this is significant. Before working hard on this, one\n",
    "should ask if there is any way to control the many choices involved, so as to define\n",
    "universal exponents. One context in which this can be studied systematically is\n",
    "transfer learning, by distinguishing the dependence on the pretraining and fine\n",
    "tuning datasets [55]. Another relevant and practical question is whether one\n",
    "can prune the dataset to improve the scaling. It is intuitively plausible and can\n",
    "be shown in examples that sets of data items are worth more if they are diverse\n",
    "than if they are similar. The challenge is to find simple ways to quantify this\n",
    "similarity; in [126] many proposals are studied.\n",
    "23Regularization is a standard technique in statistics and ML used to control overfitting by\n",
    "models with too many parameters. If one does not regularize one sees other phenomena such\n",
    "as double descent [14]. For further discussion see [11, 13].\n",
    "13Scaling laws can arise in many ways, not specific to language models. One\n",
    "hypothesis is that the data lies on a low dimensional submanifold in a higher\n",
    "dimensional space.24Both the number of parameters and the number of points\n",
    "required to fit this manifold go as the dimension dof the manifold, and this\n",
    "leads to αP=αD= 4/d(the precise coefficient 4 depends on assumptions\n",
    "about smoothness) [8].\n",
    "A related hypothesis is that the spectral density of the data covariance falls\n",
    "off as a power law, and in [85] Eq. 4 is derived for a random feature model with\n",
    "this covariance. This hypothesis follows from the low dimensional hypothesis\n",
    "but it is more general, for example these authors argue that additional features\n",
    "derived from the data (as in nonlinear models such as FFN’s) generally have\n",
    "the same spectrum as the original data. One can also try to relate Eq. 4 and\n",
    "corrections to it to hypotheses about how tasks are learned [98].\n",
    "What does the scaling of the information theoretic quantity Eq. 3 have to\n",
    "do with performance on tasks requiring intelligence? A priori , not much, but\n",
    "one way to motivate a focus on it is to draw an analogy with particle physics.\n",
    "In the 30’s cosmic ray observations gave strong hints of new physics at higher\n",
    "energies, but the interesting events were too rare and uncontrolled to draw solid\n",
    "conclusions. Thus physicists were motivated to build accelerators. These are\n",
    "not that expensive when they fit on a tabletop, but rapidly grow in size and\n",
    "cost. How large does an accelerator need to be? The right measure is not its\n",
    "sizeper se but rather the energy of the particles it can produce. The physics\n",
    "relating size and energy is not trivial (due to effects such as synchrotron ra-\n",
    "diation) but can be worked out, so one can make a good prediction of energy\n",
    "reach. Still, as one increases energy, will one find a smooth extrapolation of\n",
    "what came before, or will one discover qualitatively new phenomena? In the\n",
    "golden age of accelerator physics (the 50’s-70’s) much new physics was discov-\n",
    "ered, mostly associated with new particles which are produced only above sharp\n",
    "energy thresholds. Currently the highest energy accelerator is the Large Hadron\n",
    "Collider at Cern, where the Higgs particle was discovered in 2012. While we\n",
    "are still waiting for further important discoveries, the potential for discovery is\n",
    "determined by measurable properties of the accelerator – by energy and secon-\n",
    "darily by intensity or “luminosity” – which we can judge even in the absence of\n",
    "qualitative discoveries. In the analogy, perplexity is playing a similar role as an\n",
    "objective measure of language model performance defined independently of the\n",
    "more interesting qualitative behaviors which reflect “intelligence.”\n",
    "How far can one push this analogy? Could perplexity be as central to lan-\n",
    "guage as energy is to physics? Eq. 3 has a fairly objective definition, so the\n",
    "idea is not completely crazy. But, not only was its relation to performance on\n",
    "actual tasks not predictable in advance, even after the fact clear “thresholds” or\n",
    "other signals for emergence of tasks have not yet been identified [133]. Perhaps\n",
    "if there are universal thresholds, evidence for them could be seen in humans.25\n",
    "More likely, additional variables (the quality and nature of the training corpus,\n",
    "24In§5 we explain how text can be thought of embedded in a high dimensional space.\n",
    "25Thanks to Misha Tsodyks for this suggestion.\n",
    "14details of the tasks, etc.) would need to be controlled to see them. This is\n",
    "another question probably better studied in simpler tasks using synthetic data.\n",
    "The final topic we discuss is the behavior of the objective function (Eq. 3)\n",
    "as a function of training time.26In almost all ML runs, such a plot shows long\n",
    "plateaus interspersed with steep drops. This has been interpreted in many ways,\n",
    "ranging from evidence about the nature of learning, to a simple consequence of\n",
    "randomness of eigenvalues of the Hessian of the loss function. A more recent\n",
    "observation is to compare training and testing accuracy on the same plot. In\n",
    "[9, 110] it was argued that these two metrics improve at two distinct stages of\n",
    "training. First, the model memorizes training examples. Later, it generalizes\n",
    "to the testing examples. This “grokking” phenomenon has been suggested as\n",
    "evidence for learning of circuits [103], an idea we discuss in §7.\n",
    "5 Simpler language models\n",
    "Here we describe a few generative language models in detail to fix the concepts.\n",
    "As points of notation, let Wbe the set of words (or, if the reader prefers,\n",
    "numbers which index a position in a list of words). We denote the cardinality of\n",
    "a setSas|S|, so|W|is the number of distinct words. The space of N-component\n",
    "real vectors is denoted RN.\n",
    "The simplest model is the N-gram model defined in terms of the conditional\n",
    "probabilities\n",
    "P(wN|w1w2. . . w N−1), (5)\n",
    "which are all taken to be independent. Given this minimalist assumption, a\n",
    "plausible way to estimate them from the corpus is\n",
    "P(wN|w1w2. . . w N−1) =Number of occurrences of w1w2. . . w N−1wNP\n",
    "wNumber of occurrences of w1w2. . . w N−1w.\n",
    "(6)\n",
    "This simple model with N= 3 or 4 works better than one might think (see exam-\n",
    "ples in [64]) and can be improved a bit by simple statistical tricks (“smoothing”).\n",
    "But the exponential growth of the number of strings in Nmeans that there is\n",
    "no hope of taking Nlarge enough to model even a single paragraph. The entire\n",
    "internet contains (in order of magnitude) 1012words, and such a corpus will\n",
    "contain only a vanishingly small fraction of the likely twenty word strings.27\n",
    "A more general principle which we can take from the N-gram model is the dis-\n",
    "tributional hypothesis, which has been pithily summarized as “you shall know\n",
    "a word by the company it keeps.” [44] In other words, by proper use of the\n",
    "statistics of neighboring words, one can define quantities which capture prop-\n",
    "erties and even the meanings of words. The simplest expression of this idea\n",
    "26This is roughly the time in which the gradient descent operates, see Eq. 16. In LLMs one\n",
    "often considers each data item only once in a training run, so it is related to (but different\n",
    "from) dataset size.\n",
    "27Statistical estimates of perplexity are in the 100’s, and the best current LLMs have per-\n",
    "plexity ∼20.\n",
    "15is the co-occurrence matrix. Before explaining this, let us mention a detail of\n",
    "practical systems, which in place of words use “tokens,” meaningful components\n",
    "of words. A physics illustration is the word “supersymmetrization.” Even for a\n",
    "non-physicist reader encountering it for the first time, this word naturally breaks\n",
    "up into “super,” “symmetry” and “ization,” pieces which appear in many words\n",
    "and which are called tokens. And not only does this decomposition apply to\n",
    "many words, it helps to understand their meaning. This process of replacing\n",
    "single words by strings of tokens (“tokenization”) is a first step in LLM pro-\n",
    "cessing, and henceforth when we say “word” we will mean word or token in this\n",
    "sense.\n",
    "Given a corpus, we define its N-gram co-occurrence matrix MNto be the\n",
    "|W| × |W| matrix whose ( w, w′) entry counts the number of N-grams in the\n",
    "corpus containing both words. This matrix defines a map from words to vectors\n",
    "ι:W →Rp(7)\n",
    "(where the dimension p=|W|), by taking a word to the corresponding column\n",
    "ofMN. Such a map is called a word embedding.\n",
    "Applying this map to each word independently, we can map a string of k\n",
    "words (in Wk) to a string of vectors, and this is the next step (after tokenization)\n",
    "of LLM processing. One might worry that these are very high dimensional\n",
    "vectors with many zero entries, which seems wasteful. A standard statistical\n",
    "cure for this problem is to do principal component analysis (PCA). In words,\n",
    "instead of columns of MNwe use the columns of a p×|W| matrix Zchosen such\n",
    "thatZtZis the best rank papproximation to MNin the sense that it minimizes\n",
    "tr (ZtZ−MN)2. One can do better, but this gives the right idea.\n",
    "Next, we feed this string of vectors into some machine learning model to get\n",
    "an output which we use to predict the next word. If we just want the most\n",
    "likely next word, a good way is to output a vector v∈Rp, and choose the\n",
    "word wwhich maximizes the inner product v·ι(w). We denote this relationship\n",
    "asv∼ι(w). More generally, the standard inverse map from a vector to a\n",
    "probability distribution on words is the Boltzmann distribution on the inner\n",
    "products. Explicitly, we postulate an inverse temperature β= 1/Tand take28\n",
    "v→P(w) =eβv·ι(w)\n",
    "P\n",
    "w′eβv·ι(w′)(8)\n",
    "Here is an observation [99] which supports the idea that word embeddings\n",
    "contain information about meaning. Since the embeddings are vectors, they can\n",
    "be added. Consider the following equation:\n",
    "ι(king) −ι(man) + ι(woman) ∼ι(?) (9)\n",
    "28Tis the temperature parameter which can be set in (say) the GPT user interface. Also,\n",
    "this ratio of exponentials is usually called “softmax” in machine learning as its β→ ∞ limit\n",
    "is the “argmax” function producing a vector whose nonzero components have the same index\n",
    "values as the largest of the input(s).\n",
    "16One might hope that the word which maximizes this inner product is “queen,”\n",
    "and indeed it is so. There are many more such examples; empirically one needs\n",
    "the dimension p≳100 for this to work. One can argue [108, 5] that it follows\n",
    "from relations between co-occurence statistics:29\n",
    "∀w,MN(w,king) /#(king)\n",
    "MN(w,queen) /#(queen)≈MN(w,man) /#(man)\n",
    "MN(w,woman) /#(woman)(10)\n",
    "Given these ideas and a map Ffrom a list of vectors to a vector, we can\n",
    "now propose a very general class of L-gram autoregressive language models as\n",
    "the combination of the following steps:\n",
    "1. Map the Linput words witoLvectors ι(wi).\n",
    "2. Apply Fto the list of these vectors to get a prediction vector v.\n",
    "3. Use the inverse map Eq. 8 to get a probability distribution over words.\n",
    "Furthermore, if the map Fhas parameters, given a corpus we can determine\n",
    "them by optimizing the function Eq. 3 with respect to the parameters. And once\n",
    "we bring in optimization, we can also optimize with respect to the coefficients of\n",
    "the embedding map Eq. 7, so that we can dispense with co-occurence statistics.\n",
    "This is the general prescription followed by the LLMs, and to complete it we\n",
    "just need to specify a family of maps F. One possibility is to use a general (fully\n",
    "connected) feed forward neural network (FFN, also called MLP for multilayer\n",
    "perceptron). We recall that an FFN is a composition of two general types of\n",
    "functions, linear maps Wiand nonlinear maps θ, so that\n",
    "F(v) =Wd◦θ◦Wd−1◦θ◦. . .◦W1◦θ◦W0. (11)\n",
    "In more concrete terms, the maps Wiare multiplication by rectangular matrices\n",
    "of parameters (usually called “weights” in this context), while the maps θact\n",
    "independently on each component of their input vector by a fixed nonlinear\n",
    "function such as tanh or (more typically) ReLU (identity for x≥0 and zero\n",
    "forx <0). The main fact we recall about FFN’s is that, in the limit that the\n",
    "number of parameters becomes large, they can approximate any given function\n",
    "arbitrarily well [38]. We refer the reader interested in learning more to [40, 116].\n",
    "We can get a very natural deep learning version of the L-gram models by\n",
    "using an FFN for the map Fin the prescription above [18]. Since this asked for\n",
    "a map from a list of vectors to a vector, we need to convert the input list into a\n",
    "single vector. This is easy: we can take the direct sum of the input vectors, i.e.\n",
    "the dimension L×pvector whose components are the concatenated lists of their\n",
    "components. Using today’s FFNs, one could implement this with L∼100 or so.\n",
    "There does not seem to be much work on large fully connected FFN language\n",
    "models, because by the time the technology advanced to this point the far more\n",
    "efficient transformer models had taken over. Still, they illustrate the general\n",
    "29Here #( w) denotes the number of occurences of “ w” in the corpus. These ratios can also\n",
    "be expressed in terms of the pairwise mutual information, PN(w, u)/P(w)P(u).\n",
    "17idea and also one of its most obvious limitations. Even with L∼100, often\n",
    "predicting the next word requires remembering words which appeared farther\n",
    "back. To solve this problem we need to incorporate some sort of memory into\n",
    "the model.\n",
    "The simplest memory is an additional state variable which is updated with\n",
    "each word and used like the other inputs. To do this, we should take the state\n",
    "to be a vector in some Rq. This brings us to the recurrent neural network or\n",
    "RNN. Its definition is hardly any more complex than what we saw before. With\n",
    "each word position (say with index i) we will associate a state vector siwhich\n",
    "can depend on words up to wiand on the immediately previous state. Then,\n",
    "we let the map Fdetermine both the next word and the next state as\n",
    "(vi+1, si+1) =F(si, vi, vi−1, vi−2, . . . , v i−k+1), (12)\n",
    "where the parenthesis notation on the left hand side means that the output\n",
    "vector of Fis the concatenation of two direct summand output vectors.\n",
    "Mathematically, Eq. 12 is a discrete dynamical system. If we grant that\n",
    "Fcan be an arbitrary map, this is a very general class of systems. One way\n",
    "of characterizing its generality is through computational complexity theory, by\n",
    "asking what classes of computation it can perform. In [123] it was argued that\n",
    "the RNN is a universal computer, but this granted that the computation of F\n",
    "in Eq. 11 could use infinite precision numbers. Under realistic assumptions\n",
    "the right complexity class is a finite state machine, which can recognize regular\n",
    "languages [26, 134]. We will say more from this point of view in §7.\n",
    "There are many variations on the RNN such as LSTM’s [57], each with their\n",
    "own advantages, but we must move on.\n",
    "6 Recipe for an LLM\n",
    "We are now ready to define the transformer model.30It is simply another class\n",
    "of maps Ffrom lists of vectors to a vector to be used in the prescription above.\n",
    "Indeed, it is a natural generalization of the FFN which is associated to permu-\n",
    "tational symmetry. This is in direct analogy to the use of convolutional neural\n",
    "networks (CNNs) for image recognition, which are FFNs which are equivariant\n",
    "under the symmetry of translations in two dimensions which is natural for the\n",
    "set of images.\n",
    "A transformer is a composition of two types of functions (layers) taken in\n",
    "alternation, each mapping an input list of Lvectors {ui}to an output list of L\n",
    "vectors {vi}. One of these is an FFN as previously discussed, but now applied\n",
    "to each embedding vector independently, so vi=FFFN(ui).\n",
    "30Other reviews explaining these definitions include [109, 131].\n",
    "18The other layer type is called attention, and it is defined as follows:\n",
    "{ui} → { vi=WiX\n",
    "j=1ci,juj} (13)\n",
    "ci,j≡expui·B·ujPi\n",
    "j=1expui·B·uj(14)\n",
    "where Bis a learnable matrix whose elements are model parameters (equiva-\n",
    "lently, u·B·vis a bilinear form) and Wis a linear map (also learnable).\n",
    "In words, an item viin the output vector is (a linear transformation of) a\n",
    "weighted sum of the inputs ujwith i≤jand can depend on any of them.31\n",
    "The weights ci,jare given by a “softmax” or Boltzmann weight just as in Eq.\n",
    "8. Thus there is a very general learnable way for each output to choose which\n",
    "of the input vectors are most useful as inputs. Suppose the product u·B·vis\n",
    "the dot product, then attention selects the input components ujmost similar\n",
    "to the current unit’s input, uj∼uiin the notation of §5. The matrix Ballows\n",
    "for comparing different parts of the embeddings and ignoring other parts, in a\n",
    "way determined by optimizing the objective function Eq. 3.\n",
    "Composing these two types of functions (or layers) produces a map from\n",
    "Rp×LtoRp×L. Often one takes, instead of the pure FFN and attention func-\n",
    "tions, sums of these with the identity function (residual connections). The FFNs\n",
    "generally have a single hidden layer which can be of a different dimension, call\n",
    "thisph.32Finally, while the language model prescription asked for a map to Rp,\n",
    "this is easily obtained by just taking the last vector in the final output list.\n",
    "There are two more essential details to cover (and many minor details we\n",
    "will skip). The first is the concept of “attention head.” The definition Eq. 13\n",
    "allowed for a general linear transformation Wwhose range is the output vector.\n",
    "We are free to choose its dimension, call it q, and typically one takes this to\n",
    "be much less than the embedding dimension p. In return one can use many\n",
    "copies of Eqs. 13,14 in parallel with different choices for BandW, to produce\n",
    "many outputs. One can then concatenate these outputs to get a final output of\n",
    "dimension p. These copies are called attention heads and we will denote their\n",
    "number by H, sop=Hq.\n",
    "The second essential detail is that, so far, there is nothing in the definition\n",
    "that keeps track of the order of the list of input vectors; the output of Eq. 13\n",
    "will be invariant under a general permutation of the input vectors. While this\n",
    "is an elegant property, it is not what we want for processing language, for which\n",
    "the order of the words matters. The cure for this is very simple: one takes as\n",
    "inputs not the word embeddings Eq. 7, but the direct sum (concatenation) of\n",
    "these with positional embedding vectors, i.e.vectors which encode the position\n",
    "(index) of the word in the string. These can be a combination of sines and\n",
    "31The restriction i≤jto previous or current inputs is done to get an autoregressive model;\n",
    "one can relax this for other purposes.\n",
    "32Explicitly, vi=W1·max(0 , W0·ui+b0) +b1, where b0,1are more learnable parameters.\n",
    "19cosines of various frequencies, such as [132]\n",
    "(e2i−1, e2i) = ( cosposition\n",
    "100002i/dpos,sinposition\n",
    "100002i/dpos); i∈ {1, . . . ,dpos\n",
    "2}(15)\n",
    "One could instead treat these vectors as learnable parameters. Still, the trig\n",
    "function basis for positions may be significant. It has been generalized to rep-\n",
    "resent other graph structures by using eigenfunctions of the graph Laplacian as\n",
    "positional embeddings.\n",
    "The invariance of the transformer model under permutation symmetry is\n",
    "reminiscent of the point we mentioned earlier, that translation symmetry mo-\n",
    "tivates the CNN. However permutation symmetry is badly broken in language,\n",
    "even in the simplest formal languages,33and it is not obvious why this should\n",
    "be a useful property for the model to have. One might argue that although any\n",
    "particular language breaks permutation symmetry, it acts naturally on the en-\n",
    "semble of languages and thus should have a simple representation. For example,\n",
    "besides the usual infix arithmetic notation “ a+b”, one could instead use prefix\n",
    "“+a b” or postfix “ a b+”. Translating between these notations is arguably\n",
    "easier for permutation invariant maps using position embeddings. An oppos-\n",
    "ing view would be that permutation symmetry is just a secondary property of\n",
    "the simplest model using attention, and that the main point is to explain the\n",
    "value of attention. In addition to its ability to select similar items, it provides\n",
    "a simple way to take products of embedding vectors. In computational com-\n",
    "plexity terms, attention enlarges the class of circuits which can be simulated by\n",
    "a constant depth transformer [28, 41, 95, 96]. Physics analogies of Eqs. 13,14,\n",
    "especially to the Hopfield model, may be important [59, 115].\n",
    "A major practical advantage of the transformer over the RNN and other\n",
    "previous architectures is that the computations in the attention mechanism can\n",
    "be done in parallel, so (given sufficiently many processors) the time required does\n",
    "not increase with the window length L. This is by contrast with the RNN in\n",
    "which information propagates from one word to the next, so a window of length\n",
    "Lrequires time Lto process. On the other hand the ability of each unit to pay\n",
    "attention to every previous unit means that the total computation required by\n",
    "the transformer scales as L2. This is the limiting factor for increasing Land\n",
    "this is widely seen as a problem. There has been a lot of work to improve this\n",
    "scaling, by removing some of the connections (as in sparse attention [30]), by\n",
    "introducing multiscale structure, or in other ways.\n",
    "Let us summarize by listing the hyperparameters34and their values for the\n",
    "largest (175B) GPT-3 [22]. They are\n",
    "•Embedding dimension p= 12288 and hidden layer dimension ph= 4p.\n",
    "•Window length L= 4096 or 8192.\n",
    "•Depth D= 96, counting both FFN (Eq. 11) and attention (Eq. 13)\n",
    "layers.35\n",
    "33Compare the logical implications A→BandB→A.\n",
    "34This term refers to model choices which are not learned through gradient descent.\n",
    "35Some of the attention layers in GPT-3 are sparse.\n",
    "20•Number of heads H= 96 (the equality with Dis a coincidence as far as I\n",
    "know).\n",
    "The total number of parameters is roughly 12 Dp2.\n",
    "As mentioned earlier, all of these parameters, and the parameters of the em-\n",
    "bedding map Eq. 7, are determined as follows. One generally starts with “ran-\n",
    "dom” initial conditions, usually meaning that each parameter is drawn from a\n",
    "normal distribution with mean zero and variance chosen so that the linear maps\n",
    "have expected norm independent of the hyperparameters. As in random matrix\n",
    "theory, this typically means var( Wi,j)∼1/p, though there are refinements [140].\n",
    "One then sequences through the training corpus and performs a step of gradient\n",
    "descent of Eq. 3 for each “batch” of words (here a group of ∼106words). In\n",
    "each step, the parameters ⃗θare modified as\n",
    "⃗θ→⃗θ−η∂Lb\n",
    "∂⃗θ(16)\n",
    "where Lbis Eq. 3 restricted to the batch, the conditional probability Pcomes\n",
    "out of Eq. 8 applied to the output of the transformer, and ηis a positive real\n",
    "number (the learning rate hyperparameter, here around 10−4).\n",
    "The result of following this procedure on a dataset of natural language text,36\n",
    "supplemented by many enhancements which are described in the literature and\n",
    "in the model source codes but which may be less important for conceptual\n",
    "understanding, is an LLM with the capabiliities we described.\n",
    "7 Studying the internal workings\n",
    "The success of this procedure raises many questions. Some can be asked about\n",
    "more or less any ML model – for example, questions about when and how\n",
    "optimization of the objective function Eq. 3 achieves “good” local minima\n",
    "(value near the global minimum and models which generalize well), and the\n",
    "origin of scaling laws like Eq. 4. These are the subject of the general theory of\n",
    "machine learning, for which we refer to [19, 97, 116] and much other work.\n",
    "Other questions, and understanding the many striking abilities discussed\n",
    "earlier, sound more specific to LLMs. What would it mean to understand how\n",
    "ChatGPT writes poetry based on prompts, or solves physics word problems?\n",
    "At present this is by no means clear and it may be that entirely new concepts\n",
    "are needed to do this. Still, I share the belief that we can go very far towards\n",
    "understanding LLMs by building on previous work in computer science, ma-\n",
    "chine learning and AI, and many other fields. There is a well established field\n",
    "of statistical physics and ML [97] which will surely contribute. Physics ideas\n",
    "are also very relevant for tasks with spatial symmetry, such as image genera-\n",
    "tion [125] and recognition [35]. The unexpected mathematical simplicity of the\n",
    "36As always in ML it is important that the dataset be “clean” – consistently tokenized, not\n",
    "having too much garbage text or repetitions, etc.. Many later LLMs also use programming\n",
    "language code in the dataset. Besides making code generation possible, it has been reported\n",
    "that this improves performance on natural language reasoning tasks.\n",
    "21transformer model means that mathematical insights could be valuable. We can\n",
    "also follow approaches used in neuroscience, psychology, and cognitive science.\n",
    "An evident observation is that the paradigm of neuroscience – careful study\n",
    "of the microscopic workings of the system, following a reductionist philosophy\n",
    "– is far more practical for ML models than it is for human brains, as the micro-\n",
    "scopic workings are fully explicit. This is not to say that it is easy, as we still face\n",
    "the difficulty of extracting meaning from a system with billions of components\n",
    "and parameters. How could we do this for LLMs?\n",
    "One familiar starting point in neuroscience is to measure the activity of\n",
    "neurons and try to correlate it with properties of the system inputs or outputs.\n",
    "The “grandmother cell” which fires when a subject sees his or her grandmother\n",
    "is an extreme (and controversial) example. Better established are the “place\n",
    "cells” in the hippocampus which fire when an animal passes through a specific\n",
    "part of its environment.\n",
    "Generally there is no reason why the representation should be so direct; there\n",
    "might be some “neural code” which maps stimuli onto specific combinations\n",
    "or patterns of activity. The details of the neural code could even be different\n",
    "between one individual and the next. Analogous concepts in LLMs are the maps\n",
    "from input strings to intermediate results or “activations.” The first of these\n",
    "is the embedding map Eq. 7. Considering each layer in succession, its outputs\n",
    "(sometimes called “contextualized embeddings”) also define such a map. The\n",
    "details of these maps depend on details of the model, the training dataset and\n",
    "the choices made in the training procedure. Besides the hyperparameters, these\n",
    "include the random initializations of the parameters, the order in which data\n",
    "items are considered in training and their grouping into batches. Even small\n",
    "differences can be amplified by the nonlinear nature of the loss landscape.\n",
    "One way to deal with this indeterminacy is to look for structure in the maps\n",
    "which does not depend on these choices. The linear relations Eq. 9 between word\n",
    "embeddings are a very elegant example, telling us (and presumably the model)\n",
    "something about the meanings of the words they represent. Moving on to the\n",
    "later layers, one can ask whether contextualized embeddings carry information\n",
    "about the grammatical role of a word, about other words it is associated to\n",
    "(such as the referent of a pronoun), etc.. One can go on to ask whether any\n",
    "of the many structures which – one would think – need to be represented to\n",
    "understand the real world, are visible in these embeddings.\n",
    "Many structures are too intricate to show up in linear relations. A more\n",
    "general approach is to postulate a “target” for each training data item and\n",
    "train a “probe” model (usually an FFN) to predict it from the embeddings. If\n",
    "this works, one can go on to modify the internal representation in a minimal way\n",
    "which changes the probe prediction, and check if this leads to the corresponding\n",
    "effects on the output (see [12] and references there).\n",
    "This procedure is simpler to explain in an example. A pretty example of\n",
    "probing for a world model is the recent work of Li et al [78] (see also [130]) on\n",
    "representations in a transformer model trained to play the board game Othello.37\n",
    "37For readers not familiar with this game, two players alternate in placing black and white\n",
    "22They train a model “Othello-GPT”38to take as input a sequence of 60 legal\n",
    "moves, for example “E3 D3 ...” in the standard algebraic notation, and at each\n",
    "step to predict the next move. The trained model outputs only legal moves\n",
    "with very high accuracy, and the question is whether this is done using internal\n",
    "representations which reflect the state of the game board, say the presence\n",
    "of a given color tile in a given position. Following the probe paradigm, they\n",
    "obtain FFNs which, given intermediate activations, can predict whether a board\n",
    "position is occupied and by which color tile. Furthermore, after modifying\n",
    "the activations so that the FFN’s output has flipped a tile color, the model\n",
    "predicts legal moves for the modified board state, confirming the identification.\n",
    "Neuroscientists can only dream of doing such targeted experiments.\n",
    "Numerous probe studies have been done on LLMs. One very basic ques-\n",
    "tion is how they understand grammatical roles and relations such as subject,\n",
    "object and the like. This question can be sharpened to probing their internal\n",
    "representations for parse trees, a concept we review in the appendix. To get\n",
    "the targets for the probe, one can use a large dataset of sentences labeled with\n",
    "parse trees, the Penn Treebank [90]. This was done for BERT in [27, 56, 88] by\n",
    "the following procedure: denote the embedding (in a fixed layer) of word ias\n",
    "ui, then the model learns a projection Pon this space, such that the distances\n",
    "d(i, j)≡ ||P(ui−uj)||in this inner product well approximate the distance be-\n",
    "tween words iandjdefined as the length of the shortest path connecting them\n",
    "in the parse tree. For BERT (with d∼1000) this worked well with a projection\n",
    "Pof rank ∼50.\n",
    "Once one knows something about how information is represented by the\n",
    "models, one can go on to try to understand how the computations are done. One\n",
    "approach, also analogous to neuroscience, is to look for specific “circuits” which\n",
    "perform specific computations. An example of a circuit which appears in trained\n",
    "transformer models is the induction head [42, 107]. This performs the following\n",
    "task: given a sequence such as “ A B . . . A ” it predicts a repetition, in this\n",
    "example “ B.” The matching between the tokens (the two A’s in the example) is\n",
    "done by attention. A number of works have proposed and studied such circuits,\n",
    "with various motivations and using various theoretical lenses: interpretability\n",
    "and LLMs [106], in-context learning [107, 2], formal language theory [94, 28],\n",
    "computational complexity theory [41, 82], etc..\n",
    "Reverse engineering a large network ab initio ,i.e.with minimal assumptions\n",
    "about what it is doing, seems challenging, but maybe automated methods will be\n",
    "developed [33, 46]. Another approach is to first develop a detailed computational\n",
    "model (CM) to perform a task without looking too much at the system under\n",
    "study, and then look for evidence for or against the hypothesis that the system\n",
    "under study uses it. This approach also has a long history in neuroscience [91]\n",
    "and ways to test such hypotheses have been much discussed. As an example\n",
    "tiles on an 8 ×8 board, and each move results in “flipping” some opponent pieces to the\n",
    "player’s color. The main point for us is that the function from moves to board state is easily\n",
    "computable yet very nonlocal and nonlinear.\n",
    "38While this model shares the GPT architecture, it is not trained on any language data,\n",
    "just on Othello games.\n",
    "23of a research tactic which does not require opening the black box, one can\n",
    "consider illusions which fool the system in some way. The response to these\n",
    "will often depend on contingent and non-optimal aspects of the model, so one\n",
    "can distinguish different models which solve the same task. A new class of\n",
    "predictions which becomes testable for LLMs is to look at performance as a\n",
    "function of model size (depth; number of parameters). A particular CM might\n",
    "require a certain model size or dataset properties in order to perform well. And\n",
    "of course, one can open the black box: by assuming a particular CM, one can\n",
    "make predictions for what probe experiments should work.\n",
    "Simple tasks studied in this approach include modular addition [103] and\n",
    "linear regression [2], where several CM’s (gradient descent, ridge regression and\n",
    "exact least squares) were compared. Turning to language processing, a CM for\n",
    "parsing by transformer LLMs was developed in Zhou et al [143]. While this\n",
    "is too lengthy to explain in detail here, let us give the basic idea, starting\n",
    "from the PCFG framework discussed in the appendix. Rather than try to\n",
    "represent a parse tree in terms of nodes and edges, it is represented by giving\n",
    "each position iin the list of words a set of variables αi,t,j, where tindexes a\n",
    "nonterminal (a left hand side of a rule) and jis another position. If αi,t,jis\n",
    "turned on, this means that a rule with ton the l.h.s. was used to generate\n",
    "that part of the tree stretching from position ito position j. This can be\n",
    "generalized to let αi,t,jbe the probability that a rule is used. These variables\n",
    "(and additional variables βdescribing the rules used higher in the tree) satisfy\n",
    "simple recursion relations (the Inside-Outside parsing algorithm [87]). If the\n",
    "rules have at most two symbols on the r.h.s.,39these recursion relations are\n",
    "quadratic in the variables. By encoding the αvariables as components of the\n",
    "embedding, they can be implemented using attention.\n",
    "Naively, this model predicts that embedding dimension pmust be very large,\n",
    "of order the number of nonterminals times the length of a sentence. Since\n",
    "realistic grammars for English have many hundreds of nonterminals, this seems\n",
    "to contradict the good performance of transformers with p∼1000. This problem\n",
    "is resolved by two observations, of which the first is that one can get fairly good\n",
    "parsing with many fewer ( ∼20) nonterminals. The second is compression, that\n",
    "embeddings and circuits which are simple and interpretable can be mapped into\n",
    "more “random-looking” lower dimensional forms. This is a well understood\n",
    "concept for metric spaces [92], which was implicit in the discussion of word\n",
    "embeddings in §5. There the simplest construction (the co-occurence matrix)\n",
    "produced vectors with one component for each word, but by projecting on a\n",
    "subspace one could greatly reduce this dimension with little loss in accuracy.\n",
    "The generalization of these ideas to neural networks seems important.\n",
    "Once one believes an LLM is carrying out a task using a particular circuit\n",
    "or CM, one can go on to ask how it learned this implementation from the data.\n",
    "One can get theoretical results in the limit of infinite training data and/or for\n",
    "simple tasks in which the dataset is constructed by a random process. Learning\n",
    "39One can rewrite any grammar to have this property (Chomsky normal form) by introduc-\n",
    "ing more nonterminals.\n",
    "24in transformer models trained on realistic amounts of data is mostly studied\n",
    "empirically and using synthetic data. A few recent interesting works are [2, 51,\n",
    "103]. Intuitively one expects that simpler instances of a task are learned first,\n",
    "allowing the model to learn features which are needed to analyze more complex\n",
    "instances, and there is a lot of evidence for this. The idea that many submodels\n",
    "can be learned simultaneously, including straight memorization and submodels\n",
    "which rely on structure, also seems important. Ultimately learnability is crucial\n",
    "but we should keep in mind that in analogous questions in physics, evolution,\n",
    "and so on, it is much easier to understand optimal and critical points in the\n",
    "landscape than to understand dynamics.\n",
    "This brings us to in-context learning, the ability of an LLM to perform\n",
    "diverse tasks given only a few examples of input-output pairs. The simplest\n",
    "hypothesis is that the model has learned the individual tasks, and the examples\n",
    "are selecting a particular task from this repertoire. It has been argued that\n",
    "this is guaranteed to happen (in the infinite data limit) for a model trained\n",
    "on a mixture of tasks [139, 136]. If the many tasks have common aspects (for\n",
    "example parsing might be used in any linguistic task), one can ask how the\n",
    "model takes advantage of this, a question discussed in [54].\n",
    "Understanding LLMs is a very active research area and there is much more\n",
    "we could say, but let us finish by summarizing the two main approaches we\n",
    "described. One can postulate a representation and a computation designed to\n",
    "perform a task, and look for evidence that the LLM actually uses the postu-\n",
    "lated structure. Alternatively, one can look for a function in some simpler class\n",
    "(such as digital circuits) which well approximates the function computed by the\n",
    "transformer model, and then “reverse engineer” the simpler function to find\n",
    "out what it is doing. Either or both of these procedures could lead to inter-\n",
    "pretable systems and if so, are answers to the question “what has the LLM\n",
    "learned.” There is no guarantee that they will work and it might turn out that\n",
    "one cannot understand LLMs without new ideas, but they deserve to be tried.\n",
    "8 Questions and discussion\n",
    "Large language models have revolutionized computational linguistics and opened\n",
    "up many new applications of AI. Understanding how they work is both straight-\n",
    "forward (we explained it in §6) and at the same time an outstanding scientific\n",
    "challenge. This is because the question “how do they work” has multiple mean-\n",
    "ings. On the one hand, LLMs are a relatively simple solution to the task of\n",
    "predicting the likely next word in a text. On the other hand, they also seem to\n",
    "perform many other tasks which require intelligence, such as solving the physics\n",
    "word problem in Figure 1. While we do not have a strong understanding of\n",
    "what a system which can perform these tasks must do, a vast body of work in\n",
    "cognitive science and AI supports one’s first naive intuition that such a system\n",
    "must be doing sophisticated analyses of language, must contain models of the\n",
    "real world, and must be able to do fairly general logical reasoning. Before it\n",
    "was demonstrated, the idea that all this could be learned as a byproduct of\n",
    "25word prediction would have seemed hopelessly optimistic, had anyone dared to\n",
    "suggest it.\n",
    "Extraordinary claims should be greeted with skepticism. One must guard\n",
    "against the possibility that a successful ML system is actually picking up on\n",
    "superficial aspects or statistical regularities of the inputs, the “clever Hans”\n",
    "effect. Addressing this is an important function of the benchmark evaluations\n",
    "discussed in §4. Of course as LLMs get good at performing tasks of practical\n",
    "value, the skeptical position becomes hard to maintain.\n",
    "Intelligence and language are incredibly complex and diverse. According to\n",
    "Minsky,40this diversity is a defining feature of intelligence. The goal of under-\n",
    "standing LLMs (or any general AI) will not be accomplished by understanding\n",
    "all of the content in their training data, the “entire internet.” Rather, the trick\n",
    "we need to understand is how a single system can learn from this diverse corpus\n",
    "to perform a wide range of tasks. Theories of “what is learnable” are a central\n",
    "part of computer science [68]. Although theoretical understanding has a long\n",
    "way to go to catch up with LLM capabilities, for simpler and better understood\n",
    "tasks much is known.\n",
    "In these notes we mostly looked at this question through the lens of computer\n",
    "science, and took as the gold standard for explaining how an LLM learns and\n",
    "performs a task, a computational model expressed as an algorithm or a circuit\n",
    "together with arguments that the trained LLM realizes this model. This point of\n",
    "view has many more insights to offer, but before we discuss them let us consider\n",
    "some other points of view. In §7 we drew the analogy between detailed study\n",
    "of transformer circuits and neuroscience – what others can we consider?\n",
    "Another analogy is with cognitive psychology. LLMs are sufficiently human-\n",
    "like to make this interesting, and there is a growing literature which applies tests\n",
    "and experimental protocols from psychology to LLMs, see for example [53] and\n",
    "the many references there. When discussing this, we should keep in mind the\n",
    "vast differences between how humans and LLMs function. Human brains are\n",
    "not believed to use the backpropagation learning algorithm, indeed it has been\n",
    "argued that biological neural systems cannot use it [37]. Perhaps related to this,\n",
    "brains are not feed-forward networks but have many bidirectional connections.\n",
    "Whatever brains are doing, it works very well: LLMs (like other current deep\n",
    "learning systems) need far more training data than humans. Furthermore, the\n",
    "LLMs we discussed do not interact with the world. Some argue that on philo-\n",
    "sophical grounds, a model trained only on language prediction can never learn\n",
    "meaning [16]. While I do not find this particular claim convincing, I agree that\n",
    "we should not assume that LLMs perform tasks the same way humans do. Still\n",
    "both similarities and differences are interesting; can we make the analogies with\n",
    "cognitive psychology more precise?\n",
    "One analogy [17, 50], is with the well known concept of “fast and slow think-\n",
    "ing” in behavioral psychology [66]. To summarize, humans are postulated to\n",
    "have two modes of thought, “system 1” which makes fast, intuitive judgments,\n",
    "40What magical trick makes us intelligent? The trick is that there is no trick. The power\n",
    "of intelligence stems from our vast diversity, not from any single, perfect principle. [100]\n",
    "26and “system 2” which can focus attention and do calculations, logic, and plan-\n",
    "ning. While system 2 is more general and less error-prone, using it requires\n",
    "conscious attention and effort. According to the analogy, LLMs implement sys-\n",
    "tem 1 thinking, and are weak at system 2 thinking.\n",
    "In [84] it is argued that LLMs have “formal linguistic competence” but not\n",
    "“functional competence.” In plainer terms, they are solving problems by manip-\n",
    "ulating language using rules, but they lack other mechanisms of human thought.\n",
    "While it may be surprising that a purely rule-based system could do all that\n",
    "LLMs can do, we do not have a good intuition about what rule-based systems\n",
    "with billions of rules can do.\n",
    "What are the other mechanisms? There is a long-standing hypothesis in cog-\n",
    "nitive science, modularity of mind [45], according to which the human brain has\n",
    "many “mental modules” with different capabilities. These include a language\n",
    "module of the sort that Chomsky famously advocated and many others, includ-\n",
    "ing one for geometric and physical reasoning, another for social reasoning and\n",
    "theory of mind, and perhaps others. Notably, formal logic and mathematical\n",
    "reasoning seem to call upon different brain regions from those which specialize\n",
    "in language [3], suggesting that these functions are performed by different men-\n",
    "tal modules. One can thus hypothesize that LLMs have commonalities with the\n",
    "human language module and might be useful scientific models for it,41but that\n",
    "progress towards human level capability will eventually stall without analogs of\n",
    "the other modules. [73]\n",
    "A related claim is that current LLMs, even when they perform well on bench-\n",
    "marks, do not construct models of the world. Consider reasoning about spatial\n",
    "relations – for example if A is in front of B is in front of C, then A is in front of\n",
    "C. Such reasoning is greatly facilitated by representing the locations of objects\n",
    "in space, perhaps in terms of coordinates, perhaps using “place cells” or in some\n",
    "other non-linguistic way. If distance from the observer is explicitly represented\n",
    "and used in reasoning, then it becomes hard to get this type of question wrong.\n",
    "Conversely, to the extent that LLMs do get it wrong, this might be evidence\n",
    "that they lack this type of world model or cannot effectively use it.\n",
    "There are many papers exhibiting LLM errors and suggesting such inter-\n",
    "pretations, but often one finds that next years’ model does not make the same\n",
    "errors. At the present rate of progress it seems premature to draw any strong\n",
    "conclusions. My own opinion is that there is no barrier in principle to LLMs\n",
    "constructing internal non-linguistic models of the world, and the work [78] on\n",
    "Othello-GPT discussed in §7 is a nice demonstration of what is possible. This\n",
    "is not to say that any and all models can be learned, but rather that it might\n",
    "be better for now to focus on other significant differences between LLM and\n",
    "human reasoning, of which there are many. I will come back to this below.\n",
    "If LLMs and other connectionist systems do not work in the same way as\n",
    "brains, what other guidance do we have? In §7 we discussed one answer, the\n",
    "hypothesis that they work much like the algorithms and circuits studied in\n",
    "41Chomsky rejects this idea, saying that “The child’s operating system is completely differ-\n",
    "ent from that of a machine learning program.” (New York Times, March 8, 2023).\n",
    "27computer science. Perhaps trained LLMs implement algorithms like those de-\n",
    "signed by computational linguists, or perhaps new algorithms which were not\n",
    "previously thought up but which can be understood in similar terms. In either\n",
    "version this is still a hypothesis, but if we grant it we can draw on insights from\n",
    "theoretical computer science which apply to all such algorithms.\n",
    "Computational complexity theory [4, 137] makes many statements and con-\n",
    "jectures about how the time and space required by a particular computation\n",
    "depends on the size of the problem (usually meaning the length of the input).\n",
    "The most famous of these, the P̸=NPconjecture, states (very loosely) that for\n",
    "problems which involve satisfying general logical statements, finding a solution\n",
    "can be much harder than checking that the solution is correct.\n",
    "From this point of view, a central question is the complexity class of circuits\n",
    "which can be realized by constant depth transformers, meaning that the number\n",
    "of layers does not grow with the window size. Roughly, this is the complexity\n",
    "class TC0of constant depth circuits with threshold gates [28, 41, 95, 96]. Of\n",
    "course in an autoregressive LLM one can repeat this operation to compute a\n",
    "sequence of words: thus the circuit defines the transition function of a finite\n",
    "state machine (FSM) where the state is the window, and the LLM has learned\n",
    "to simulate this FSM. If a natural algorithm to perform a task is in a more\n",
    "difficult complexity class than the FSM can handle, this is a reason to think the\n",
    "task cannot be learned by this type of LLM. Conversely, one might conjecture\n",
    "that any task for which there is an algorithm in this class can be learned, at\n",
    "least in the limit of an infinite amount of training data.\n",
    "What about the lenses of pure mathematics, theoretical physics and allied\n",
    "fields? Besides my own personal interest in them, these fields have made sub-\n",
    "stantial contributions to statistics and machine learning, especially the interface\n",
    "between statistical physics and machine learning is a vibrant field of research\n",
    "[71, 97]. Spin glass theory made a very deep impact, starting with the Hopfield\n",
    "model and developing into a far-reaching theory of optimization landscapes and\n",
    "complexity. Random matrix theory is central to high dimensional statistics [63]\n",
    "and in many approaches to understanding deep learning [116]. Mathematical\n",
    "approaches to language such as [20, 34, 86, 89] can reveal new structure and\n",
    "provide deeper understanding.\n",
    "Another reason to think pure mathematics and theoretical physics have more\n",
    "to contribute is that neural networks, transformers, and many of the models of\n",
    "neuroscience, are formulated in terms of real variables and continuous mathe-\n",
    "matics. By contrast, computer science is largely based on discrete mathematics,\n",
    "appropriate for some but not all questions. Perhaps word embeddings have im-\n",
    "portant geometric properties, or perhaps the dynamics of gradient descent are\n",
    "best understood through the intuitions of continuous mathematics and physics.\n",
    "Arguments such as those in §7 which reduce neural networks to digital circuits,\n",
    "even if they do explain their functioning, may not be adequate to explain how\n",
    "they are learned.\n",
    "Having at least mentioned some of the many points of view, let me combine\n",
    "these insights and speculate a bit on where this is going. Let me focus on\n",
    "three capabilities which seem lacking in current LLMs: planning, confidence\n",
    "28judgments, and reflection.\n",
    "Planning, solving problems whose solution requires choosing a series of ac-\n",
    "tions and/or the consideration of future actions by other agents, is one of the\n",
    "core problems of AI. Making plans generally requires search, and in general\n",
    "search is hard (assuming P̸=NP). A familiar example is a chess program,\n",
    "which searches through a game tree to judge the longer term value of a candi-\n",
    "date move by hypothesizing possible future moves. While much of the success\n",
    "of AlphaGo and AlphaZero is attributed to reinforcement learning by self-play,\n",
    "they also search through game trees; indeed the Monte Carlo tree search algo-\n",
    "rithm on which they built [36, 23] was considered a key enabling breakthrough.\n",
    "By contrast, LLMs have no component dedicated to search. While it does\n",
    "not seem impossible that search trees or other structures could be learned inter-\n",
    "nally (like world models), it seems intuitively clear that an autoregressive model\n",
    "which predicts one word at a time and cannot go back to revise its predictions\n",
    "in light of what comes later will be seriously handicapped in planning. This\n",
    "observation is motivating a fair amount of current work on ways to incorporate\n",
    "search. LeCun has suggested adding a dynamic programming component to\n",
    "search through multiword predictions, as part of his “path towards autonomous\n",
    "machine intelligence” [75]. Another proposal, the “tree of thoughts” model [141],\n",
    "works with a search tree of LLM responses. A system which uses hierarchical\n",
    "planning for mathematical theorem proving was developed in [62].\n",
    "The next capability on my list, making and working with confidence judg-\n",
    "ments, has to do with the well known “hallucination” problem, that LLMs often\n",
    "simply invent statements, including untrue facts and imaginary citations. While\n",
    "advantageous for a poetry generator, and bearable for a system which makes\n",
    "suggestions which an expert human user will verify, this is a huge obstacle to\n",
    "many practical applications. Thus it is the subject of a great deal of research\n",
    "– a few of this month’s papers are [43, 79, 81]. Perhaps by the time you read\n",
    "these words there will have already been major progress.\n",
    "Why are LLMs producing these hallucinations? One intuition is that they\n",
    "are doing some sort of compression, analogous to JPEG image compression,\n",
    "which introduces errors [29]. This point of view suggests that the problem will\n",
    "eventually be solved with larger models and perhaps better training protocols\n",
    "which focus on the more informative data items [126].\n",
    "A related intuition is that the problems follow from inability to properly\n",
    "generalize. This comes back to the point about “world models” – a correct\n",
    "model, for example an internal encoding of place information, by definition\n",
    "correctly treats the properties being modeled. Now suppose we grant that the\n",
    "LLM is solving some class of problems, not by constructing such a model, but by\n",
    "rule-based reasoning. In other words, the LLM somehow learns rules from the\n",
    "corpus which it uses to make particular inferences which agree with the model.\n",
    "While such rules can cover any number of cases, there is no clear reason for such\n",
    "a rule set to ever cover all cases.\n",
    "Another intuition is that the training data contains errors and this is reflected\n",
    "in the results. Certainly the internet is not known for being a completely reliable\n",
    "source of truth. This intuition also fits with the observation that adding code\n",
    "29(computer programs) to the training set improves natural language reasoning.\n",
    "Code is a good source of rules because almost all of it has been debugged, leading\n",
    "to rules which are correct in their original context (of course they might not be\n",
    "correctly applied). It is a longstanding question whether internal representations\n",
    "(both in AI and in humans) are shared between different natural languages; it\n",
    "would be truly fascinating to know how much they are also shared with code.\n",
    "If this intuition is right, then LLMs reasoning capability might be improved by\n",
    "training on far more code and other content which is guaranteed to be correct.\n",
    "Such content could be generated synthetically as tautologies, or even better as\n",
    "formal verified mathematics (as proposed in [129]).\n",
    "Here is a different point of view: the problem is not that the systems make\n",
    "things up, after all creativity has value. Rather, it is that they do not provide\n",
    "much indication about the confidence to place in a particular output, and do\n",
    "not have ways to adapt their reasoning to statements known at different levels of\n",
    "confidence. Much of our reasoning involves uncertain claims and claims which\n",
    "turn out to be false, the point is to distinguish these from justified claims and\n",
    "keep track of our confidence in each belief. While it is possible to extract\n",
    "confidence scores from LLMs [65], there is also a philosophical point to make\n",
    "here: not all facts have the same epistemological status. Some facts are grounded\n",
    "in evidence; others are true by definition.\n",
    "LLMs are of course statistical models. Even for a completely deterministic\n",
    "task, say doing arithmetic, a statistical approach to learning is very powerful.\n",
    "This is because learning based on inputs which consist of finitely many training\n",
    "examples, given in a random order, is naturally formulated in statistical terms.\n",
    "But without making additional non-statistical assumptions, one can never go\n",
    "from almost 100% confidence to 100% confidence.\n",
    "This difference is crucial in many aspects of human thought. Of course,\n",
    "logical reasoning and mathematics stand out as prime examples. Long chains\n",
    "of reasoning are only possible if the individual links are reliable. But it is also\n",
    "crucial in social reasoning. There is an essential difference between statistical\n",
    "and evidence-based statements, say “Michael is a popular name,” and tauto-\n",
    "logical, definitional and descriptive statements such as “My name is Michael.”\n",
    "While the first statement might be a subject of discussion, a model which can\n",
    "get confused about the second statement is clearly missing a defining aspect of\n",
    "human thought, and will lose the confidence of its interlocutor. Perhaps episte-\n",
    "mological status and tautological correctness need to be somehow represented\n",
    "in the model. It need not be designed in, but the model needs to be given\n",
    "additional signals beyond next word prediction to learn it.\n",
    "The third point on my list, reflection, does not seem to be much discussed,\n",
    "but to me seems just as important. In computer science, reflection is the ca-\n",
    "pability of a system to work with its programs as a form of data [124, 138].\n",
    "This is naturally possible for a computer programmed in assembly language, in\n",
    "which instructions are encoded in integers. To some extent it is also possible\n",
    "in Lisp, in which programs are encoded in a universal list data structure. As\n",
    "type systems and other programming language refinements are introduced, re-\n",
    "flection becomes more difficult to provide, but it is necessary for systems-level\n",
    "30programming and makes various standard tasks easier to implement.\n",
    "Since an LLM operates on language, reflection for an LLM is the ability to\n",
    "work with its internal model in linguistic terms. This is related to ML inter-\n",
    "pretability, the ability to translate a model into understandable terms. In §7 we\n",
    "discussed interpretability of LLMs in terms of circuits and computational mod-\n",
    "els, implicitly leaving these for a human to interpret and understand. One can\n",
    "imagine an “interpretation engine” which given a model, automatically produces\n",
    "a more interpretable description, in terms of circuits, rules, or even a description\n",
    "of the model’s functioning in natural language. Given such an interpretation\n",
    "engine, by applying it to an LLM and sending its output as an input to the\n",
    "LLM, we can implement a form of reflection.\n",
    "A basic human capability which corresponds to this process is the translation\n",
    "from procedural or other implicit forms of memory to linguistic, explicit mem-\n",
    "ory. Very often, we learn by doing – riding a bicycle, solving math problems,\n",
    "interacting socially. We then reflect on what we have learned – in some uncon-\n",
    "scious way – and occasionally come up with verbal observations, summaries, in\n",
    "a word reflections. It is fascinating that combining the ideas we discussed brings\n",
    "us into contact with such topics.\n",
    "To conclude, and for what it is worth, out of the forty years I have followed\n",
    "AI, this is by far the most exciting period. I agree with those who think LLMs\n",
    "are a major milestone and believe the ideas behind them – including the trans-\n",
    "former architecture – will remain important even in the light of future progress.\n",
    "The questions they raise are interesting and important enough that – even as\n",
    "the specialists make remarkable progress – we need not leave the field to them,\n",
    "but as scientists and thinkers we should engage and try to contribute.\n",
    "A Grammars and parsing\n",
    "Most readers will have encountered the idea of “sentence diagram,” which graph-\n",
    "ically represents the decomposition of a sentence into clauses with a subject,\n",
    "verb and object, the assignment of adjectives and prepositional phrases to the\n",
    "nouns and verbs they modify, and so on. Formal versions of this concept are\n",
    "foundational in linguistics and computer science, and a short introduction (or\n",
    "review) is a good way to bring the general ideas we are discussing to life.\n",
    "A formal grammar can be given by a set of “production rules” which can\n",
    "be used to generate grammatical strings. A simple example is in Figure 3.\n",
    "Each line is a rule, which is made up of two strings of symbols separated by\n",
    "→, the left hand side or lhs and right hand side or rhs. These symbols can be\n",
    "“terminals” which appear in the language (such as +, ∗,x, 0 and so on in our\n",
    "example) or “nonterminals” which do not (such as TERM).\n",
    "These rules are used as follows: We start with a string Scontaining a dis-\n",
    "tinguished “start” symbol (here EXPR). We then iterate the following process:\n",
    "choose a rule whose lhs occurs in S, and apply it by substituting one occurrence\n",
    "of this lhs in Swith the rhs. Every string Swhich can be obtained by a finite\n",
    "sequence of these operations is considered grammatical, and by keeping track\n",
    "31EXPR →TERM + EXPR (17)\n",
    "EXPR →( EXPR )\n",
    "EXPR →TERM\n",
    "TERM →VALUE ∗TERM\n",
    "TERM →( EXPR )\n",
    "TERM →VALUE\n",
    "VALUE →x\n",
    "VALUE →y\n",
    "VALUE →1\n",
    "Figure 3: A grammar for arithmetic expressions.\n",
    "of the rule applications we get a parse tree. This is a graph whose nodes are\n",
    "symbols and whose edges connect a symbol which appears on the lhs of a rule\n",
    "application with the nodes for each of the symbols which appear on the rhs.42\n",
    "A good exercise is to work out the parse tree for the expression y+ 1∗xand\n",
    "check that multiplication takes precedence over addition.\n",
    "Our example of a grammar is a context-free grammar, meaning that the\n",
    "left hand side of each rule consists of a single symbol. If we do not put this\n",
    "restriction, the resulting class of languages are universal computers (and thus\n",
    "suffer from potential undecidability). There is also a more restricted class of\n",
    "grammars called regular grammars (this hierarchy was found by Chomsky), but\n",
    "these cannot describe nested structures such as the parentheses of Eq. 17. The\n",
    "context-free grammars are the right degree of complexity for many purposes. In\n",
    "particular, programming languages and the formal languages of mathematical\n",
    "logic can be described using CFG’s and thus the algorithms for working with\n",
    "them and associated theory are well developed.\n",
    "Besides recognizing and parsing languages, one can describe other linguistic\n",
    "tasks in similar terms. A trivial example would be word replacement, with\n",
    "rules such as OLD i→NEW i. Realistic tasks benefit from frameworks with\n",
    "more structure. For example, to use the grammar in Eq. 17 to do arithmetic,\n",
    "we would be much better off with a framework in which the token VALUE\n",
    "carries an associated numerical or symbolic value. This can be done with the\n",
    "framework of attribute grammars. When we suggest in §8 that LLMs perform\n",
    "natural language tasks using systems of large numbers of rules, we have this\n",
    "sort of extended grammatical framework in mind.\n",
    "CFG’s are not really adequate for natural languages, with their inherent\n",
    "ambiguity and their many special cases and exceptions. A more general for-\n",
    "malism is the probabilistic CFG. This is obtained by associating a probability\n",
    "distribution to each symbol which appears on the left hand side of a rule (the\n",
    "nonterminals). For example, we might stipulate that a VALUE has a 75% chance\n",
    "42One can see examples for English sentences in the Wikipedia article “Parse tree.”\n",
    "32to be a number and a 25% chance to be a variable. With this information, a\n",
    "PCFG defines a probability distribution on strings, which gives zero probability\n",
    "to nongrammatical strings.\n",
    "A symbolic approach to parsing would propose two primary algorithms. One\n",
    "is a parser, which given a grammar and an input produces the parse tree. An-\n",
    "other would be an algorithm for learning a grammar from a corpus. Since any\n",
    "finite corpus can be described by many grammars, PCFG’s are better suited\n",
    "than CFG’s to this problem. In any case the learning and parsing algorithms\n",
    "are not necessarily related.\n",
    "In the connectionist approach followed by LLMs, these two algorithms are\n",
    "subsumed into the definition of a model which can parse any PCFG whose rules\n",
    "are encoded in its weights. By training this on a corpus, the model learns a\n",
    "particular PCFG which generates the corpus. Interpretability as discussed in\n",
    "§7 then means reversing this relation, by extracting a parser and PCFG from\n",
    "the trained model.\n",
    "References\n",
    "[1] Reproduced under the cc by 4.0 license. https://creativecommons.org/\n",
    "licenses/by/4.0/ .\n",
    "[2] Ekin Aky¨ urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny\n",
    "Zhou. What learning algorithm is in-context learning? Investigations\n",
    "with linear models, November 2022. arXiv:2211.15661 [cs]. URL: http:\n",
    "//arxiv.org/abs/2211.15661 ,doi:10.48550/arXiv.2211.15661 .\n",
    "[3] Marie Amalric and Stanislas Dehaene. A distinct cortical network for\n",
    "mathematical knowledge in the human brain. NeuroImage , 189:19–31,\n",
    "April 2019. URL: https://www.sciencedirect.com/science/article/\n",
    "pii/S1053811919300011 ,doi:10.1016/j.neuroimage.2019.01.001 .\n",
    "[4] Sanjeev Arora and Boaz Barak. Computational complexity: a modern\n",
    "approach . Cambridge University Press, 2009.\n",
    "[5] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Ris-\n",
    "teski. A Latent Variable Model Approach to PMI-based Word Embed-\n",
    "dings. arXiv:1502.03520 [cs, stat] , June 2019. arXiv: 1502.03520. URL:\n",
    "http://arxiv.org/abs/1502.03520 .\n",
    "[6] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W.\n",
    "Ayers, Dragomir Radev, and Jeremy Avigad. ProofNet: Autoformalizing\n",
    "and Formally Proving Undergraduate-Level Mathematics, February 2023.\n",
    "arXiv:2302.12433 [cs]. URL: http://arxiv.org/abs/2302.12433 ,doi:\n",
    "10.48550/arXiv.2302.12433 .\n",
    "[7] Sebastian Bader and Pascal Hitzler. Dimensions of Neural-symbolic In-\n",
    "tegration - A Structured Survey, November 2005. arXiv:cs/0511042.\n",
    "33URL: http://arxiv.org/abs/cs/0511042 ,doi:10.48550/arXiv.cs/\n",
    "0511042 .\n",
    "[8] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh\n",
    "Sharma. Explaining Neural Scaling Laws. February 2021. URL: https:\n",
    "//arxiv.org/abs/2102.06701v1 .\n",
    "[9] Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran\n",
    "Malach, and Cyril Zhang. Hidden Progress in Deep Learning: SGD Learns\n",
    "Parities Near the Computational Limit, July 2022. arXiv:2207.08799 [cs,\n",
    "math, stat]. URL: http://arxiv.org/abs/2207.08799 ,doi:10.48550/\n",
    "arXiv.2207.08799 .\n",
    "[10] Peter L. Bartlett, Philip M. Long, G´ abor Lugosi, and Alexander Tsigler.\n",
    "Benign overfitting in linear regression. Proceedings of the National\n",
    "Academy of Sciences , 117(48):30063–30070, 2020. Publisher: National\n",
    "Acad Sciences.\n",
    "[11] Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learn-\n",
    "ing: a statistical viewpoint. arXiv:2103.09177 [cs, math, stat] , March\n",
    "2021. arXiv: 2103.09177. URL: http://arxiv.org/abs/2103.09177 .\n",
    "[12] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and\n",
    "advances. Computational Linguistics , 48:207–219, 2021. URL: http:\n",
    "//arxiv.org/abs/2102.12452 .\n",
    "[13] Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of\n",
    "deep learning through the prism of interpolation. arXiv:2105.14368 [cs,\n",
    "math, stat] , May 2021. arXiv: 2105.14368. URL: http://arxiv.org/\n",
    "abs/2105.14368 .\n",
    "[14] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Man-\n",
    "dal. Reconciling modern machine-learning practice and the classical\n",
    "bias–variance trade-off. Proceedings of the National Academy of Sci-\n",
    "ences , 116(32):15849–15854, 2019. Publisher: National Academy of\n",
    "Sciences eprint: https://www.pnas.org/content/116/32/15849.full.pdf.\n",
    "URL: https://www.pnas.org/content/116/32/15849 ,doi:10.1073/\n",
    "pnas.1903070116 .\n",
    "[15] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep\n",
    "learning we need to understand kernel learning. February 2018. URL:\n",
    "https://arxiv.org/abs/1802.01396v3 .\n",
    "[16] Emily M. Bender and Alexander Koller. Climbing towards NLU: On mean-\n",
    "ing, form, and understanding in the age of data. In Proceedings of the\n",
    "58th Annual Meeting of the Association for Computational Linguistics ,\n",
    "pages 5185–5198, Online, July 2020. Association for Computational Lin-\n",
    "guistics. URL: https://aclanthology.org/2020.acl-main.463 ,doi:\n",
    "10.18653/v1/2020.acl-main.463 .\n",
    "34[17] Yoshua Bengio. From system 1 deep learning to system 2 deep\n",
    "learning, December 2019. URL: https://slideslive.com/38922304/\n",
    "from-system-1-deep-learning-to-system-2-deep-learning .\n",
    "[18] Yoshua Bengio, R´ ejean Ducharme, and Pascal Vincent. A neural proba-\n",
    "bilistic language model. Advances in neural information processing sys-\n",
    "tems, 13, 2000.\n",
    "[19] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and\n",
    "machine learning , volume 4. Springer, 2006.\n",
    "[20] Tai-Danae Bradley, John Terilla, and Yiannis Vlassopoulos. An enriched\n",
    "category theory of language: from syntax to semantics. arXiv:2106.07890\n",
    "[cs, math] , June 2021. arXiv: 2106.07890. URL: http://arxiv.org/abs/\n",
    "2106.07890 .\n",
    "[21] Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Robert L\n",
    "Mercer, et al. The mathematics of statistical machine translation: Pa-\n",
    "rameter estimation. 1993.\n",
    "[22] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\n",
    "Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\n",
    "Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\n",
    "Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\n",
    "Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\n",
    "Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\n",
    "pher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\n",
    "Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\n",
    "[cs], June 2020. arXiv: 2005.14165. URL: http://arxiv.org/abs/2005.\n",
    "14165 .\n",
    "[23] Cameron B. Browne, Edward Powley, Daniel Whitehouse, Simon M. Lu-\n",
    "cas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez,\n",
    "Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree\n",
    "search methods. IEEE Transactions on Computational Intelligence and\n",
    "AI in games , 4(1):1–43, 2012.\n",
    "[24] S´ ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes\n",
    "Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi\n",
    "Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\n",
    "and Yi Zhang. Sparks of Artificial General Intelligence: Early experi-\n",
    "ments with GPT-4, March 2023. URL: https://arxiv.org/abs/2303.\n",
    "12712v1 .\n",
    "[25] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering\n",
    "Latent Knowledge in Language Models Without Supervision, December\n",
    "2022. arXiv:2212.03827 [cs]. URL: http://arxiv.org/abs/2212.03827 .\n",
    "35[26] Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin\n",
    "Knight. Recurrent Neural Networks as Weighted Language Recognizers,\n",
    "March 2018. arXiv:1711.05408 [cs]. URL: http://arxiv.org/abs/1711.\n",
    "05408 ,doi:10.48550/arXiv.1711.05408 .\n",
    "[27] Ethan A. Chi, John Hewitt, and Christopher D. Manning. Finding Uni-\n",
    "versal Grammatical Relations in Multilingual BERT. arXiv:2005.04511\n",
    "[cs], May 2020. arXiv: 2005.04511. URL: http://arxiv.org/abs/2005.\n",
    "04511 .\n",
    "[28] David Chiang, Peter Cholak, and Anand Pillay. Tighter Bounds on\n",
    "the Expressivity of Transformer Encoders, May 2023. arXiv:2301.10743\n",
    "[cs]. URL: http://arxiv.org/abs/2301.10743 ,doi:10.48550/arXiv.\n",
    "2301.10743 .\n",
    "[29] Ted Chiang. Chatgpt is a blurry jpeg of the web. The New Yorker ,\n",
    "February 2023.\n",
    "[30] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating\n",
    "Long Sequences with Sparse Transformers. April 2019. URL: https:\n",
    "//arxiv.org/abs/1904.10509v1 .\n",
    "[31] Anna Choromanska, Mikael Henaff, Michael Mathieu, G´ erard Ben Arous,\n",
    "and Yann LeCun. The Loss Surfaces of Multilayer Networks, January\n",
    "2015. arXiv:1412.0233 [cs]. URL: http://arxiv.org/abs/1412.0233 ,\n",
    "doi:10.48550/arXiv.1412.0233 .\n",
    "[32] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. PaLM: Scal-\n",
    "ing Language Modeling with Pathways. arXiv:2204.02311 [cs] , April 2022.\n",
    "arXiv: 2204.02311. URL: http://arxiv.org/abs/2204.02311 .\n",
    "[33] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A Toy Model of Uni-\n",
    "versality: Reverse Engineering How Networks Learn Group Operations,\n",
    "May 2023. arXiv:2302.03025 [cs, math]. URL: http://arxiv.org/abs/\n",
    "2302.03025 ,doi:10.48550/arXiv.2302.03025 .\n",
    "[34] Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical\n",
    "Foundations for a Compositional Distributional Model of Meaning, March\n",
    "2010. arXiv:1003.4394 [cs, math]. URL: http://arxiv.org/abs/1003.\n",
    "4394 ,doi:10.48550/arXiv.1003.4394 .\n",
    "[35] Taco Cohen and Max Welling. Group equivariant convolutional net-\n",
    "works. In International conference on machine learning , pages 2990–2999.\n",
    "PMLR, 2016. arXiv:1602.07576.\n",
    "[36] R´ emi Coulom. Efficient selectivity and backup operators in monte-carlo\n",
    "tree search. In International conference on computers and games , pages\n",
    "72–83. Springer, 2006.\n",
    "36[37] Francis Crick. The recent excitement about neural networks. Nature ,\n",
    "337:129–132, 1989.\n",
    "[38] George V. Cybenko. Approximation by superpositions of a sigmoidal\n",
    "function. Mathematics of Control, Signals and Systems , 2:303–314, 1989.\n",
    "[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Un-\n",
    "derstanding. October 2018. arXiv: 1810.04805. URL: https://arxiv.\n",
    "org/abs/1810.04805v1 .\n",
    "[40] Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural Network\n",
    "Approximation. arXiv:2012.14501 [cs, math] , December 2020. arXiv:\n",
    "2012.14501. URL: http://arxiv.org/abs/2012.14501 .\n",
    "[41] Benjamin L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.\n",
    "Inductive Biases and Variable Creation in Self-Attention Mechanisms.\n",
    "arXiv:2110.10090 [cs, stat] , October 2021. arXiv: 2110.10090. URL:\n",
    "http://arxiv.org/abs/2110.10090 .\n",
    "[42] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom\n",
    "Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn\n",
    "Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario\n",
    "Amodei, Martin Wattenberg, and Christopher Olah. Toy Models of\n",
    "Superposition, September 2022. arXiv:2209.10652 [cs]. URL: http:\n",
    "//arxiv.org/abs/2209.10652 ,doi:10.48550/arXiv.2209.10652 .\n",
    "[43] Philip Feldman, James R. Foulds, and Shimei Pan. Trapping LLM Hal-\n",
    "lucinations Using Tagged Context Prompts, June 2023. arXiv:2306.06085\n",
    "[cs]. URL: http://arxiv.org/abs/2306.06085 ,doi:10.48550/arXiv.\n",
    "2306.06085 .\n",
    "[44] John Rupert Firth. Studies in linguistic analysis . Wiley-Blackwell, 1957.\n",
    "[45] Jerry A Fodor. The modularity of mind . MIT press, 1983.\n",
    "[46] Dan Friedman, Alexander Wettig, and Danqi Chen. Learning Transformer\n",
    "Programs, June 2023. arXiv:2306.01128 [cs]. URL: http://arxiv.org/\n",
    "abs/2306.01128 ,doi:10.48550/arXiv.2306.01128 .\n",
    "[47] Artur d’Avila Garcez and Luis C. Lamb. Neurosymbolic AI: The 3rd\n",
    "Wave, December 2020. arXiv:2012.05876 [cs]. URL: http://arxiv.org/\n",
    "abs/2012.05876 .\n",
    "[48] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What\n",
    "Can Transformers Learn In-Context? A Case Study of Simple Function\n",
    "Classes, January 2023. arXiv:2208.01066 [cs]. URL: http://arxiv.org/\n",
    "abs/2208.01066 ,doi:10.48550/arXiv.2208.01066 .\n",
    "37[49] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning . MIT\n",
    "press, 2016.\n",
    "[50] Anirudh Goyal and Yoshua Bengio. Inductive Biases for Deep Learn-\n",
    "ing of Higher-Level Cognition, August 2022. arXiv:2011.15091 [cs,\n",
    "stat]. URL: http://arxiv.org/abs/2011.15091 ,doi:10.48550/arXiv.\n",
    "2011.15091 .\n",
    "[51] Andrey Gromov. Grokking modular arithmetic, January 2023.\n",
    "arXiv:2301.02679 [cond-mat]. URL: http://arxiv.org/abs/2301.\n",
    "02679 ,doi:10.48550/arXiv.2301.02679 .\n",
    "[52] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini,\n",
    "Fosca Giannotti, and Dino Pedreschi. A Survey of Methods for Explain-\n",
    "ing Black Box Models. ACM Computing Surveys , 51(5):1–42, September\n",
    "2019. URL: https://dl.acm.org/doi/10.1145/3236009 ,doi:10.1145/\n",
    "3236009 .\n",
    "[53] Thilo Hagendorff. Machine Psychology: Investigating Emergent Capabil-\n",
    "ities and Behavior in Large Language Models Using Psychological Meth-\n",
    "ods, April 2023. arXiv:2303.13988 [cs]. URL: http://arxiv.org/abs/\n",
    "2303.13988 ,doi:10.48550/arXiv.2303.13988 .\n",
    "[54] Michael Hahn and Navin Goyal. A Theory of Emergent In-Context\n",
    "Learning as Implicit Structure Induction, March 2023. arXiv:2303.07971\n",
    "[cs]. URL: http://arxiv.org/abs/2303.07971 ,doi:10.48550/arXiv.\n",
    "2303.07971 .\n",
    "[55] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCan-\n",
    "dlish. Scaling Laws for Transfer, February 2021. arXiv:2102.01293\n",
    "[cs]. URL: http://arxiv.org/abs/2102.01293 ,doi:10.48550/arXiv.\n",
    "2102.01293 .\n",
    "[56] John Hewitt and Christopher D Manning. A Structural Probe for Finding\n",
    "Syntax in Word Representations. page 10, 2019.\n",
    "[57] Sepp Hochreiter and J¨ urgen Schmidhuber. Long short-term memory. Neu-\n",
    "ral computation , 9(8):1735–1780, 1997.\n",
    "[58] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\n",
    "Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne\n",
    "Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\n",
    "Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\n",
    "Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\n",
    "Vinyals, and Laurent Sifre. Training Compute-Optimal Large Language\n",
    "Models, March 2022. arXiv:2203.15556 [cs]. URL: http://arxiv.org/\n",
    "abs/2203.15556 ,doi:10.48550/arXiv.2203.15556 .\n",
    "38[59] Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik\n",
    "Strobelt, Duen Horng Chau, Mohammed J. Zaki, and Dmitry Krotov.\n",
    "Energy Transformer, February 2023. arXiv:2302.07253 [cond-mat, q-bio,\n",
    "stat]. URL: http://arxiv.org/abs/2302.07253 ,doi:10.48550/arXiv.\n",
    "2302.07253 .\n",
    "[60] Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated\n",
    "the world chess champion . Princeton University Press, 2002.\n",
    "[61] Myeongjun Jang and Thomas Lukasiewicz. Consistency Analysis of Chat-\n",
    "GPT, March 2023. arXiv:2303.06273 [cs]. URL: http://arxiv.org/abs/\n",
    "2303.06273 ,doi:10.48550/arXiv.2303.06273 .\n",
    "[62] Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu,\n",
    "Mateja Jamnik, Timoth´ ee Lacroix, Yuhuai Wu, and Guillaume Lample.\n",
    "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal\n",
    "Proofs, November 2022. arXiv:2210.12283 [cs]. URL: http://arxiv.org/\n",
    "abs/2210.12283 ,doi:10.48550/arXiv.2210.12283 .\n",
    "[63] Iain M. Johnstone. High Dimensional Statistical Inference and Ran-\n",
    "dom Matrices, November 2006. URL: https://arxiv.org/abs/math/\n",
    "0611589v1 .\n",
    "[64] Dan Jurafsky and James H Martin. Speech and language processing, 2009.\n",
    "[65] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn\n",
    "Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova Das-\n",
    "Sarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones,\n",
    "Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman,\n",
    "Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson\n",
    "Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson,\n",
    "Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben\n",
    "Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language Mod-\n",
    "els (Mostly) Know What They Know, July 2022. arXiv:2207.05221 [cs].\n",
    "URL: http://arxiv.org/abs/2207.05221 .\n",
    "[66] Daniel Kahneman. Fast and slow thinking. Allen Lane and Penguin\n",
    "Books, New York , 2011.\n",
    "[67] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Ben-\n",
    "jamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and\n",
    "Dario Amodei. Scaling Laws for Neural Language Models, January 2020.\n",
    "arXiv:2001.08361 [cs, stat]. URL: http://arxiv.org/abs/2001.08361 ,\n",
    "doi:10.48550/arXiv.2001.08361 .\n",
    "[68] Michael J Kearns and Umesh Vazirani. An introduction to computational\n",
    "learning theory . MIT press, 1994.\n",
    "[69] Daphne Koller and Nir Friedman. Probabilistic graphical models: princi-\n",
    "ples and techniques . MIT press, 2009.\n",
    "39[70] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi-\n",
    "fication with deep convolutional neural networks. Communications of the\n",
    "ACM , 60(6):84–90, 2017.\n",
    "[71] Florent Krzakala, Federico Ricci-Tersenghi, Lenka Zdeborova, Eric W\n",
    "Tramel, Riccardo Zecchina, and Leticia F Cugliandolo. Statistical Physics,\n",
    "Optimization, Inference, and Message-Passing Algorithms: Lecture Notes\n",
    "of the Les Houches School of Physics: Special Issue, October 2013 . Num-\n",
    "ber 2013. Oxford University Press, 2016.\n",
    "[72] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level\n",
    "concept learning through probabilistic program induction. Science ,\n",
    "350(6266):1332–1338, December 2015. URL: https://www.sciencemag.\n",
    "org/lookup/doi/10.1126/science.aab3050 ,doi:10.1126/science.\n",
    "aab3050 .\n",
    "[73] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J\n",
    "Gershman. Building Machines That Learn and Think Like People. April\n",
    "2016. URL: http://arxiv.org/abs/1604.00289 .\n",
    "[74] Yann LeCun. Popular talks and private discussion, 2015.\n",
    "[75] Yann LeCun. A path towards autonomous machine intelligence, 2022.\n",
    "URL: https://openreview.net/forum?id=BZ5a1r-kVsf .\n",
    "[76] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature ,\n",
    "521:436–444, 2015.\n",
    "[77] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk\n",
    "Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\n",
    "Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and\n",
    "Vedant Misra. Solving Quantitative Reasoning Problems with Lan-\n",
    "guage Models, June 2022. Number: arXiv:2206.14858 arXiv:2206.14858\n",
    "[cs]. URL: http://arxiv.org/abs/2206.14858 ,doi:10.48550/arXiv.\n",
    "2206.14858 .\n",
    "[78] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi´ egas, Hanspeter\n",
    "Pfister, and Martin Wattenberg. Emergent World Representations: Ex-\n",
    "ploring a Sequence Model Trained on a Synthetic Task, February 2023.\n",
    "arXiv:2210.13382 [cs]. URL: http://arxiv.org/abs/2210.13382 ,doi:\n",
    "10.48550/arXiv.2210.13382 .\n",
    "[79] Kenneth Li, Oam Patel, Fernanda Vi´ egas, Hanspeter Pfister, and Mar-\n",
    "tin Wattenberg. Inference-Time Intervention: Eliciting Truthful Answers\n",
    "from a Language Model, June 2023. arXiv:2306.03341 [cs]. URL: http:\n",
    "//arxiv.org/abs/2306.03341 ,doi:10.48550/arXiv.2306.03341 .\n",
    "[80] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara\n",
    "Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai\n",
    "40Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan,\n",
    "Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher\n",
    "R´ e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus,\n",
    "Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav\n",
    "Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,\n",
    "Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Hender-\n",
    "son, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\n",
    "Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav\n",
    "Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and\n",
    "Yuta Koreeda. Holistic Evaluation of Language Models, November 2022.\n",
    "arXiv:2211.09110 [cs]. URL: http://arxiv.org/abs/2211.09110 ,doi:\n",
    "10.48550/arXiv.2211.09110 .\n",
    "[81] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen\n",
    "Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and\n",
    "Karl Cobbe. Let’s Verify Step by Step, May 2023. arXiv:2305.20050\n",
    "[cs]. URL: http://arxiv.org/abs/2305.20050 ,doi:10.48550/arXiv.\n",
    "2305.20050 .\n",
    "[82] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and\n",
    "Cyril Zhang. Transformers Learn Shortcuts to Automata, October 2022.\n",
    "arXiv:2210.10749 [cs, stat]. URL: http://arxiv.org/abs/2210.10749 .\n",
    "[83] David JC MacKay. Information theory, inference and learning algorithms .\n",
    "Cambridge university press, 2003.\n",
    "[84] Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher,\n",
    "Joshua B. Tenenbaum, and Evelina Fedorenko. Dissociating language\n",
    "and thought in large language models: a cognitive perspective, January\n",
    "2023. arXiv:2301.06627 [cs]. URL: http://arxiv.org/abs/2301.06627 ,\n",
    "doi:10.48550/arXiv.2301.06627 .\n",
    "[85] Alexander Maloney, Daniel A. Roberts, and James Sully. A Solvable\n",
    "Model of Neural Scaling Laws, October 2022. arXiv:2210.16859 [hep-th,\n",
    "stat]. URL: http://arxiv.org/abs/2210.16859 ,doi:10.48550/arXiv.\n",
    "2210.16859 .\n",
    "[86] Yuri Manin and Matilde Marcolli. Semantic Spaces. arXiv.org , May 2016.\n",
    "arXiv: 1605.04238v1. URL: http://arxiv.org/abs/1605.04238v1 .\n",
    "[87] Christopher Manning and Hinrich Schutze. Foundations of statistical nat-\n",
    "ural language processing . MIT press, 1999.\n",
    "[88] Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal,\n",
    "and Omer Levy. Emergent linguistic structure in artificial neural net-\n",
    "works trained by self-supervision. Proceedings of the National Academy of\n",
    "Sciences , 117(48):30046–30054, 2020.\n",
    "41[89] Matilde Marcolli, Noam Chomsky, and Robert Berwick. Mathe-\n",
    "matical Structure of Syntactic Merge, May 2023. arXiv:2305.18278\n",
    "[cs, math]. URL: http://arxiv.org/abs/2305.18278 ,doi:10.48550/\n",
    "arXiv.2305.18278 .\n",
    "[90] Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Build-\n",
    "ing a large annotated corpus of english: The penn treebank. 1993.\n",
    "[91] David Marr. Vision: A computational investigation into the human rep-\n",
    "resentation and processing of visual information . MIT press, 2010.\n",
    "[92] Jir ˇi Matouˇ sek. Lecture notes on metric embeddings. 2013. URL: https:\n",
    "//kam.mff.cuni.cz/ ~matousek/ba-a4.pdf .\n",
    "[93] Pamela McCorduck and Cli Cfe. Machines who think: A personal inquiry\n",
    "into the history and prospects of artificial intelligence . CRC Press, 2004.\n",
    "[94] William Merrill. On the Linguistic Capacity of Real-Time Counter Au-\n",
    "tomata. arXiv:2004.06866 [cs] , April 2020. arXiv: 2004.06866. URL:\n",
    "http://arxiv.org/abs/2004.06866 .\n",
    "[95] William Merrill and Ashish Sabharwal. The Parallelism Tradeoff: Lim-\n",
    "itations of Log-Precision Transformers, April 2023. arXiv:2207.00729\n",
    "[cs]. URL: http://arxiv.org/abs/2207.00729 ,doi:10.48550/arXiv.\n",
    "2207.00729 .\n",
    "[96] William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated Trans-\n",
    "formers are Constant-Depth Threshold Circuits. arXiv:2106.16213 [cs] ,\n",
    "April 2022. arXiv: 2106.16213. URL: http://arxiv.org/abs/2106.\n",
    "16213 .\n",
    "[97] Marc Mezard and Andrea Montanari. Information, physics, and compu-\n",
    "tation . Oxford University Press, 2009.\n",
    "[98] Eric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The Quan-\n",
    "tization Model of Neural Scaling, March 2023. arXiv:2303.13506 [cond-\n",
    "mat]. URL: http://arxiv.org/abs/2303.13506 ,doi:10.48550/arXiv.\n",
    "2303.13506 .\n",
    "[99] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient\n",
    "Estimation of Word Representations in Vector Space, September 2013.\n",
    "arXiv:1301.3781 [cs]. URL: http://arxiv.org/abs/1301.3781 .\n",
    "[100] Marvin Minsky. Society of mind . Simon and Schuster, 1988.\n",
    "[101] David Mumford. Pattern theory: the mathematics of perception. arXiv\n",
    "preprint math/0212400 , 2002.\n",
    "[102] David Mumford and Agn` es Desolneux. Pattern theory: the stochastic\n",
    "analysis of real-world signals . CRC Press, 2010.\n",
    "42[103] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Stein-\n",
    "hardt. Progress measures for grokking via mechanistic interpretability,\n",
    "January 2023. arXiv:2301.05217 [cs]. URL: http://arxiv.org/abs/\n",
    "2301.05217 ,doi:10.48550/arXiv.2301.05217 .\n",
    "[104] Allen Newell, John Clifford Shaw, and Herbert A Simon. Empirical explo-\n",
    "rations of the logic theory machine: a case study in heuristic. In Papers\n",
    "presented at the February 26-28, 1957, western joint computer conference:\n",
    "Techniques for reliability , pages 218–230, 1957.\n",
    "[105] Nils J Nilsson. The quest for artificial intelligence . Cambridge University\n",
    "Press, 2009.\n",
    "[106] Chris Olah. Mechanistic interpretability, variables, and the importance of\n",
    "interpretable bases, 2022. URL: https://transformer-circuits.pub/\n",
    "2022/mech-interp-essay/index.html .\n",
    "[107] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das-\n",
    "Sarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna\n",
    "Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\n",
    "Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\n",
    "Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared\n",
    "Kaplan, Sam McCandlish, and Chris Olah. In-context Learning and\n",
    "Induction Heads, September 2022. arXiv:2209.11895 [cs]. URL: http:\n",
    "//arxiv.org/abs/2209.11895 ,doi:10.48550/arXiv.2209.11895 .\n",
    "[108] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe:\n",
    "Global Vectors for Word Representation. In Proceedings of the 2014 Con-\n",
    "ference on Empirical Methods in Natural Language Processing (EMNLP) ,\n",
    "pages 1532–1543, Doha, Qatar, October 2014. Association for Com-\n",
    "putational Linguistics. URL: https://www.aclweb.org/anthology/\n",
    "D14-1162 ,doi:10.3115/v1/D14-1162 .\n",
    "[109] Mary Phuong and Marcus Hutter. Formal Algorithms for Transformers,\n",
    "July 2022. arXiv:2207.09238 [cs]. URL: http://arxiv.org/abs/2207.\n",
    "09238 .\n",
    "[110] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant\n",
    "Misra. Grokking: Generalization Beyond Overfitting on Small Algorith-\n",
    "mic Datasets. arXiv:2201.02177 [cs] , January 2022. arXiv: 2201.02177.\n",
    "URL: http://arxiv.org/abs/2201.02177 .\n",
    "[111] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith,\n",
    "and Mike Lewis. Measuring and Narrowing the Compositionality Gap\n",
    "in Language Models, October 2022. arXiv:2210.03350 [cs]. URL: http:\n",
    "//arxiv.org/abs/2210.03350 ,doi:10.48550/arXiv.2210.03350 .\n",
    "[112] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n",
    "Improving language understanding by generative pre-training. 2018. Pub-\n",
    "lisher: OpenAI.\n",
    "43[113] Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and\n",
    "Ilya Sutskever. Language Models are Unsupervised Multitask Learners.\n",
    "undefined , 2019. URL: https://www.semanticscholar.org/paper/\n",
    "Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/\n",
    "9405cc0d6169988371b2755e573cc28650d14dfe .\n",
    "[114] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\n",
    "Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Explor-\n",
    "ing the Limits of Transfer Learning with a Unified Text-to-Text Trans-\n",
    "former. arXiv:1910.10683 [cs, stat] , July 2020. arXiv: 1910.10683. URL:\n",
    "http://arxiv.org/abs/1910.10683 .\n",
    "[115] Hubert Ramsauer, Bernhard Sch¨ afl, Johannes Lehner, Philipp Seidl,\n",
    "Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner,\n",
    "Milena Pavlovi´ c, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael\n",
    "Kopp, G¨ unter Klambauer, Johannes Brandstetter, and Sepp Hochreiter.\n",
    "Hopfield Networks is All You Need, April 2021. arXiv:2008.02217 [cs,\n",
    "stat]. URL: http://arxiv.org/abs/2008.02217 ,doi:10.48550/arXiv.\n",
    "2008.02217 .\n",
    "[116] Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep\n",
    "Learning Theory. arXiv:2106.10165 [hep-th, stat] , August 2021. arXiv:\n",
    "2106.10165. URL: http://arxiv.org/abs/2106.10165 .\n",
    "[117] Frank Rosenblatt. The perceptron: a probabilistic model for information\n",
    "storage and organization in the brain. Psychological review , 65(6):386,\n",
    "1958.\n",
    "[118] David E. Rumelhart, Geoffrey E. Hinton, and James L. McClelland. A\n",
    "general framework for parallel distributed processing. 1986.\n",
    "[119] Stuart J Russell. Artificial intelligence a modern approach . Pearson Ed-\n",
    "ucation, Inc., 2010.\n",
    "[120] Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo. Are emergent\n",
    "abilities of large language models a mirage? ArXiv , abs/2304.15004, 2023.\n",
    "[121] Terrence J Sejnowski. The deep learning revolution . MIT press, 2018.\n",
    "[122] Claude E Shannon. Xxii. programming a computer for playing chess. The\n",
    "London, Edinburgh, and Dublin Philosophical Magazine and Journal of\n",
    "Science , 41(314):256–275, 1950.\n",
    "[123] Hava T Siegelmann and Eduardo D Sontag. On the computational power\n",
    "of neural nets. In Proceedings of the fifth annual workshop on Computa-\n",
    "tional learning theory , pages 440–449, 1992.\n",
    "[124] Brian Cantwell Smith. Procedural reflection in programming languages\n",
    "volume i. 1982.\n",
    "44[125] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya\n",
    "Ganguli. Deep unsupervised learning using nonequilibrium thermodynam-\n",
    "ics. In International Conference on Machine Learning , pages 2256–2265.\n",
    "PMLR, 2015. arXiv:1503.03585.\n",
    "[126] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and\n",
    "Ari S. Morcos. Beyond neural scaling laws: beating power law scaling via\n",
    "data pruning, June 2022. Number: arXiv:2206.14486 arXiv:2206.14486\n",
    "[cs, stat]. URL: http://arxiv.org/abs/2206.14486 ,doi:10.48550/\n",
    "arXiv.2206.14486 .\n",
    "[127] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. Beyond the\n",
    "Imitation Game: Quantifying and extrapolating the capabilities of lan-\n",
    "guage models. Technical Report arXiv:2206.04615, arXiv, June 2022.\n",
    "arXiv:2206.04615 [cs, stat] type: article. URL: http://arxiv.org/abs/\n",
    "2206.04615 .\n",
    "[128] Richard Sutton. The bitter lesson, 2019. URL: http://www.\n",
    "incompleteideas.net/IncIdeas/BitterLesson.html .\n",
    "[129] Christian Szegedy. A promising path towards autoformalization and gen-\n",
    "eral artificial intelligence. In International Conference on Intelligent Com-\n",
    "puter Mathematics , 2020.\n",
    "[130] Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gim-\n",
    "pel. Chess as a Testbed for Language Model State Tracking, May\n",
    "2022. arXiv:2102.13249 [cs]. URL: http://arxiv.org/abs/2102.13249 ,\n",
    "doi:10.48550/arXiv.2102.13249 .\n",
    "[131] Richard E. Turner. An Introduction to Transformers, July 2023.\n",
    "arXiv:2304.10557 [cs]. URL: http://arxiv.org/abs/2304.10557 ,doi:\n",
    "10.48550/arXiv.2304.10557 .\n",
    "[132] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n",
    "Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Atten-\n",
    "tion Is All You Need. June 2017. arXiv: 1706.03762. URL: https:\n",
    "//arxiv.org/abs/1706.03762 .\n",
    "[133] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas-\n",
    "tian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald\n",
    "Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang,\n",
    "Jeff Dean, and William Fedus. Emergent Abilities of Large Language\n",
    "Models. 2022. Publisher: arXiv Version Number: 2. URL: https:\n",
    "//arxiv.org/abs/2206.07682 ,doi:10.48550/ARXIV.2206.07682 .\n",
    "[134] Gail Weiss, Yoav Goldberg, and Eran Yahav. On the Practical Compu-\n",
    "tational Power of Finite Precision RNNs for Language Recognition, May\n",
    "2018. arXiv:1805.04908 [cs, stat]. URL: http://arxiv.org/abs/1805.\n",
    "04908 ,doi:10.48550/arXiv.1805.04908 .\n",
    "45[135] Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin\n",
    "Choi, and Kyunghyun Cho. NaturalProofs: Mathematical Theorem Prov-\n",
    "ing in Natural Language. page 14, 2021.\n",
    "[136] Noam Wies, Yoav Levine, and Amnon Shashua. The Learnability of In-\n",
    "Context Learning, March 2023. arXiv:2303.07895 [cs]. URL: http://\n",
    "arxiv.org/abs/2303.07895 ,doi:10.48550/arXiv.2303.07895 .\n",
    "[137] Avi Wigderson. Mathematics and computation: A theory revolutionizing\n",
    "technology and science . Princeton University Press, 2019.\n",
    "[138] Wikipedia. URL: https://en.wikipedia.org/wiki/Reflective_\n",
    "programming .\n",
    "[139] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An\n",
    "Explanation of In-context Learning as Implicit Bayesian Inference, July\n",
    "2022. arXiv:2111.02080 [cs]. URL: http://arxiv.org/abs/2111.02080 ,\n",
    "doi:10.48550/arXiv.2111.02080 .\n",
    "[140] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong\n",
    "Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jian-\n",
    "feng Gao. Tensor Programs V: Tuning Large Neural Networks via Zero-\n",
    "Shot Hyperparameter Transfer, March 2022. arXiv:2203.03466 [cond-\n",
    "mat]. URL: http://arxiv.org/abs/2203.03466 ,doi:10.48550/arXiv.\n",
    "2203.03466 .\n",
    "[141] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,\n",
    "Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Prob-\n",
    "lem Solving with Large Language Models, May 2023. arXiv:2305.10601\n",
    "[cs]. URL: http://arxiv.org/abs/2305.10601 ,doi:10.48550/arXiv.\n",
    "2305.10601 .\n",
    "[142] Yuhui Zhang, Michihiro Yasunaga, Zhengping Zhou, Jeff Z. HaoChen,\n",
    "James Zou, Percy Liang, and Serena Yeung. Beyond Positive Scaling:\n",
    "How Negation Impacts Scaling Trends of Language Models, May 2023.\n",
    "arXiv:2305.17311 [cs]. URL: http://arxiv.org/abs/2305.17311 ,doi:\n",
    "10.48550/arXiv.2305.17311 .\n",
    "[143] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do\n",
    "Transformers Parse while Predicting the Masked Word?, March 2023.\n",
    "arXiv:2303.08117 [cs]. URL: http://arxiv.org/abs/2303.08117 ,doi:\n",
    "10.48550/arXiv.2303.08117 .\n",
    "[144] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F: a cross-\n",
    "system benchmark for formal Olympiad-level mathematics, February\n",
    "2022. arXiv:2109.00110 [cs]. URL: http://arxiv.org/abs/2109.00110 ,\n",
    "doi:10.48550/arXiv.2109.00110 .\n",
    "46\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = clean_research_paper(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' of Physics, Stony Brook University\\nmdouglas@cmsa harvard edu\\nJuly 2023\\nAbstract\\nArtificial intelligence is making spectacular progress, and one of the\\nbest examples is the development of large language models (LLMs) such\\nas OpenAI’s GPT series  In these lectures, written for readers with a\\nbackground in mathematics or physics, we give a brief history and survey\\nof the state of the art, and describe the underlying transformer architec-\\nture in detail  We then explore some current ideas on how LLMs work\\nand how models trained to predict the next word in a text are able to\\nperform other tasks displaying intelligence 05782v1    11 Jul 20231 Introduction\\nAt the end of November 2022, OpenAI released a system called ChatGPT which\\ninteracts with its users in natural language  It can answer questions, engage in\\ndialogs, translate between languages and write computer code with a fluency\\nand ability far exceeding all previous publically available systems  Although it\\nfalls well short of human abilities in many ways, still the large language model\\ntechnology of which it is an example is widely considered to be a major advance\\nin artificial intelligence 1\\nFew developments in science and technology entered the popular conscious-\\nness as quickly as ChatGPT  There is no mystery about why  The ability to use\\nlanguage is a defining property of humanity, and for the first time a computer\\nis doing this well enough to make a comparison with humans interesting  All\\nof the hopes and fears which have developed around AI, robots and technology\\nmore generally are being brought into the discussion  In my opinion this is\\njustified; the speed of recent progress makes it urgent to better understand AI,\\nto forecast its capabilities and limitations, and to make wise decisions about\\nits development and use  With great opportunities will come great challenges,\\nwhich will concern all of us \\nIn these lecture notes we give an introduction to this subject for mathe-\\nmaticians, physicists, and other scientists and readers who are mathematically\\nknowledgeable but not necessarily expert in machine learning or artificial intel-\\nligence  We begin with a very brief overview of AI in §2 to explain some ideas\\nwe consider to be essential context, the basic principles of the symbolic and\\nconnectionist approaches  In §3 we define statistical language models and relate\\nthe history of transformer-based LLMs up through GPT-4  In §4 we discuss\\nmeasures of what LLMs do and how well they do it  We then give a precise\\nexplanation of simpler language models in §5 and the transformer architecture\\nin§6 \\nIt is amazing that a model defined by a few short equations, trained to go\\nthrough a text and simply predict each next word as it appears – a task which\\nseems only loosely related to any definition of intelligence – can do tasks which\\n“obviously” require intelligence, such as solving word problems like the one in\\nFigure 1 below  At present nobody really understands how this works  Even\\nthe interpretation of what LLMs are doing is controversial, ranging from the\\nbelief that they are “simply” rearranging the sentences they were trained on,\\nall the way to the belief that the LLMs are learning sophisticated models of\\nthe world and that “simply” scaling up the computations will produce artificial\\ngeneral intelligence  Any forecast for progress must take into account the current\\nmodels’ shortcomings – lack of long term memory and ability to plan, tendency\\nto make up facts and “hallucinate,” unreliability in logical reasoning, etc  Do\\nthese problems have technical solutions which will also look simple once we have\\nthem? Or are they more significant barriers?\\nMost current work on LLMs takes an engineering and problem solving per-\\n1A few of the many other milestones in LLM development are  \\n2Question: Assume that the variance of the first nnatural numbers is 10, and\\nthe variance of the first meven natural numbers is 16  Compute m+n \\nModel output: Letσ2be the variance of the first meven natural numbers,\\nandτ2be the variance of the first nnatural numbers  Then, we have τ2=n2−1\\n12\\nandσ2=m2−1\\n3  We solve the system of equations:\\nn2−1\\n12= 10\\nm2−1\\n3= 16 \\nThis translates into the system\\nn2= 121\\nm2= 48 + 1 = 49  \\nTherefore, n= 11 andm= 7 , son+m= 18  \\nFigure 1: A question-answer pair solved by Minerva  From Lewkowycz et al\\n, “Solving quantitative reasoning problems with language models,” 2022 \\nspective, but there are many interesting works which focus more on understand-\\ning how LLMs work  One would think this should be far easier than understand-\\ning how human brains work, as we have full knowledge of an LLM’s microscopic\\nworkings and can do a wide variety of experiments on it 2These efforts are in\\ntheir early days, but in §7 we survey current approaches to understanding how\\nLLMs do what they do  We conclude in §8 with more general discussion, some\\nquestions and potentially important developments to watch for \\nBefore we start, let me say a little about my own background  I was trained\\nas a theoretical physicist and most of my contributions to science are in string\\ntheory and its interface with mathematics, but I have followed AI fairly closely\\nsince the 80’s and in detail since 2016  In addition I spent eight years in quan-\\ntitative finance where I gained a good deal of “hands-on” experience with ma-\\nchine learning  I have given many lectures telling computer scientists about\\nphysics and physicists about computational topics, and benefited from conver-\\nsations with many people – more than I can name here, but let me thank\\nGerald Jay Sussman, David McAllester, Yann LeCun, Sanjeev Arora, Surya\\nGanguli, Jeremy Avigad, Vijay Balasubramanian, Dmitri Chklovskii, David\\nDonoho, Steve Skiena, Christian Szegedy, Misha Tsodyks, Tony Wu and the\\n2At least, this was the case before March 2023  Currently the weights and even the de-\\nsign parameters of GPT-4, the most advanced LLM, are held in confidence by OpenAI as\\nproprietary information \\n3many speakers in the CMSA New Technologies seminar series,3and Josef Urban\\nand the AITP community 4Thanks to David McAllester and Sergiy Verstyuk\\nfor comments on the first draft  These notes would not have been possible with-\\nout their input and advice, and I hope their signal to noise ratio approaches\\nthat of what they shared with me \\n2 Symbolic and connectionist AI\\nThe goal of artificial intelligence is to build computational systems which can\\nperform tasks which require intelligence  Although intelligence is hard to define\\nprecisely, an operational definition suitable for LLMs is ability at tasks requiring\\nlanguage, reasoning, and planning, as judged by humans who interact with the\\nsystem  Some famous and difficult “challenge tasks” include playing chess ,\\nproving mathematical theorems , and answering natural language questions\\nusing generally known facts and common sense 5\\nThese problems have been the subject of intense investigation since the mid-\\n50’s, and a few textbooks and histories are   Essentially from\\nthe start, two broad approaches were set out, which would later be called sym-\\nbolic and connectionist AI 6The symbolic approach originated in mathematical\\nlogic and generative linguistic theory, and tracked the development of computer\\ntechnology (both hardware and software) as a tool for solving practical and\\nscientific problems  Central topics in this approach are formal logic and lan-\\nguage theory, search and heuristics, and engineering techniques for designing\\nand building large and complex systems \\nSymbolic AI systems are designed, meaning that their creators develop a de-\\ntailed understanding of the task, and encode this understanding into the system\\nby programming, by hardware design and otherwise  As an example, consider\\nthe task of parsing: given an input string of words, determine its grammatical\\nstructure  Most of us learned to diagram sentences in elementary school, and\\nalthough linguists have developed far more sophisticated notions of grammar,\\nthis simple notion gives the right idea  The grammar of a language is encoded\\nin rules, which belong to a formal framework – see the appendix for the example\\nof context-free grammars  Given such a framework, one can design a parsing\\nalgorithm which takes as input a rule set and an input string, and produces an\\noutput which states whether the string is a grammatical sentence and if so makes\\nits structure explicit  This is a symbolic AI approach, not because the words\\n“symbolize” anything (after all grammar does not have to come with mean-\\n3https://live-hu-cmsa-222 io/\\n4http://aitp-conference org/2023/\\n5This challenge originates with Turing’s famous test, but the restriction to question answer-\\ning makes it better defined and testable using benchmarks, standardized question-answer sets \\nDiscussion of the original test can be found at https://en wikipedia org/wiki/Turing test\\n6Symbolic AI is sometimes called “GOFAI” for “good old-fashioned AI ” Related terms\\ninclude “rule based,” “logic based,” “expert system” and “feature engineering ” The con-\\nnectionist approach has many other names, reflecting its mixed ancestry: “neural,” “deep\\nlearning,” “parallel distributed processing,” “differentiable,” and “representation learning ”\\n4ing), but because the grammatical rules and the parsing algorithm (including\\nits internal data structures) have a clear meaning to their designers \\nSymbolic methods have had considerable success at many tasks requiring\\nintelligence, famously including chess playing  and symbolic algebra7as well\\nas more prosaic but very central tasks such as translating high level computer\\nlanguages to machine code (compiling)  And a great deal of work has been\\ndone to broaden their scope, for example to build question answering systems\\nsuch as the well known IBM Watson  Many valuable techniques came out of\\nthis; ways to systematize rules into “knowledge bases” or “knowledge graphs,”\\nmethods for automated logical reasoning, and so on  But it was long ago realized\\nthat once one goes beyond “formal worlds” such as chess and algebra to the\\ncomplex and messy situations of real life, although one can postulate rules which\\ncapture many truths and can be used for reasoning, rules which are valid in all\\ncases are very rare  Furthermore, the sheer number of rules required to cover\\neven the likely possibilities is very large  These difficulties were addressed by\\nimplementing probabilistic reasoning and by getting teams of humans to develop\\nthe requisite enormous rule sets, leading to the “expert system” approach which\\nwas applied (for example) to medical question answering  Cyc,8an early and\\nwell known expert system, is commercially available and has a database of\\ncommonly known facts with over 25 million assertions; however this is dwarfed\\nby knowledge bases such as Wikidata (over one billion “facts”) but which do\\nnot have a systematic reasoning engine  It is clear that any approach which\\ndepends on careful human analysis of such large knowledge bases is impractical \\nMeanwhile, a very different “connectionist” approach to AI was being cham-\\npioned by other researchers  They drew their inspiration from hypotheses about\\nhow the brain works, from information theory and statistics, from physics and\\nother natural sciences, and from applied mathematics and particularly opti-\\nmization theory  These diverse points of view came together in the 1990’s and\\nled to a great deal of interdisciplinary work,9of which the part most related to\\nAI and which lies behind LLMs is called machine learning (ML) \\nThe usual starting point in modern treatments of ML is to rephrase a task\\nas a statistical inference problem based on a large dataset  A canonical example\\nis image recognition – say, given an array of pixels (light intensity values),\\nestimate the probability that the image depicts a cat  Rather than design a\\nsystem to do this, one starts with a large dataset of images with labels (cat and\\nnon-cat)  One then designs a very general statistical model and “trains” it on\\nthis dataset to predict the label given the image  This is supervised learning,\\none can also do “self-supervised” learning in which the system predicts some\\npart of the data given other parts (say, filling in part of an image)  A third\\nstandard ML paradigm is reinforcement learning, which applies to tasks which\\ninvolve choosing actions to fulfill a longer term goal  The classic example is\\ngame playing, as in AlphaGo and AlphaZero \\nIn any case, since the problem is formulated statistically, it is possible to\\n7https://www com/\\n9I first learned about this from  \\n5consider the training dataset one item at a time, and use it to incrementally\\nimprove the model  This is almost always done by formulating the task in terms\\nof an “objective function” which measures the quality with which it is performed,\\nfor example the accuracy with which correct labels are assigned to images  One\\nthen takes a parameterized model and trains it by optimizing this function,\\nevaluated on the training dataset, with respect to the model parameters  For\\nthe classic models of statistics this can be done analytically, as in a least squares\\nfit  For more general models one uses numerical methods, such as gradient\\ndescent  Either way, a central question of statistics and machine learning is\\ngeneralization, meaning the extent to which the model well describes data not\\nin the training set but sampled from the same probability distribution  A well\\nknown principle which speaks to this question is “Occam’s razor,” that simpler\\nmodels will generalize better  This is often simplified to the rule that a model\\nshould have the minimal number of parameters needed to fit the dataset \\nNot all machine learning systems are “deep learning”  or “connection-\\nist”   These terms generally refer to the use of neural networks with large\\nnumbers of parameters which provide effective universal function approxima-\\ntors  While the idea is very old , before 2012 it was widely believed to be\\nimpractical  One argument for this was the “dull side” of Occam’s razor – mod-\\nels with so many parameters were destined to overfit and would not generalize \\nEvidently this is not the case, leading to concepts such as “benign overfitting ”\\n Another argument was that the objective functions for these models\\nare highly nonconvex and optimization would get stuck at poor quality local\\nminima  This can be a problem, but turns out to be solvable for reasons that\\nare partially understood   Finally, despite the effectiveness of the trained\\nmodel in performing a task, the large number of parameters often makes it very\\nhard to understand how such a model works, and why a given input produces\\na particular output  This “interpretability problem” remains a key issue with\\ndeep learning models, and is the subject of much research  \\nThere are many other variations and hybrid approaches in the story  Another\\nimportant one is the “pattern recognition” approach   This is also based\\non statistical inference but – like the symbolic approach – it emphasizes the value\\nof detailed understanding of the problem domain in designing the system  For\\nexample, one could hand-code the initial layers of an image recognition network\\nto detect lines or other significant “features” of the image  But unlike a purely\\nsymbolic approach, these features would be used as input to a general statistical\\nor neural model \\nAnother concept which illustrates the relation between the two approaches\\nis probabilistic reasoning, the use of rules such as “the chance of rain when it is\\ncloudy is 50%”  One can state and use such rules in a symbolic approach (see\\nfor example ), the essential distinction with connectionism is not the use of\\nprobabilities but rather the representation of knowledge in terms of explicit and\\nmeaningful rules \\nAs we suspect every reader has already heard, the symbolic approach was\\ndominant from the early days until 2012, and (along with many other suc-\\ncesses) led to a superhuman chess player, but seemed inadequate for our other\\n6two challenge tasks (theorem proving and question answering)  In 2012 the\\nconnectionist approach surpassed other approaches to computer vision , and\\never since neural systems have gone from triumph to triumph  In 2016 the\\ndeep learning system AlphaZero surpassed the symbolic AI chess players (and\\nof course humans)  Over the last few years, transformer models trained on a\\nlarge corpus of natural language to predict each next word as it appears, have\\nrevolutionized the field of natural language processing  As we write this the\\nstate of the art GPT-4 demonstrates truly remarkable performance at question\\nanswering, code generation and many other tasks  \\nThe simplest and arguably deepest explanation for this history is that it is\\na consequence of the exponential growth of computational power and training\\ndatasets, which continues to the present day  Given limited computing power\\nand data, the ability of the symbolic and pattern recognition approaches to\\ndirectly incorporate human understanding into a system is a significant advan-\\ntage  On the other hand, given sufficiently large computing power and data,\\nthis advantage is nullified and may even become disadvantageous, as the human\\neffort required to code the system becomes the limiting resource  This point,\\nthat the most significant advances in AI (and computation more generally) have\\ncome from hardware improvements and replacing human engineering with data-\\ndriven methods, is forcefully made by Sutton in his “bitter lesson” essay  \\nIn§3,§4 and §8 we will discuss scaling laws and evidence for and against the\\nidea that by continuing along the current path, training ever-larger models on\\never-larger datasets, we will achieve AGI (artificial general intelligence, whatever\\nthat means) and the realms beyond \\nUp to now the symbolic and connectionist approaches have generally been\\nconsidered to be in tension 10There is another point of view which consid-\\ners them complementary, with a symbolic approach better suited for certain\\nproblems (for example logical reasoning) and connectionist for others (for ex-\\nample image recognition)  Given this point of view one can seek a synthesis or\\n“neurosymbolic” approach, advocated in many works  \\nBut are they in conflict at all? Another reconciliation is the hypothesis that\\nproblems which in the symbolic approach are solved using rules and algorithms,\\nare also being solved that way by neural systems and in particular by LLMs \\nHowever, rather than the algorithms and rules being coded by humans, as the\\nresult of its training procedure the LLM has somehow learned them, encoded\\nin its networks in some as yet mysterious way  This vague hypothesis can be\\nsharpened in many ways, in part by proposing specific mechanisms by which\\nalgorithms and rules are encoded, in part by making general claims about the\\nalgorithms which are being learned  We discuss these ideas in §7 and §8 \\n10To better discuss this point one should refine the symbolic-connectionist dichotomy into\\nmultiple axes: system design versus learning from data; meaningful rules versus uninterpreted\\nmodels; combinatorial versus differentiable optimization; deterministic versus probabilistic \\n73 Language models\\nThroughout the history of linguistics, languages have been described in terms\\nof rules: rules of grammar, phonology, morphology, and so on, along with log-\\nical and other frameworks for describing meaning  This remains the case in\\nChomskyan linguistics and in much of theoretical linguistics \\nBy contrast, LLMs are statistical language models, meaning that they encode\\na probability distribution on strings of words, call this P(w1  w L), which\\napproximates the distribution realized by a large body (or “corpus”) of text in\\nthe language  The simplest example is the frequency or “1-gram” model defined\\nby taking the words to be independently distributed, so\\nP(w1  w L) =LY\\ni=1P(wi);P(w) =number of occurrences of win the corpus\\ntotal number of words in the corpus \\n(1)\\nOf course, this model captures very little of the structure of language, which\\ninvolves dependencies between the word choices \\nLLMs are generative models,11by which we will mean that there is a practi-\\ncal method for sampling from the distribution  To explain this, consider a word\\nprediction task in which some words in a string are given (the “input”) and\\nothers left blank (the “output”)  Given a probability distribution P(w1  w L),\\nthere is a corresponding conditional probability distribution for the output given\\nthe input  As an example, suppose we are given the string “The cat \\noutside,” where “” is a “token” which marks the position of the missing\\nword  The relevant conditional probabilities might be\\nP(the cat went outside |the cat  outside) = 0  5\\nP(the cat sat outside |the cat  outside) = 0  2\\nand so on, summing to total probability 1  In the masked word prediction task,\\nthe model must determine (or sample from) this distribution \\nA particularly convenient case is to give the conditional probability of the\\nword which follows a given string, which we denote as\\nP(wn+1|w1w2  w n−1wn)  (2)\\nBy sampling this distribution to get a new word wn+1and appending it to the\\nend, the string can be extended one word at a time  Repeating this process\\ngives an arbitrarily long string, which by the laws of probability is a sample\\nfrom the original probability distribution P(w1  w L), for example\\nP(the cat went outside) = P(the) P(cat|the)P(went |the cat) P(outside |the cat went)  \\nThis factorization of the probability into successive conditional probabilities\\ndefines the class of autoregressive models  One could furthermore require that\\n11Many different definitions of this term can be found in the literature \\n8the conditional probability Eq  2 depends only on the kmost recent words, in\\nwhich case one would have a Markov model whose state is a string of kwords \\nTo evaluate how good a language model is, we want to quantify how well its\\nprobability distribution approximates that of the corpus (the empirical distribu-\\ntion)  The standard measure of this is the cross entropy  For an autoregressive\\nmodel this is a sum of terms, one for each word in the corpus,12\\nL=−1\\nNN−nX\\ni=1logP(wi+n|wiwi+1  w i+n−1) (3)\\nOne also refers to exp −Las “perplexity ” In a machine learning approach, we\\ncan use Eq  3 as an objective function and minimize it as a function of the\\nnetwork parameters to train the network  We can then apply the many tools of\\nML: backpropagation, splitting the sum into batches, varying the learning rate\\nand so on, to get an efficient and effective model  While the details are an art\\nwhich depends on the particular domain and model architecture,13conceptually\\nthese are much the same for LLMs as for other machine learning models \\nThis statistical approach to modeling language has been pursued since the\\nlate 80’s  and many models were developed, such as the recurrent neu-\\nral network (RNN) which we will describe in §5  Following the general machine\\nlearning experience that supervised tasks (learning from input-output pairs)\\nare easier than unsupervised tasks, many of these works addressed machine\\ntranslation and parsing, for which there are good labeled datasets (documents\\nwith their translations; sentences with their grammatical structure)  However\\nunlabeled datasets are much larger and by 2015 or so there was a sense that self-\\nsupervised learning was the next frontier , leading to more focus on masked\\nword prediction \\nThe history of transformer models starts with the 2017 proposal of Vaswani\\net al   Their model was designed for a translation task and was more com-\\nplicated than what we will explain in §6, but the essential idea to use attention\\nand positional encoding to represent all the relations between the words in a\\ntext originated here and is fully present \\nThe transformer architecture was taken up by many groups, and particularly\\ninfluential 2018 works include BERT  and GPT   BERT was trained by\\nmasking arbitrary words in a sentence (not just the next word), which allows\\nthe model to look backward and forward for context and leads to better results \\nHowever it is not straightforward to sample from such a model, and eventually\\nthe simpler next word prediction approach followed by GPT won out \\nBoth of these models, and most work of this period, followed the paradigm\\nof pretraining followed by fine tuning  The idea was to first train for word\\nprediction on a very large corpus, to get a general purpose model  This would\\nthen be adapted to specific tasks such as question answering by fine tuning \\nThis means doing a second pass of supervised learning on a much smaller labeled\\n12With the sign, L ≥0 and better models have smaller L  The term “loss function” is often\\nused for an objective function with these properties \\n13In CS this term generally refers to the large scale arrangement of components of a system \\n9dataset, replacing next word prediction by the objective function for the specific\\ntask  Say we are doing question answering, this could be the accuracy of the\\nanswers  This two step procedure was justified by the notion of transfer learning,\\nmeaning that the capabilities of the general purpose model “transfer” to related\\nbut different tasks  This approach led to SOTA14results on many benchmarks\\nand motivated much further work \\nMost importantly, a great deal of ingenuity and hard work was put into\\nsolving the engineering problems of training larger and larger models on larger\\nand larger datasets  As for the data, a lot of text is available on the web, with\\none much used archive of this data provided by Common Crawl 15Training can\\nlargely be done in parallel by dividing up this data, and the availability of large\\nclusters of GPU-enabled servers at industrial labs and through cloud computing\\nmeant that sufficient computing resources were available in principle  However,\\nthe overall cost of training scales as (at least) the product of model size and\\ndataset size, and this was becoming expensive  While the precise cost figures\\nfor the GPT series are not public, it is estimated that a single training run\\nof the largest GPT-3 models cost tens of millions of dollars  To motivate and\\nefficiently carry out such costly experiments, one needs some ability to predict in\\nadvance how changes in model and dataset size will affect the training methods\\n(for example the optimal choice of learning rate) and performance \\nAn important advance in this direction was the observation of power law\\nscaling in language model performance   Figure 2 plots the test loss16against\\nthe logarithms of the sizes and compute resources used, and these straight lines\\ncorrespond to a power law relation between size and perplexity  This scaling\\nholds over many decades in model size and, while the exponents α∼ −0 076 to\\n−0 095 are rather small, this is a strong argument that larger models will have\\nbetter performance  These ideas were also used to determine optimal model-\\ndataset size tradeoff  and the scaling of hyperparameters   These results\\nwere a significant input into the decision to do this very expensive research \\nYear Model Number of Parameters Dataset size (tokens)\\n2018 GPT 110M 1B\\n2018 BERT 340M 3B\\n2019 GPT-2 1 5B 10B\\n2020 GPT-3 175B 500B\\n2022 PaLM 540B 780B\\n2023 GPT-4 1 4T (?) ?\\nTable 1: Large Language Models (M/B/T = million/billion/trillion)  In many\\ncases several model sizes were considered; we quote the largest \\n14State of the art, in other words an improvement over all previously evaluated models org/\\n16This is Eq  3 (minus log perplexity) evaluated on texts which were removed or “held out”\\nof the training set, to get a measure of generalization ability \\n10Dataset Size tokensParameters non-embeddingCompute PF-days, non-embeddingTest LossFigure 2: Language modeling performance as a function of model size, dataset\\nsize, and amount of compute used for training  From Kaplan et al, “Scaling\\nLaws for Neural Language Models,” 2020  \\nNow it should be realized that, while the measure being improved here is\\nfairly objective, still there was no strong reason to think that improving it would\\nlead to models with qualitatively new “emergent” capabilities  But it appears\\nthat this is what happened: GPT-3 and its fine-tuned cousins (such as Codex)\\nwere able to do tasks, such as write computer code from a natural language\\ndescription, for which smaller models were almost worthless 17We will discuss\\nmore of this progress shortly, and speculate a bit in the conclusions \\nOne of the most interesting LLM phenomena is in-context learning, first\\ndiscussed in the original GPT-3 paper   This refers to the ability of an\\nLLM to carry out tasks different from its original objective without modify-\\ning its parameters, indeed without any need for additional training on the new\\ntask (fine tuning)  Rather, after being given (as input text) a few examples\\nof input-output pairs, the LLM can be given another input and will generate\\na suitable output  Say the new task is question answering, then after a few\\nquestion-answer examples the LLM will answer the next question it is given \\nWhile intuition based on human abilities might find this unremarkable, it is\\nactually quite unusual for an ML model and this is why the pretraining-fine\\ntuning paradigm was the usual approach in previous work  Of course the train-\\ning set already contains many examples of QA pairs  More striking are tasks\\nwhich are not much represented in the training set, such as finding anagrams\\nor rearranging letters in words  One can even do in-context “meta-learning” of\\nmachine learning tasks such as linear regression (see §4) \\nOnce it is established that the model can generalize from a few examples, a\\nfurther step towards human capabilities is to try zero examples, instead simply\\nexplaining the task in natural language  At this point it becomes difficult to\\nclassify the tasks – should we consider the task of writing code from a natural\\nlanguage specification to be a form of translation, or an example of explaining\\nthe task, or something else? The relation between the input text or “prompt”\\n17A quantitative version of this claim is that performance for the “emergent” capability\\nimproves rapidly at some threshold value of the word prediction loss  This claim is disputed,\\nsee  for discussion \\n11and the output has many surprising features  For example, a standard tech-\\nnique in LLM question answering which measurably improves performance is to\\nprecede the question with a prompt such as “I will answer this question help-\\nfully and truthfully ” Is this somehow biasing the network towards certain texts\\nand away from others (after all the internet corpus is hardly a reliable source of\\ntruth) ? Suppose we have a theory of how this works, how can we test it? Does\\nthe model “know” anything about the truth of statements? \\nAs has been much reported, one of the major difficulties in using LLMs\\nfor practical tasks is their propensity to invent facts (especially citations) and\\ntheir limited ability to do logical reasoning, algebra and other symbolic tasks \\nA device for improving this, called “chain of thought prompting,” is to give\\nexamples (say of question answer task for definiteness) with some intermediate\\nreasoning steps spelled out  This was used in the Minerva QA system \\nwhich produced the example in Figure 1  Still the fraction of problems it solved\\ncorrectly is around 50% (the later GPT-4 is similar)  Even for simpler questions,\\nthe reliability of GPT-4 is more like 90%  Much current research is devoted to\\nthis problem of reliable reasoning, as we discuss in §8 \\n4 Phenomenology of language models\\nIn this section we discuss general claims, “non-invasive” experiments, and the-\\noretical arguments which do not depend on “microscopic details” of the models\\nsuch as the trained weights 18This includes evaluation of model capabiliities,\\nqualitative observations and scaling laws \\nWhat can LLMs do? There is a huge body of work on this question, and any\\nattempt to review it would rapidly go out of date, but let us review the primary\\nmethod for studying it  This is benchmarking, the development of standardized\\nsets of test items for which model accuracy can be evaluated in a reproducible\\nway  This is in principle straightforward if the input corresponds to a single\\ncorrect output, as in multiple choice question answering 19If the answer is free-\\nform text, one can use text comparison metrics such as the ROUGE score  One\\ncurrent standard for evaluating LLMs, BIG-bench , combines 204 language\\ntasks (at first publication; they accept new tasks) including translation, QA,\\npuzzle solving, text classification and summarization, and tests of common sense\\nreasoning  A leaderboard listing the current best LLMs is at20  Another is the\\nEleutherAI “Language Model Evaluation Harness”21and leaderboard 22The\\nbenchmark suite HELM  measures additional metrics such as tendency to\\nrepeat copyrighted material, bias, toxicity and the like \\n18We are calling this “phenomenology” following the physics use of the term, not its use in\\npsychology and philosophy to describe the study of subjective experience \\n19A potential pitfall is that after a benchmark is published, the test items can find their\\nway into future training data and then be solved by memorization  Methods to detect and\\nprevent this are discussed in the references co/spaces/HuggingFaceH4/open llmleaderboard\\n12Reasoning ability is of particular interest for mathematical and scientific ap-\\nplications – of course we all look forward to the day when computers will help us\\ngrade assignments, referee papers and do our research  There are many bench-\\nmarks for solving logical problems expressed in natural language  Benchmarks\\nfor mathematical theorem proving include NaturalProofs , MiniF2F \\nand ProofNet ; as of mid-2023 LLMs (and the best other systems) can find\\nmany proofs (20–80%) but still fail on some seemingly easy cases  Simpler as-\\npects of reasoning which have benchmarks are the ability to deal with negation\\n, consistency (between different phrasings of the same question) , and\\ncompositionality (the ability to analyze statements and problems into simpler\\nparts, solve these and combine the results)  \\nNatural language tasks are very complex, and benchmarks constructed from\\nreal world data cannot be used directly in theoretical considerations  For this\\npurpose one generally defines “toy worlds” and generates synthetic data  The\\npossibilities are endless, but some which have been used are arithmetic prob-\\nlems (decimal arithmetic; modular arithmetic), game play, solving systems of\\nequations, and parsing formal languages  A particularly interesting task is lin-\\near regression ; since this is the prototypical case of statistical inference, a\\nsystem which learns to do it can be said to be “learning how to learn ”\\nComing to scaling laws, denote the model size (number of parameters) as\\nPand the dataset size (number of tokens in the corpus) as D, then there are\\ntwo general regimes  If we hold one of these (say P) fixed and take the other\\n(sayD) to infinity, then a law of large numbers applies and L ∼ 1/D  On the\\nother hand, if we take one parameter very large and study the dependence on\\nthe other, nontrivial power law scaling can emerge  In principle one can get\\ndifferent exponents for DandP, suggesting the ansatz\\nL(P, D) =\"\\x12Pc\\nP\\x13αP/αD\\n+Dc\\nD#αD\\n  (4)\\nwhere Lis test loss Eq  3 computed in an optimally regularized model 23This\\nis a good fit to Figure 2 \\nWhile in Figure 2 the two exponents appear to differ, there is not really\\nconvincing evidence that this is significant  Before working hard on this, one\\nshould ask if there is any way to control the many choices involved, so as to define\\nuniversal exponents  One context in which this can be studied systematically is\\ntransfer learning, by distinguishing the dependence on the pretraining and fine\\ntuning datasets   Another relevant and practical question is whether one\\ncan prune the dataset to improve the scaling  It is intuitively plausible and can\\nbe shown in examples that sets of data items are worth more if they are diverse\\nthan if they are similar  The challenge is to find simple ways to quantify this\\nsimilarity; in  many proposals are studied \\n23Regularization is a standard technique in statistics and ML used to control overfitting by\\nmodels with too many parameters  If one does not regularize one sees other phenomena such\\nas double descent   For further discussion see  \\n13Scaling laws can arise in many ways, not specific to language models  One\\nhypothesis is that the data lies on a low dimensional submanifold in a higher\\ndimensional space 24Both the number of parameters and the number of points\\nrequired to fit this manifold go as the dimension dof the manifold, and this\\nleads to αP=αD= 4/d(the precise coefficient 4 depends on assumptions\\nabout smoothness)  \\nA related hypothesis is that the spectral density of the data covariance falls\\noff as a power law, and in  Eq  4 is derived for a random feature model with\\nthis covariance  This hypothesis follows from the low dimensional hypothesis\\nbut it is more general, for example these authors argue that additional features\\nderived from the data (as in nonlinear models such as FFN’s) generally have\\nthe same spectrum as the original data  One can also try to relate Eq  4 and\\ncorrections to it to hypotheses about how tasks are learned  \\nWhat does the scaling of the information theoretic quantity Eq  3 have to\\ndo with performance on tasks requiring intelligence? A priori , not much, but\\none way to motivate a focus on it is to draw an analogy with particle physics \\nIn the 30’s cosmic ray observations gave strong hints of new physics at higher\\nenergies, but the interesting events were too rare and uncontrolled to draw solid\\nconclusions  Thus physicists were motivated to build accelerators  These are\\nnot that expensive when they fit on a tabletop, but rapidly grow in size and\\ncost  How large does an accelerator need to be? The right measure is not its\\nsizeper se but rather the energy of the particles it can produce  The physics\\nrelating size and energy is not trivial (due to effects such as synchrotron ra-\\ndiation) but can be worked out, so one can make a good prediction of energy\\nreach  Still, as one increases energy, will one find a smooth extrapolation of\\nwhat came before, or will one discover qualitatively new phenomena? In the\\ngolden age of accelerator physics (the 50’s-70’s) much new physics was discov-\\nered, mostly associated with new particles which are produced only above sharp\\nenergy thresholds  Currently the highest energy accelerator is the Large Hadron\\nCollider at Cern, where the Higgs particle was discovered in 2012  While we\\nare still waiting for further important discoveries, the potential for discovery is\\ndetermined by measurable properties of the accelerator – by energy and secon-\\ndarily by intensity or “luminosity” – which we can judge even in the absence of\\nqualitative discoveries  In the analogy, perplexity is playing a similar role as an\\nobjective measure of language model performance defined independently of the\\nmore interesting qualitative behaviors which reflect “intelligence ”\\nHow far can one push this analogy? Could perplexity be as central to lan-\\nguage as energy is to physics? Eq  3 has a fairly objective definition, so the\\nidea is not completely crazy  But, not only was its relation to performance on\\nactual tasks not predictable in advance, even after the fact clear “thresholds” or\\nother signals for emergence of tasks have not yet been identified   Perhaps\\nif there are universal thresholds, evidence for them could be seen in humans 25\\nMore likely, additional variables (the quality and nature of the training corpus,\\n24In§5 we explain how text can be thought of embedded in a high dimensional space \\n25Thanks to Misha Tsodyks for this suggestion \\n14details of the tasks, etc ) would need to be controlled to see them  This is\\nanother question probably better studied in simpler tasks using synthetic data \\nThe final topic we discuss is the behavior of the objective function (Eq  3)\\nas a function of training time 26In almost all ML runs, such a plot shows long\\nplateaus interspersed with steep drops  This has been interpreted in many ways,\\nranging from evidence about the nature of learning, to a simple consequence of\\nrandomness of eigenvalues of the Hessian of the loss function  A more recent\\nobservation is to compare training and testing accuracy on the same plot  In\\n it was argued that these two metrics improve at two distinct stages of\\ntraining  First, the model memorizes training examples  Later, it generalizes\\nto the testing examples  This “grokking” phenomenon has been suggested as\\nevidence for learning of circuits , an idea we discuss in §7 \\n5 Simpler language models\\nHere we describe a few generative language models in detail to fix the concepts \\nAs points of notation, let Wbe the set of words (or, if the reader prefers,\\nnumbers which index a position in a list of words)  We denote the cardinality of\\na setSas|S|, so|W|is the number of distinct words  The space of N-component\\nreal vectors is denoted RN \\nThe simplest model is the N-gram model defined in terms of the conditional\\nprobabilities\\nP(wN|w1w2  w N−1), (5)\\nwhich are all taken to be independent  Given this minimalist assumption, a\\nplausible way to estimate them from the corpus is\\nP(wN|w1w2  w N−1) =Number of occurrences of w1w2  w N−1wNP\\nwNumber of occurrences of w1w2  w N−1w \\n(6)\\nThis simple model with N= 3 or 4 works better than one might think (see exam-\\nples in ) and can be improved a bit by simple statistical tricks (“smoothing”) \\nBut the exponential growth of the number of strings in Nmeans that there is\\nno hope of taking Nlarge enough to model even a single paragraph  The entire\\ninternet contains (in order of magnitude) 1012words, and such a corpus will\\ncontain only a vanishingly small fraction of the likely twenty word strings 27\\nA more general principle which we can take from the N-gram model is the dis-\\ntributional hypothesis, which has been pithily summarized as “you shall know\\na word by the company it keeps ”  In other words, by proper use of the\\nstatistics of neighboring words, one can define quantities which capture prop-\\nerties and even the meanings of words  The simplest expression of this idea\\n26This is roughly the time in which the gradient descent operates, see Eq  16  In LLMs one\\noften considers each data item only once in a training run, so it is related to (but different\\nfrom) dataset size \\n27Statistical estimates of perplexity are in the 100’s, and the best current LLMs have per-\\nplexity ∼20 \\n15is the co-occurrence matrix  Before explaining this, let us mention a detail of\\npractical systems, which in place of words use “tokens,” meaningful components\\nof words  A physics illustration is the word “supersymmetrization ” Even for a\\nnon-physicist reader encountering it for the first time, this word naturally breaks\\nup into “super,” “symmetry” and “ization,” pieces which appear in many words\\nand which are called tokens  And not only does this decomposition apply to\\nmany words, it helps to understand their meaning  This process of replacing\\nsingle words by strings of tokens (“tokenization”) is a first step in LLM pro-\\ncessing, and henceforth when we say “word” we will mean word or token in this\\nsense \\nGiven a corpus, we define its N-gram co-occurrence matrix MNto be the\\n|W| × |W| matrix whose ( w, w′) entry counts the number of N-grams in the\\ncorpus containing both words  This matrix defines a map from words to vectors\\nι:W →Rp(7)\\n(where the dimension p=|W|), by taking a word to the corresponding column\\nofMN  Such a map is called a word embedding \\nApplying this map to each word independently, we can map a string of k\\nwords (in Wk) to a string of vectors, and this is the next step (after tokenization)\\nof LLM processing  One might worry that these are very high dimensional\\nvectors with many zero entries, which seems wasteful  A standard statistical\\ncure for this problem is to do principal component analysis (PCA)  In words,\\ninstead of columns of MNwe use the columns of a p×|W| matrix Zchosen such\\nthatZtZis the best rank papproximation to MNin the sense that it minimizes\\ntr (ZtZ−MN)2  One can do better, but this gives the right idea \\nNext, we feed this string of vectors into some machine learning model to get\\nan output which we use to predict the next word  If we just want the most\\nlikely next word, a good way is to output a vector v∈Rp, and choose the\\nword wwhich maximizes the inner product v·ι(w)  We denote this relationship\\nasv∼ι(w)  More generally, the standard inverse map from a vector to a\\nprobability distribution on words is the Boltzmann distribution on the inner\\nproducts  Explicitly, we postulate an inverse temperature β= 1/Tand take28\\nv→P(w) =eβv·ι(w)\\nP\\nw′eβv·ι(w′)(8)\\nHere is an observation  which supports the idea that word embeddings\\ncontain information about meaning  Since the embeddings are vectors, they can\\nbe added  Consider the following equation:\\nι(king) −ι(man) + ι(woman) ∼ι(?) (9)\\n28Tis the temperature parameter which can be set in (say) the GPT user interface  Also,\\nthis ratio of exponentials is usually called “softmax” in machine learning as its β→ ∞ limit\\nis the “argmax” function producing a vector whose nonzero components have the same index\\nvalues as the largest of the input(s) \\n16One might hope that the word which maximizes this inner product is “queen,”\\nand indeed it is so  There are many more such examples; empirically one needs\\nthe dimension p≳100 for this to work  One can argue  that it follows\\nfrom relations between co-occurence statistics:29\\n∀w,MN(w,king) /#(king)\\nMN(w,queen) /#(queen)≈MN(w,man) /#(man)\\nMN(w,woman) /#(woman)(10)\\nGiven these ideas and a map Ffrom a list of vectors to a vector, we can\\nnow propose a very general class of L-gram autoregressive language models as\\nthe combination of the following steps:\\n1  Map the Linput words witoLvectors ι(wi) \\n2  Apply Fto the list of these vectors to get a prediction vector v \\n3  Use the inverse map Eq  8 to get a probability distribution over words \\nFurthermore, if the map Fhas parameters, given a corpus we can determine\\nthem by optimizing the function Eq  3 with respect to the parameters  And once\\nwe bring in optimization, we can also optimize with respect to the coefficients of\\nthe embedding map Eq  7, so that we can dispense with co-occurence statistics \\nThis is the general prescription followed by the LLMs, and to complete it we\\njust need to specify a family of maps F  One possibility is to use a general (fully\\nconnected) feed forward neural network (FFN, also called MLP for multilayer\\nperceptron)  We recall that an FFN is a composition of two general types of\\nfunctions, linear maps Wiand nonlinear maps θ, so that\\nF(v) =Wd◦θ◦Wd−1◦θ◦  (11)\\nIn more concrete terms, the maps Wiare multiplication by rectangular matrices\\nof parameters (usually called “weights” in this context), while the maps θact\\nindependently on each component of their input vector by a fixed nonlinear\\nfunction such as tanh or (more typically) ReLU (identity for x≥0 and zero\\nforx <0)  The main fact we recall about FFN’s is that, in the limit that the\\nnumber of parameters becomes large, they can approximate any given function\\narbitrarily well   We refer the reader interested in learning more to  \\nWe can get a very natural deep learning version of the L-gram models by\\nusing an FFN for the map Fin the prescription above   Since this asked for\\na map from a list of vectors to a vector, we need to convert the input list into a\\nsingle vector  This is easy: we can take the direct sum of the input vectors, i e \\nthe dimension L×pvector whose components are the concatenated lists of their\\ncomponents  Using today’s FFNs, one could implement this with L∼100 or so \\nThere does not seem to be much work on large fully connected FFN language\\nmodels, because by the time the technology advanced to this point the far more\\nefficient transformer models had taken over  Still, they illustrate the general\\n29Here #( w) denotes the number of occurences of “ w” in the corpus  These ratios can also\\nbe expressed in terms of the pairwise mutual information, PN(w, u)/P(w)P(u) \\n17idea and also one of its most obvious limitations  Even with L∼100, often\\npredicting the next word requires remembering words which appeared farther\\nback  To solve this problem we need to incorporate some sort of memory into\\nthe model \\nThe simplest memory is an additional state variable which is updated with\\neach word and used like the other inputs  To do this, we should take the state\\nto be a vector in some Rq  This brings us to the recurrent neural network or\\nRNN  Its definition is hardly any more complex than what we saw before  With\\neach word position (say with index i) we will associate a state vector siwhich\\ncan depend on words up to wiand on the immediately previous state  Then,\\nwe let the map Fdetermine both the next word and the next state as\\n(vi+1, si+1) =F(si, vi, vi−1, vi−2,   , v i−k+1), (12)\\nwhere the parenthesis notation on the left hand side means that the output\\nvector of Fis the concatenation of two direct summand output vectors  12 is a discrete dynamical system  If we grant that\\nFcan be an arbitrary map, this is a very general class of systems  One way\\nof characterizing its generality is through computational complexity theory, by\\nasking what classes of computation it can perform  In  it was argued that\\nthe RNN is a universal computer, but this granted that the computation of F\\nin Eq  11 could use infinite precision numbers  Under realistic assumptions\\nthe right complexity class is a finite state machine, which can recognize regular\\nlanguages   We will say more from this point of view in §7 \\nThere are many variations on the RNN such as LSTM’s , each with their\\nown advantages, but we must move on \\n6 Recipe for an LLM\\nWe are now ready to define the transformer model 30It is simply another class\\nof maps Ffrom lists of vectors to a vector to be used in the prescription above \\nIndeed, it is a natural generalization of the FFN which is associated to permu-\\ntational symmetry  This is in direct analogy to the use of convolutional neural\\nnetworks (CNNs) for image recognition, which are FFNs which are equivariant\\nunder the symmetry of translations in two dimensions which is natural for the\\nset of images \\nA transformer is a composition of two types of functions (layers) taken in\\nalternation, each mapping an input list of Lvectors {ui}to an output list of L\\nvectors {vi}  One of these is an FFN as previously discussed, but now applied\\nto each embedding vector independently, so vi=FFFN(ui) \\n30Other reviews explaining these definitions include  \\n18The other layer type is called attention, and it is defined as follows:\\n{ui} → { vi=WiX\\nj=1ci,juj} (13)\\nci,j≡expui·B·ujPi\\nj=1expui·B·uj(14)\\nwhere Bis a learnable matrix whose elements are model parameters (equiva-\\nlently, u·B·vis a bilinear form) and Wis a linear map (also learnable) \\nIn words, an item viin the output vector is (a linear transformation of) a\\nweighted sum of the inputs ujwith i≤jand can depend on any of them 31\\nThe weights ci,jare given by a “softmax” or Boltzmann weight just as in Eq \\n8  Thus there is a very general learnable way for each output to choose which\\nof the input vectors are most useful as inputs  Suppose the product u·B·vis\\nthe dot product, then attention selects the input components ujmost similar\\nto the current unit’s input, uj∼uiin the notation of §5  The matrix Ballows\\nfor comparing different parts of the embeddings and ignoring other parts, in a\\nway determined by optimizing the objective function Eq  3 \\nComposing these two types of functions (or layers) produces a map from\\nRp×LtoRp×L  Often one takes, instead of the pure FFN and attention func-\\ntions, sums of these with the identity function (residual connections)  The FFNs\\ngenerally have a single hidden layer which can be of a different dimension, call\\nthisph 32Finally, while the language model prescription asked for a map to Rp,\\nthis is easily obtained by just taking the last vector in the final output list \\nThere are two more essential details to cover (and many minor details we\\nwill skip)  The first is the concept of “attention head ” The definition Eq  13\\nallowed for a general linear transformation Wwhose range is the output vector \\nWe are free to choose its dimension, call it q, and typically one takes this to\\nbe much less than the embedding dimension p  In return one can use many\\ncopies of Eqs  13,14 in parallel with different choices for BandW, to produce\\nmany outputs  One can then concatenate these outputs to get a final output of\\ndimension p  These copies are called attention heads and we will denote their\\nnumber by H, sop=Hq \\nThe second essential detail is that, so far, there is nothing in the definition\\nthat keeps track of the order of the list of input vectors; the output of Eq  13\\nwill be invariant under a general permutation of the input vectors  While this\\nis an elegant property, it is not what we want for processing language, for which\\nthe order of the words matters  The cure for this is very simple: one takes as\\ninputs not the word embeddings Eq  7, but the direct sum (concatenation) of\\nthese with positional embedding vectors, i e vectors which encode the position\\n(index) of the word in the string  These can be a combination of sines and\\n31The restriction i≤jto previous or current inputs is done to get an autoregressive model;\\none can relax this for other purposes \\n32Explicitly, vi=W1·max(0 , W0·ui+b0) +b1, where b0,1are more learnable parameters \\n19cosines of various frequencies, such as \\n(e2i−1, e2i) = ( cosposition\\n100002i/dpos,sinposition\\n100002i/dpos); i∈ {1,   ,dpos\\n2}(15)\\nOne could instead treat these vectors as learnable parameters  Still, the trig\\nfunction basis for positions may be significant  It has been generalized to rep-\\nresent other graph structures by using eigenfunctions of the graph Laplacian as\\npositional embeddings \\nThe invariance of the transformer model under permutation symmetry is\\nreminiscent of the point we mentioned earlier, that translation symmetry mo-\\ntivates the CNN  However permutation symmetry is badly broken in language,\\neven in the simplest formal languages,33and it is not obvious why this should\\nbe a useful property for the model to have  One might argue that although any\\nparticular language breaks permutation symmetry, it acts naturally on the en-\\nsemble of languages and thus should have a simple representation  For example,\\nbesides the usual infix arithmetic notation “ a+b”, one could instead use prefix\\n“+a b” or postfix “ a b+”  Translating between these notations is arguably\\neasier for permutation invariant maps using position embeddings  An oppos-\\ning view would be that permutation symmetry is just a secondary property of\\nthe simplest model using attention, and that the main point is to explain the\\nvalue of attention  In addition to its ability to select similar items, it provides\\na simple way to take products of embedding vectors  In computational com-\\nplexity terms, attention enlarges the class of circuits which can be simulated by\\na constant depth transformer   Physics analogies of Eqs  13,14,\\nespecially to the Hopfield model, may be important  \\nA major practical advantage of the transformer over the RNN and other\\nprevious architectures is that the computations in the attention mechanism can\\nbe done in parallel, so (given sufficiently many processors) the time required does\\nnot increase with the window length L  This is by contrast with the RNN in\\nwhich information propagates from one word to the next, so a window of length\\nLrequires time Lto process  On the other hand the ability of each unit to pay\\nattention to every previous unit means that the total computation required by\\nthe transformer scales as L2  This is the limiting factor for increasing Land\\nthis is widely seen as a problem  There has been a lot of work to improve this\\nscaling, by removing some of the connections (as in sparse attention ), by\\nintroducing multiscale structure, or in other ways \\nLet us summarize by listing the hyperparameters34and their values for the\\nlargest (175B) GPT-3   They are\\n•Embedding dimension p= 12288 and hidden layer dimension ph= 4p \\n•Window length L= 4096 or 8192 \\n•Depth D= 96, counting both FFN (Eq  11) and attention (Eq  13)\\nlayers 35\\n33Compare the logical implications A→BandB→A \\n34This term refers to model choices which are not learned through gradient descent \\n35Some of the attention layers in GPT-3 are sparse \\n20•Number of heads H= 96 (the equality with Dis a coincidence as far as I\\nknow) \\nThe total number of parameters is roughly 12 Dp2 \\nAs mentioned earlier, all of these parameters, and the parameters of the em-\\nbedding map Eq  7, are determined as follows  One generally starts with “ran-\\ndom” initial conditions, usually meaning that each parameter is drawn from a\\nnormal distribution with mean zero and variance chosen so that the linear maps\\nhave expected norm independent of the hyperparameters  As in random matrix\\ntheory, this typically means var( Wi,j)∼1/p, though there are refinements  \\nOne then sequences through the training corpus and performs a step of gradient\\ndescent of Eq  3 for each “batch” of words (here a group of ∼106words)  In\\neach step, the parameters ⃗θare modified as\\n⃗θ→⃗θ−η∂Lb\\n∂⃗θ(16)\\nwhere Lbis Eq  3 restricted to the batch, the conditional probability Pcomes\\nout of Eq  8 applied to the output of the transformer, and ηis a positive real\\nnumber (the learning rate hyperparameter, here around 10−4) \\nThe result of following this procedure on a dataset of natural language text,36\\nsupplemented by many enhancements which are described in the literature and\\nin the model source codes but which may be less important for conceptual\\nunderstanding, is an LLM with the capabiliities we described \\n7 Studying the internal workings\\nThe success of this procedure raises many questions  Some can be asked about\\nmore or less any ML model – for example, questions about when and how\\noptimization of the objective function Eq  3 achieves “good” local minima\\n(value near the global minimum and models which generalize well), and the\\norigin of scaling laws like Eq  4  These are the subject of the general theory of\\nmachine learning, for which we refer to  and much other work \\nOther questions, and understanding the many striking abilities discussed\\nearlier, sound more specific to LLMs  What would it mean to understand how\\nChatGPT writes poetry based on prompts, or solves physics word problems?\\nAt present this is by no means clear and it may be that entirely new concepts\\nare needed to do this  Still, I share the belief that we can go very far towards\\nunderstanding LLMs by building on previous work in computer science, ma-\\nchine learning and AI, and many other fields  There is a well established field\\nof statistical physics and ML  which will surely contribute  Physics ideas\\nare also very relevant for tasks with spatial symmetry, such as image genera-\\ntion  and recognition   The unexpected mathematical simplicity of the\\n36As always in ML it is important that the dataset be “clean” – consistently tokenized, not\\nhaving too much garbage text or repetitions, etc  Many later LLMs also use programming\\nlanguage code in the dataset  Besides making code generation possible, it has been reported\\nthat this improves performance on natural language reasoning tasks \\n21transformer model means that mathematical insights could be valuable  We can\\nalso follow approaches used in neuroscience, psychology, and cognitive science \\nAn evident observation is that the paradigm of neuroscience – careful study\\nof the microscopic workings of the system, following a reductionist philosophy\\n– is far more practical for ML models than it is for human brains, as the micro-\\nscopic workings are fully explicit  This is not to say that it is easy, as we still face\\nthe difficulty of extracting meaning from a system with billions of components\\nand parameters  How could we do this for LLMs?\\nOne familiar starting point in neuroscience is to measure the activity of\\nneurons and try to correlate it with properties of the system inputs or outputs \\nThe “grandmother cell” which fires when a subject sees his or her grandmother\\nis an extreme (and controversial) example  Better established are the “place\\ncells” in the hippocampus which fire when an animal passes through a specific\\npart of its environment \\nGenerally there is no reason why the representation should be so direct; there\\nmight be some “neural code” which maps stimuli onto specific combinations\\nor patterns of activity  The details of the neural code could even be different\\nbetween one individual and the next  Analogous concepts in LLMs are the maps\\nfrom input strings to intermediate results or “activations ” The first of these\\nis the embedding map Eq  7  Considering each layer in succession, its outputs\\n(sometimes called “contextualized embeddings”) also define such a map  The\\ndetails of these maps depend on details of the model, the training dataset and\\nthe choices made in the training procedure  Besides the hyperparameters, these\\ninclude the random initializations of the parameters, the order in which data\\nitems are considered in training and their grouping into batches  Even small\\ndifferences can be amplified by the nonlinear nature of the loss landscape \\nOne way to deal with this indeterminacy is to look for structure in the maps\\nwhich does not depend on these choices  The linear relations Eq  9 between word\\nembeddings are a very elegant example, telling us (and presumably the model)\\nsomething about the meanings of the words they represent  Moving on to the\\nlater layers, one can ask whether contextualized embeddings carry information\\nabout the grammatical role of a word, about other words it is associated to\\n(such as the referent of a pronoun), etc  One can go on to ask whether any\\nof the many structures which – one would think – need to be represented to\\nunderstand the real world, are visible in these embeddings \\nMany structures are too intricate to show up in linear relations  A more\\ngeneral approach is to postulate a “target” for each training data item and\\ntrain a “probe” model (usually an FFN) to predict it from the embeddings  If\\nthis works, one can go on to modify the internal representation in a minimal way\\nwhich changes the probe prediction, and check if this leads to the corresponding\\neffects on the output (see  and references there) \\nThis procedure is simpler to explain in an example  A pretty example of\\nprobing for a world model is the recent work of Li et al  (see also ) on\\nrepresentations in a transformer model trained to play the board game Othello 37\\n37For readers not familiar with this game, two players alternate in placing black and white\\n22They train a model “Othello-GPT”38to take as input a sequence of 60 legal\\nmoves, for example “E3 D3  ” in the standard algebraic notation, and at each\\nstep to predict the next move  The trained model outputs only legal moves\\nwith very high accuracy, and the question is whether this is done using internal\\nrepresentations which reflect the state of the game board, say the presence\\nof a given color tile in a given position  Following the probe paradigm, they\\nobtain FFNs which, given intermediate activations, can predict whether a board\\nposition is occupied and by which color tile  Furthermore, after modifying\\nthe activations so that the FFN’s output has flipped a tile color, the model\\npredicts legal moves for the modified board state, confirming the identification \\nNeuroscientists can only dream of doing such targeted experiments \\nNumerous probe studies have been done on LLMs  One very basic ques-\\ntion is how they understand grammatical roles and relations such as subject,\\nobject and the like  This question can be sharpened to probing their internal\\nrepresentations for parse trees, a concept we review in the appendix  To get\\nthe targets for the probe, one can use a large dataset of sentences labeled with\\nparse trees, the Penn Treebank   This was done for BERT in  by\\nthe following procedure: denote the embedding (in a fixed layer) of word ias\\nui, then the model learns a projection Pon this space, such that the distances\\nd(i, j)≡ ||P(ui−uj)||in this inner product well approximate the distance be-\\ntween words iandjdefined as the length of the shortest path connecting them\\nin the parse tree  For BERT (with d∼1000) this worked well with a projection\\nPof rank ∼50 \\nOnce one knows something about how information is represented by the\\nmodels, one can go on to try to understand how the computations are done  One\\napproach, also analogous to neuroscience, is to look for specific “circuits” which\\nperform specific computations  An example of a circuit which appears in trained\\ntransformer models is the induction head   This performs the following\\ntask: given a sequence such as “ A B   A ” it predicts a repetition, in this\\nexample “ B ” The matching between the tokens (the two A’s in the example) is\\ndone by attention  A number of works have proposed and studied such circuits,\\nwith various motivations and using various theoretical lenses: interpretability\\nand LLMs , in-context learning , formal language theory ,\\ncomputational complexity theory , etc \\nReverse engineering a large network ab initio ,i e with minimal assumptions\\nabout what it is doing, seems challenging, but maybe automated methods will be\\ndeveloped   Another approach is to first develop a detailed computational\\nmodel (CM) to perform a task without looking too much at the system under\\nstudy, and then look for evidence for or against the hypothesis that the system\\nunder study uses it  This approach also has a long history in neuroscience \\nand ways to test such hypotheses have been much discussed  As an example\\ntiles on an 8 ×8 board, and each move results in “flipping” some opponent pieces to the\\nplayer’s color  The main point for us is that the function from moves to board state is easily\\ncomputable yet very nonlocal and nonlinear \\n38While this model shares the GPT architecture, it is not trained on any language data,\\njust on Othello games \\n23of a research tactic which does not require opening the black box, one can\\nconsider illusions which fool the system in some way  The response to these\\nwill often depend on contingent and non-optimal aspects of the model, so one\\ncan distinguish different models which solve the same task  A new class of\\npredictions which becomes testable for LLMs is to look at performance as a\\nfunction of model size (depth; number of parameters)  A particular CM might\\nrequire a certain model size or dataset properties in order to perform well  And\\nof course, one can open the black box: by assuming a particular CM, one can\\nmake predictions for what probe experiments should work \\nSimple tasks studied in this approach include modular addition  and\\nlinear regression , where several CM’s (gradient descent, ridge regression and\\nexact least squares) were compared  Turning to language processing, a CM for\\nparsing by transformer LLMs was developed in Zhou et al   While this\\nis too lengthy to explain in detail here, let us give the basic idea, starting\\nfrom the PCFG framework discussed in the appendix  Rather than try to\\nrepresent a parse tree in terms of nodes and edges, it is represented by giving\\neach position iin the list of words a set of variables αi,t,j, where tindexes a\\nnonterminal (a left hand side of a rule) and jis another position  If αi,t,jis\\nturned on, this means that a rule with ton the l h s  was used to generate\\nthat part of the tree stretching from position ito position j  This can be\\ngeneralized to let αi,t,jbe the probability that a rule is used  These variables\\n(and additional variables βdescribing the rules used higher in the tree) satisfy\\nsimple recursion relations (the Inside-Outside parsing algorithm )  If the\\nrules have at most two symbols on the r h s ,39these recursion relations are\\nquadratic in the variables  By encoding the αvariables as components of the\\nembedding, they can be implemented using attention \\nNaively, this model predicts that embedding dimension pmust be very large,\\nof order the number of nonterminals times the length of a sentence  Since\\nrealistic grammars for English have many hundreds of nonterminals, this seems\\nto contradict the good performance of transformers with p∼1000  This problem\\nis resolved by two observations, of which the first is that one can get fairly good\\nparsing with many fewer ( ∼20) nonterminals  The second is compression, that\\nembeddings and circuits which are simple and interpretable can be mapped into\\nmore “random-looking” lower dimensional forms  This is a well understood\\nconcept for metric spaces , which was implicit in the discussion of word\\nembeddings in §5  There the simplest construction (the co-occurence matrix)\\nproduced vectors with one component for each word, but by projecting on a\\nsubspace one could greatly reduce this dimension with little loss in accuracy \\nThe generalization of these ideas to neural networks seems important \\nOnce one believes an LLM is carrying out a task using a particular circuit\\nor CM, one can go on to ask how it learned this implementation from the data \\nOne can get theoretical results in the limit of infinite training data and/or for\\nsimple tasks in which the dataset is constructed by a random process  Learning\\n39One can rewrite any grammar to have this property (Chomsky normal form) by introduc-\\ning more nonterminals \\n24in transformer models trained on realistic amounts of data is mostly studied\\nempirically and using synthetic data  A few recent interesting works are   Intuitively one expects that simpler instances of a task are learned first,\\nallowing the model to learn features which are needed to analyze more complex\\ninstances, and there is a lot of evidence for this  The idea that many submodels\\ncan be learned simultaneously, including straight memorization and submodels\\nwhich rely on structure, also seems important  Ultimately learnability is crucial\\nbut we should keep in mind that in analogous questions in physics, evolution,\\nand so on, it is much easier to understand optimal and critical points in the\\nlandscape than to understand dynamics \\nThis brings us to in-context learning, the ability of an LLM to perform\\ndiverse tasks given only a few examples of input-output pairs  The simplest\\nhypothesis is that the model has learned the individual tasks, and the examples\\nare selecting a particular task from this repertoire  It has been argued that\\nthis is guaranteed to happen (in the infinite data limit) for a model trained\\non a mixture of tasks   If the many tasks have common aspects (for\\nexample parsing might be used in any linguistic task), one can ask how the\\nmodel takes advantage of this, a question discussed in  \\nUnderstanding LLMs is a very active research area and there is much more\\nwe could say, but let us finish by summarizing the two main approaches we\\ndescribed  One can postulate a representation and a computation designed to\\nperform a task, and look for evidence that the LLM actually uses the postu-\\nlated structure  Alternatively, one can look for a function in some simpler class\\n(such as digital circuits) which well approximates the function computed by the\\ntransformer model, and then “reverse engineer” the simpler function to find\\nout what it is doing  Either or both of these procedures could lead to inter-\\npretable systems and if so, are answers to the question “what has the LLM\\nlearned ” There is no guarantee that they will work and it might turn out that\\none cannot understand LLMs without new ideas, but they deserve to be tried \\n8 Questions and discussion\\nLarge language models have revolutionized computational linguistics and opened\\nup many new applications of AI  Understanding how they work is both straight-\\nforward (we explained it in §6) and at the same time an outstanding scientific\\nchallenge  This is because the question “how do they work” has multiple mean-\\nings  On the one hand, LLMs are a relatively simple solution to the task of\\npredicting the likely next word in a text  On the other hand, they also seem to\\nperform many other tasks which require intelligence, such as solving the physics\\nword problem in Figure 1  While we do not have a strong understanding of\\nwhat a system which can perform these tasks must do, a vast body of work in\\ncognitive science and AI supports one’s first naive intuition that such a system\\nmust be doing sophisticated analyses of language, must contain models of the\\nreal world, and must be able to do fairly general logical reasoning  Before it\\nwas demonstrated, the idea that all this could be learned as a byproduct of\\n25word prediction would have seemed hopelessly optimistic, had anyone dared to\\nsuggest it \\nExtraordinary claims should be greeted with skepticism  One must guard\\nagainst the possibility that a successful ML system is actually picking up on\\nsuperficial aspects or statistical regularities of the inputs, the “clever Hans”\\neffect  Addressing this is an important function of the benchmark evaluations\\ndiscussed in §4  Of course as LLMs get good at performing tasks of practical\\nvalue, the skeptical position becomes hard to maintain \\nIntelligence and language are incredibly complex and diverse  According to\\nMinsky,40this diversity is a defining feature of intelligence  The goal of under-\\nstanding LLMs (or any general AI) will not be accomplished by understanding\\nall of the content in their training data, the “entire internet ” Rather, the trick\\nwe need to understand is how a single system can learn from this diverse corpus\\nto perform a wide range of tasks  Theories of “what is learnable” are a central\\npart of computer science   Although theoretical understanding has a long\\nway to go to catch up with LLM capabilities, for simpler and better understood\\ntasks much is known \\nIn these notes we mostly looked at this question through the lens of computer\\nscience, and took as the gold standard for explaining how an LLM learns and\\nperforms a task, a computational model expressed as an algorithm or a circuit\\ntogether with arguments that the trained LLM realizes this model  This point of\\nview has many more insights to offer, but before we discuss them let us consider\\nsome other points of view  In §7 we drew the analogy between detailed study\\nof transformer circuits and neuroscience – what others can we consider?\\nAnother analogy is with cognitive psychology  LLMs are sufficiently human-\\nlike to make this interesting, and there is a growing literature which applies tests\\nand experimental protocols from psychology to LLMs, see for example  and\\nthe many references there  When discussing this, we should keep in mind the\\nvast differences between how humans and LLMs function  Human brains are\\nnot believed to use the backpropagation learning algorithm, indeed it has been\\nargued that biological neural systems cannot use it   Perhaps related to this,\\nbrains are not feed-forward networks but have many bidirectional connections \\nWhatever brains are doing, it works very well: LLMs (like other current deep\\nlearning systems) need far more training data than humans  Furthermore, the\\nLLMs we discussed do not interact with the world  Some argue that on philo-\\nsophical grounds, a model trained only on language prediction can never learn\\nmeaning   While I do not find this particular claim convincing, I agree that\\nwe should not assume that LLMs perform tasks the same way humans do  Still\\nboth similarities and differences are interesting; can we make the analogies with\\ncognitive psychology more precise?\\nOne analogy , is with the well known concept of “fast and slow think-\\ning” in behavioral psychology   To summarize, humans are postulated to\\nhave two modes of thought, “system 1” which makes fast, intuitive judgments,\\n40What magical trick makes us intelligent? The trick is that there is no trick  The power\\nof intelligence stems from our vast diversity, not from any single, perfect principle  \\n26and “system 2” which can focus attention and do calculations, logic, and plan-\\nning  While system 2 is more general and less error-prone, using it requires\\nconscious attention and effort  According to the analogy, LLMs implement sys-\\ntem 1 thinking, and are weak at system 2 thinking \\nIn  it is argued that LLMs have “formal linguistic competence” but not\\n“functional competence ” In plainer terms, they are solving problems by manip-\\nulating language using rules, but they lack other mechanisms of human thought \\nWhile it may be surprising that a purely rule-based system could do all that\\nLLMs can do, we do not have a good intuition about what rule-based systems\\nwith billions of rules can do \\nWhat are the other mechanisms? There is a long-standing hypothesis in cog-\\nnitive science, modularity of mind , according to which the human brain has\\nmany “mental modules” with different capabilities  These include a language\\nmodule of the sort that Chomsky famously advocated and many others, includ-\\ning one for geometric and physical reasoning, another for social reasoning and\\ntheory of mind, and perhaps others  Notably, formal logic and mathematical\\nreasoning seem to call upon different brain regions from those which specialize\\nin language , suggesting that these functions are performed by different men-\\ntal modules  One can thus hypothesize that LLMs have commonalities with the\\nhuman language module and might be useful scientific models for it,41but that\\nprogress towards human level capability will eventually stall without analogs of\\nthe other modules  \\nA related claim is that current LLMs, even when they perform well on bench-\\nmarks, do not construct models of the world  Consider reasoning about spatial\\nrelations – for example if A is in front of B is in front of C, then A is in front of\\nC  Such reasoning is greatly facilitated by representing the locations of objects\\nin space, perhaps in terms of coordinates, perhaps using “place cells” or in some\\nother non-linguistic way  If distance from the observer is explicitly represented\\nand used in reasoning, then it becomes hard to get this type of question wrong \\nConversely, to the extent that LLMs do get it wrong, this might be evidence\\nthat they lack this type of world model or cannot effectively use it \\nThere are many papers exhibiting LLM errors and suggesting such inter-\\npretations, but often one finds that next years’ model does not make the same\\nerrors  At the present rate of progress it seems premature to draw any strong\\nconclusions  My own opinion is that there is no barrier in principle to LLMs\\nconstructing internal non-linguistic models of the world, and the work  on\\nOthello-GPT discussed in §7 is a nice demonstration of what is possible  This\\nis not to say that any and all models can be learned, but rather that it might\\nbe better for now to focus on other significant differences between LLM and\\nhuman reasoning, of which there are many  I will come back to this below \\nIf LLMs and other connectionist systems do not work in the same way as\\nbrains, what other guidance do we have? In §7 we discussed one answer, the\\nhypothesis that they work much like the algorithms and circuits studied in\\n41Chomsky rejects this idea, saying that “The child’s operating system is completely differ-\\nent from that of a machine learning program ” (New York Times, March 8, 2023)  Perhaps trained LLMs implement algorithms like those de-\\nsigned by computational linguists, or perhaps new algorithms which were not\\npreviously thought up but which can be understood in similar terms  In either\\nversion this is still a hypothesis, but if we grant it we can draw on insights from\\ntheoretical computer science which apply to all such algorithms \\nComputational complexity theory  makes many statements and con-\\njectures about how the time and space required by a particular computation\\ndepends on the size of the problem (usually meaning the length of the input) \\nThe most famous of these, the P̸=NPconjecture, states (very loosely) that for\\nproblems which involve satisfying general logical statements, finding a solution\\ncan be much harder than checking that the solution is correct \\nFrom this point of view, a central question is the complexity class of circuits\\nwhich can be realized by constant depth transformers, meaning that the number\\nof layers does not grow with the window size  Roughly, this is the complexity\\nclass TC0of constant depth circuits with threshold gates   Of\\ncourse in an autoregressive LLM one can repeat this operation to compute a\\nsequence of words: thus the circuit defines the transition function of a finite\\nstate machine (FSM) where the state is the window, and the LLM has learned\\nto simulate this FSM  If a natural algorithm to perform a task is in a more\\ndifficult complexity class than the FSM can handle, this is a reason to think the\\ntask cannot be learned by this type of LLM  Conversely, one might conjecture\\nthat any task for which there is an algorithm in this class can be learned, at\\nleast in the limit of an infinite amount of training data \\nWhat about the lenses of pure mathematics, theoretical physics and allied\\nfields? Besides my own personal interest in them, these fields have made sub-\\nstantial contributions to statistics and machine learning, especially the interface\\nbetween statistical physics and machine learning is a vibrant field of research\\n  Spin glass theory made a very deep impact, starting with the Hopfield\\nmodel and developing into a far-reaching theory of optimization landscapes and\\ncomplexity  Random matrix theory is central to high dimensional statistics \\nand in many approaches to understanding deep learning   Mathematical\\napproaches to language such as  can reveal new structure and\\nprovide deeper understanding \\nAnother reason to think pure mathematics and theoretical physics have more\\nto contribute is that neural networks, transformers, and many of the models of\\nneuroscience, are formulated in terms of real variables and continuous mathe-\\nmatics  By contrast, computer science is largely based on discrete mathematics,\\nappropriate for some but not all questions  Perhaps word embeddings have im-\\nportant geometric properties, or perhaps the dynamics of gradient descent are\\nbest understood through the intuitions of continuous mathematics and physics \\nArguments such as those in §7 which reduce neural networks to digital circuits,\\neven if they do explain their functioning, may not be adequate to explain how\\nthey are learned \\nHaving at least mentioned some of the many points of view, let me combine\\nthese insights and speculate a bit on where this is going  Let me focus on\\nthree capabilities which seem lacking in current LLMs: planning, confidence\\n28judgments, and reflection \\nPlanning, solving problems whose solution requires choosing a series of ac-\\ntions and/or the consideration of future actions by other agents, is one of the\\ncore problems of AI  Making plans generally requires search, and in general\\nsearch is hard (assuming P̸=NP)  A familiar example is a chess program,\\nwhich searches through a game tree to judge the longer term value of a candi-\\ndate move by hypothesizing possible future moves  While much of the success\\nof AlphaGo and AlphaZero is attributed to reinforcement learning by self-play,\\nthey also search through game trees; indeed the Monte Carlo tree search algo-\\nrithm on which they built  was considered a key enabling breakthrough \\nBy contrast, LLMs have no component dedicated to search  While it does\\nnot seem impossible that search trees or other structures could be learned inter-\\nnally (like world models), it seems intuitively clear that an autoregressive model\\nwhich predicts one word at a time and cannot go back to revise its predictions\\nin light of what comes later will be seriously handicapped in planning  This\\nobservation is motivating a fair amount of current work on ways to incorporate\\nsearch  LeCun has suggested adding a dynamic programming component to\\nsearch through multiword predictions, as part of his “path towards autonomous\\nmachine intelligence”   Another proposal, the “tree of thoughts” model ,\\nworks with a search tree of LLM responses  A system which uses hierarchical\\nplanning for mathematical theorem proving was developed in  \\nThe next capability on my list, making and working with confidence judg-\\nments, has to do with the well known “hallucination” problem, that LLMs often\\nsimply invent statements, including untrue facts and imaginary citations  While\\nadvantageous for a poetry generator, and bearable for a system which makes\\nsuggestions which an expert human user will verify, this is a huge obstacle to\\nmany practical applications  Thus it is the subject of a great deal of research\\n– a few of this month’s papers are   Perhaps by the time you read\\nthese words there will have already been major progress \\nWhy are LLMs producing these hallucinations? One intuition is that they\\nare doing some sort of compression, analogous to JPEG image compression,\\nwhich introduces errors   This point of view suggests that the problem will\\neventually be solved with larger models and perhaps better training protocols\\nwhich focus on the more informative data items  \\nA related intuition is that the problems follow from inability to properly\\ngeneralize  This comes back to the point about “world models” – a correct\\nmodel, for example an internal encoding of place information, by definition\\ncorrectly treats the properties being modeled  Now suppose we grant that the\\nLLM is solving some class of problems, not by constructing such a model, but by\\nrule-based reasoning  In other words, the LLM somehow learns rules from the\\ncorpus which it uses to make particular inferences which agree with the model \\nWhile such rules can cover any number of cases, there is no clear reason for such\\na rule set to ever cover all cases \\nAnother intuition is that the training data contains errors and this is reflected\\nin the results  Certainly the internet is not known for being a completely reliable\\nsource of truth  This intuition also fits with the observation that adding code\\n29(computer programs) to the training set improves natural language reasoning \\nCode is a good source of rules because almost all of it has been debugged, leading\\nto rules which are correct in their original context (of course they might not be\\ncorrectly applied)  It is a longstanding question whether internal representations\\n(both in AI and in humans) are shared between different natural languages; it\\nwould be truly fascinating to know how much they are also shared with code \\nIf this intuition is right, then LLMs reasoning capability might be improved by\\ntraining on far more code and other content which is guaranteed to be correct \\nSuch content could be generated synthetically as tautologies, or even better as\\nformal verified mathematics (as proposed in ) \\nHere is a different point of view: the problem is not that the systems make\\nthings up, after all creativity has value  Rather, it is that they do not provide\\nmuch indication about the confidence to place in a particular output, and do\\nnot have ways to adapt their reasoning to statements known at different levels of\\nconfidence  Much of our reasoning involves uncertain claims and claims which\\nturn out to be false, the point is to distinguish these from justified claims and\\nkeep track of our confidence in each belief  While it is possible to extract\\nconfidence scores from LLMs , there is also a philosophical point to make\\nhere: not all facts have the same epistemological status  Some facts are grounded\\nin evidence; others are true by definition \\nLLMs are of course statistical models  Even for a completely deterministic\\ntask, say doing arithmetic, a statistical approach to learning is very powerful \\nThis is because learning based on inputs which consist of finitely many training\\nexamples, given in a random order, is naturally formulated in statistical terms \\nBut without making additional non-statistical assumptions, one can never go\\nfrom almost 100% confidence to 100% confidence \\nThis difference is crucial in many aspects of human thought  Of course,\\nlogical reasoning and mathematics stand out as prime examples  Long chains\\nof reasoning are only possible if the individual links are reliable  But it is also\\ncrucial in social reasoning  There is an essential difference between statistical\\nand evidence-based statements, say “Michael is a popular name,” and tauto-\\nlogical, definitional and descriptive statements such as “My name is Michael ”\\nWhile the first statement might be a subject of discussion, a model which can\\nget confused about the second statement is clearly missing a defining aspect of\\nhuman thought, and will lose the confidence of its interlocutor  Perhaps episte-\\nmological status and tautological correctness need to be somehow represented\\nin the model  It need not be designed in, but the model needs to be given\\nadditional signals beyond next word prediction to learn it \\nThe third point on my list, reflection, does not seem to be much discussed,\\nbut to me seems just as important  In computer science, reflection is the ca-\\npability of a system to work with its programs as a form of data  \\nThis is naturally possible for a computer programmed in assembly language, in\\nwhich instructions are encoded in integers  To some extent it is also possible\\nin Lisp, in which programs are encoded in a universal list data structure  As\\ntype systems and other programming language refinements are introduced, re-\\nflection becomes more difficult to provide, but it is necessary for systems-level\\n30programming and makes various standard tasks easier to implement \\nSince an LLM operates on language, reflection for an LLM is the ability to\\nwork with its internal model in linguistic terms  This is related to ML inter-\\npretability, the ability to translate a model into understandable terms  In §7 we\\ndiscussed interpretability of LLMs in terms of circuits and computational mod-\\nels, implicitly leaving these for a human to interpret and understand  One can\\nimagine an “interpretation engine” which given a model, automatically produces\\na more interpretable description, in terms of circuits, rules, or even a description\\nof the model’s functioning in natural language  Given such an interpretation\\nengine, by applying it to an LLM and sending its output as an input to the\\nLLM, we can implement a form of reflection \\nA basic human capability which corresponds to this process is the translation\\nfrom procedural or other implicit forms of memory to linguistic, explicit mem-\\nory  Very often, we learn by doing – riding a bicycle, solving math problems,\\ninteracting socially  We then reflect on what we have learned – in some uncon-\\nscious way – and occasionally come up with verbal observations, summaries, in\\na word reflections  It is fascinating that combining the ideas we discussed brings\\nus into contact with such topics \\nTo conclude, and for what it is worth, out of the forty years I have followed\\nAI, this is by far the most exciting period  I agree with those who think LLMs\\nare a major milestone and believe the ideas behind them – including the trans-\\nformer architecture – will remain important even in the light of future progress \\nThe questions they raise are interesting and important enough that – even as\\nthe specialists make remarkable progress – we need not leave the field to them,\\nbut as scientists and thinkers we should engage and try to contribute \\nA Grammars and parsing\\nMost readers will have encountered the idea of “sentence diagram,” which graph-\\nically represents the decomposition of a sentence into clauses with a subject,\\nverb and object, the assignment of adjectives and prepositional phrases to the\\nnouns and verbs they modify, and so on  Formal versions of this concept are\\nfoundational in linguistics and computer science, and a short introduction (or\\nreview) is a good way to bring the general ideas we are discussing to life \\nA formal grammar can be given by a set of “production rules” which can\\nbe used to generate grammatical strings  A simple example is in Figure 3 \\nEach line is a rule, which is made up of two strings of symbols separated by\\n→, the left hand side or lhs and right hand side or rhs  These symbols can be\\n“terminals” which appear in the language (such as +, ∗,x, 0 and so on in our\\nexample) or “nonterminals” which do not (such as TERM) \\nThese rules are used as follows: We start with a string Scontaining a dis-\\ntinguished “start” symbol (here EXPR)  We then iterate the following process:\\nchoose a rule whose lhs occurs in S, and apply it by substituting one occurrence\\nof this lhs in Swith the rhs  Every string Swhich can be obtained by a finite\\nsequence of these operations is considered grammatical, and by keeping track\\n31EXPR →TERM + EXPR (17)\\nEXPR →( EXPR )\\nEXPR →TERM\\nTERM →VALUE ∗TERM\\nTERM →( EXPR )\\nTERM →VALUE\\nVALUE →x\\nVALUE →y\\nVALUE →1\\nFigure 3: A grammar for arithmetic expressions \\nof the rule applications we get a parse tree  This is a graph whose nodes are\\nsymbols and whose edges connect a symbol which appears on the lhs of a rule\\napplication with the nodes for each of the symbols which appear on the rhs 42\\nA good exercise is to work out the parse tree for the expression y+ 1∗xand\\ncheck that multiplication takes precedence over addition \\nOur example of a grammar is a context-free grammar, meaning that the\\nleft hand side of each rule consists of a single symbol  If we do not put this\\nrestriction, the resulting class of languages are universal computers (and thus\\nsuffer from potential undecidability)  There is also a more restricted class of\\ngrammars called regular grammars (this hierarchy was found by Chomsky), but\\nthese cannot describe nested structures such as the parentheses of Eq  17  The\\ncontext-free grammars are the right degree of complexity for many purposes  In\\nparticular, programming languages and the formal languages of mathematical\\nlogic can be described using CFG’s and thus the algorithms for working with\\nthem and associated theory are well developed \\nBesides recognizing and parsing languages, one can describe other linguistic\\ntasks in similar terms  A trivial example would be word replacement, with\\nrules such as OLD i→NEW i  Realistic tasks benefit from frameworks with\\nmore structure  For example, to use the grammar in Eq  17 to do arithmetic,\\nwe would be much better off with a framework in which the token VALUE\\ncarries an associated numerical or symbolic value  This can be done with the\\nframework of attribute grammars  When we suggest in §8 that LLMs perform\\nnatural language tasks using systems of large numbers of rules, we have this\\nsort of extended grammatical framework in mind \\nCFG’s are not really adequate for natural languages, with their inherent\\nambiguity and their many special cases and exceptions  A more general for-\\nmalism is the probabilistic CFG  This is obtained by associating a probability\\ndistribution to each symbol which appears on the left hand side of a rule (the\\nnonterminals)  For example, we might stipulate that a VALUE has a 75% chance\\n42One can see examples for English sentences in the Wikipedia article “Parse tree ”\\n32to be a number and a 25% chance to be a variable  With this information, a\\nPCFG defines a probability distribution on strings, which gives zero probability\\nto nongrammatical strings \\nA symbolic approach to parsing would propose two primary algorithms  One\\nis a parser, which given a grammar and an input produces the parse tree  An-\\nother would be an algorithm for learning a grammar from a corpus  Since any\\nfinite corpus can be described by many grammars, PCFG’s are better suited\\nthan CFG’s to this problem  In any case the learning and parsing algorithms\\nare not necessarily related \\nIn the connectionist approach followed by LLMs, these two algorithms are\\nsubsumed into the definition of a model which can parse any PCFG whose rules\\nare encoded in its weights  By training this on a corpus, the model learns a\\nparticular PCFG which generates the corpus  Interpretability as discussed in\\n§7 then means reversing this relation, by extracting a parser and PCFG from\\nthe trained model \\nReferences\\n Reproduced under the cc by 4 0 license \\n Ekin Aky¨ urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny\\nZhou  What learning algorithm is in-context learning? Investigations\\nwith linear models, November 2022  URL: http:\\n//arxiv \\n Marie Amalric and Stanislas Dehaene  A distinct cortical network for\\nmathematical knowledge in the human brain  NeuroImage , 189:19–31,\\nApril 2019  URL: https://www 2019 \\n Sanjeev Arora and Boaz Barak  Computational complexity: a modern\\napproach   Cambridge University Press, 2009 \\n Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Ris-\\nteski  A Latent Variable Model Approach to PMI-based Word Embed-\\ndings 03520  , June 2019  URL:\\nhttp://arxiv \\nAyers, Dragomir Radev, and Jeremy Avigad  ProofNet: Autoformalizing\\nand Formally Proving Undergraduate-Level Mathematics, February 2023  URL: http://arxiv 12433 ,doi:\\n10 \\n Sebastian Bader and Pascal Hitzler  Dimensions of Neural-symbolic In-\\ntegration - A Structured Survey, November 2005  arXiv:cs/0511042 \\n33URL: http://arxiv cs/\\n0511042  \\n Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh\\nSharma  February 2021  URL: https:\\n//arxiv  Edelman, Surbhi Goel, Sham Kakade, Eran\\nMalach, and Cyril Zhang  Hidden Progress in Deep Learning: SGD Learns\\nParities Near the Computational Limit, July 2022  URL: http://arxiv  Long, G´ abor Lugosi, and Alexander Tsigler \\nBenign overfitting in linear regression  Proceedings of the National\\nAcademy of Sciences , 117(48):30063–30070, 2020  Bartlett, Andrea Montanari, and Alexander Rakhlin  Deep learn-\\ning: a statistical viewpoint 09177  , March\\n2021  URL: http://arxiv  Probing classifiers: Promises, shortcomings, and\\nadvances  Computational Linguistics , 48:207–219, 2021  URL: http:\\n//arxiv  Fit without fear: remarkable mathematical phenomena of\\ndeep learning through the prism of interpolation 14368  , May 2021  URL: http://arxiv \\n Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Man-\\ndal  Reconciling modern machine-learning practice and the classical\\nbias–variance trade-off  Proceedings of the National Academy of Sci-\\nences , 116(32):15849–15854, 2019  Publisher: National Academy of\\nSciences eprint: https://www full \\nURL: https://www \\n Mikhail Belkin, Siyuan Ma, and Soumik Mandal  To understand deep\\nlearning we need to understand kernel learning  February 2018  URL:\\nhttps://arxiv  Bender and Alexander Koller  Climbing towards NLU: On mean-\\ning, form, and understanding in the age of data  In Proceedings of the\\n58th Annual Meeting of the Association for Computational Linguistics ,\\npages 5185–5198, Online, July 2020  Association for Computational Lin-\\nguistics 463 ,doi:\\n10 \\n34 Yoshua Bengio  From system 1 deep learning to system 2 deep\\nlearning, December 2019 \\n Yoshua Bengio, R´ ejean Ducharme, and Pascal Vincent  A neural proba-\\nbilistic language model  Advances in neural information processing sys-\\ntems, 13, 2000 \\n Christopher M Bishop and Nasser M Nasrabadi  Pattern recognition and\\nmachine learning , volume 4  Springer, 2006 \\n Tai-Danae Bradley, John Terilla, and Yiannis Vlassopoulos  An enriched\\ncategory theory of language: from syntax to semantics 07890\\n , June 2021  URL: http://arxiv \\n Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Robert L\\nMercer, et al  The mathematics of statistical machine translation: Pa-\\nrameter estimation  1993  Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei  Language Models are Few-Shot Learners 14165\\n, June 2020 14165  URL: http://arxiv \\n14165   Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez,\\nSpyridon Samothrakis, and Simon Colton  A survey of monte carlo tree\\nsearch methods  IEEE Transactions on Computational Intelligence and\\nAI in games , 4(1):1–43, 2012 \\n S´ ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes\\nGehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi\\nLi, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\\nand Yi Zhang  Sparks of Artificial General Intelligence: Early experi-\\nments with GPT-4, March 2023  URL: https://arxiv \\n12712v1  \\n Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt  Discovering\\nLatent Knowledge in Language Models Without Supervision, December\\n2022  URL: http://arxiv \\n35 Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin\\nKnight  Recurrent Neural Networks as Weighted Language Recognizers,\\nMarch 2018 05408   URL: http://arxiv \\n05408 ,doi:10 05408   Chi, John Hewitt, and Christopher D  Finding Uni-\\nversal Grammatical Relations in Multilingual BERT 04511\\n, May 2020 04511  URL: http://arxiv \\n04511  \\n David Chiang, Peter Cholak, and Anand Pillay  Tighter Bounds on\\nthe Expressivity of Transformer Encoders, May 2023  URL: http://arxiv  Chatgpt is a blurry jpeg of the web  The New Yorker ,\\nFebruary 2023 \\n Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever  Generating\\nLong Sequences with Sparse Transformers  April 2019  URL: https:\\n//arxiv \\n Anna Choromanska, Mikael Henaff, Michael Mathieu, G´ erard Ben Arous,\\nand Yann LeCun  The Loss Surfaces of Multilayer Networks, January\\n2015  URL: http://arxiv \\n Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al  PaLM: Scal-\\ning Language Modeling with Pathways 02311  , April 2022  URL: http://arxiv \\n Bilal Chughtai, Lawrence Chan, and Neel Nanda  A Toy Model of Uni-\\nversality: Reverse Engineering How Networks Learn Group Operations,\\nMay 2023  URL: http://arxiv \\n Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark  Mathematical\\nFoundations for a Compositional Distributional Model of Meaning, March\\n2010 4394   URL: http://arxiv \\n4394 ,doi:10 4394  \\n Taco Cohen and Max Welling  Group equivariant convolutional net-\\nworks  In International conference on machine learning , pages 2990–2999 \\nPMLR, 2016 \\n R´ emi Coulom  Efficient selectivity and backup operators in monte-carlo\\ntree search  In International conference on computers and games , pages\\n72–83  Springer, 2006 \\n36 Francis Crick  The recent excitement about neural networks  Nature ,\\n337:129–132, 1989  Approximation by superpositions of a sigmoidal\\nfunction  Mathematics of Control, Signals and Systems , 2:303–314, 1989 \\n Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova \\nBERT: Pre-training of Deep Bidirectional Transformers for Language Un-\\nderstanding  October 2018  URL: https://arxiv \\n Ronald DeVore, Boris Hanin, and Guergana Petrova 14501  , December 2020  arXiv:\\n2012  URL: http://arxiv  Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang \\nInductive Biases and Variable Creation in Self-Attention Mechanisms 10090  , October 2021  URL:\\nhttp://arxiv \\n Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom\\nHenighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn\\nDrain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario\\nAmodei, Martin Wattenberg, and Christopher Olah  Toy Models of\\nSuperposition, September 2022  URL: http:\\n//arxiv  Foulds, and Shimei Pan  Trapping LLM Hal-\\nlucinations Using Tagged Context Prompts, June 2023  URL: http://arxiv  Studies in linguistic analysis   Wiley-Blackwell, 1957  The modularity of mind   MIT press, 1983 \\n Dan Friedman, Alexander Wettig, and Danqi Chen  Learning Transformer\\nPrograms, June 2023  URL: http://arxiv \\n Artur d’Avila Garcez and Luis C  Neurosymbolic AI: The 3rd\\nWave, December 2020  URL: http://arxiv \\n Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant  What\\nCan Transformers Learn In-Context? A Case Study of Simple Function\\nClasses, January 2023  URL: http://arxiv \\n37 Ian Goodfellow, Yoshua Bengio, and Aaron Courville  MIT\\npress, 2016 \\n Anirudh Goyal and Yoshua Bengio  Inductive Biases for Deep Learn-\\ning of Higher-Level Cognition, August 2022  URL: http://arxiv \\n2011  Grokking modular arithmetic, January 2023 02679   URL: http://arxiv \\n02679 ,doi:10 02679  \\n Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini,\\nFosca Giannotti, and Dino Pedreschi  A Survey of Methods for Explain-\\ning Black Box Models  ACM Computing Surveys , 51(5):1–42, September\\n2019 acm 1145/\\n3236009   Machine Psychology: Investigating Emergent Capabil-\\nities and Behavior in Large Language Models Using Psychological Meth-\\nods, April 2023  URL: http://arxiv \\n Michael Hahn and Navin Goyal  A Theory of Emergent In-Context\\nLearning as Implicit Structure Induction, March 2023  URL: http://arxiv \\n Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCan-\\ndlish  Scaling Laws for Transfer, February 2021  URL: http://arxiv \\n John Hewitt and Christopher D Manning  A Structural Probe for Finding\\nSyntax in Word Representations  page 10, 2019 \\n Sepp Hochreiter and J¨ urgen Schmidhuber  Long short-term memory  Neu-\\nral computation , 9(8):1735–1780, 1997 \\n Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne\\nHendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W  Rae, Oriol\\nVinyals, and Laurent Sifre  Training Compute-Optimal Large Language\\nModels, March 2022  URL: http://arxiv \\n38 Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik\\nStrobelt, Duen Horng Chau, Mohammed J  Zaki, and Dmitry Krotov \\nEnergy Transformer, February 2023  URL: http://arxiv  Behind Deep Blue: Building the computer that defeated\\nthe world chess champion   Princeton University Press, 2002 \\n Myeongjun Jang and Thomas Lukasiewicz  Consistency Analysis of Chat-\\nGPT, March 2023  URL: http://arxiv  Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu,\\nMateja Jamnik, Timoth´ ee Lacroix, Yuhuai Wu, and Guillaume Lample \\nDraft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal\\nProofs, November 2022  URL: http://arxiv  High Dimensional Statistical Inference and Ran-\\ndom Matrices, November 2006  URL: https://arxiv org/abs/math/\\n0611589v1  \\n Dan Jurafsky and James H Martin  Speech and language processing, 2009 \\n Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn\\nDrain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova Das-\\nSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones,\\nNelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman,\\nStanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson\\nKernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson,\\nSam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben\\nMann, Sam McCandlish, Chris Olah, and Jared Kaplan  Language Mod-\\nels (Mostly) Know What They Know, July 2022 \\nURL: http://arxiv  Fast and slow thinking  Allen Lane and Penguin\\nBooks, New York , 2011  Brown, Ben-\\njamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and\\nDario Amodei  Scaling Laws for Neural Language Models, January 2020  URL: http://arxiv \\n Michael J Kearns and Umesh Vazirani  An introduction to computational\\nlearning theory   MIT press, 1994 \\n Daphne Koller and Nir Friedman  Probabilistic graphical models: princi-\\nples and techniques   MIT press, 2009 \\n39 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton  Imagenet classi-\\nfication with deep convolutional neural networks  Communications of the\\nACM , 60(6):84–90, 2017 \\n Florent Krzakala, Federico Ricci-Tersenghi, Lenka Zdeborova, Eric W\\nTramel, Riccardo Zecchina, and Leticia F Cugliandolo  Statistical Physics,\\nOptimization, Inference, and Message-Passing Algorithms: Lecture Notes\\nof the Les Houches School of Physics: Special Issue, October 2013   Num-\\nber 2013  Oxford University Press, 2016  Salakhutdinov, and J  Human-level\\nconcept learning through probabilistic program induction  Science ,\\n350(6266):1332–1338, December 2015  URL: https://www aab3050 ,doi:10 \\naab3050  \\n Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J\\nGershman  Building Machines That Learn and Think Like People  April\\n2016  URL: http://arxiv  Popular talks and private discussion, 2015  A path towards autonomous machine intelligence, 2022 \\n Yann LeCun, Yoshua Bengio, and Geoffrey Hinton  Nature ,\\n521:436–444, 2015 \\n Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk\\nMichalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and\\nVedant Misra  Solving Quantitative Reasoning Problems with Lan-\\nguage Models, June 2022  URL: http://arxiv  Hopkins, David Bau, Fernanda Vi´ egas, Hanspeter\\nPfister, and Martin Wattenberg  Emergent World Representations: Ex-\\nploring a Sequence Model Trained on a Synthetic Task, February 2023  URL: http://arxiv 13382 ,doi:\\n10 \\n Kenneth Li, Oam Patel, Fernanda Vi´ egas, Hanspeter Pfister, and Mar-\\ntin Wattenberg  Inference-Time Intervention: Eliciting Truthful Answers\\nfrom a Language Model, June 2023  URL: http:\\n//arxiv  Manning, Christopher\\nR´ e, Diana Acosta-Navas, Drew A  Hudson, Eric Zelikman, Esin Durmus,\\nFaisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav\\nSanthanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,\\nNathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Hender-\\nson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\\nGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav\\nChaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and\\nYuta Koreeda  Holistic Evaluation of Language Models, November 2022  URL: http://arxiv 09110 ,doi:\\n10 \\n Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen\\nBaker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and\\nKarl Cobbe  Let’s Verify Step by Step, May 2023  URL: http://arxiv  Ash, Surbhi Goel, Akshay Krishnamurthy, and\\nCyril Zhang  Transformers Learn Shortcuts to Automata, October 2022  URL: http://arxiv  Information theory, inference and learning algorithms  \\nCambridge university press, 2003  Tenenbaum, and Evelina Fedorenko  Dissociating language\\nand thought in large language models: a cognitive perspective, January\\n2023  URL: http://arxiv  Roberts, and James Sully  A Solvable\\nModel of Neural Scaling Laws, October 2022  URL: http://arxiv \\n Yuri Manin and Matilde Marcolli org , May 2016  URL: http://arxiv \\n Christopher Manning and Hinrich Schutze  Foundations of statistical nat-\\nural language processing   MIT press, 1999 \\n Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal,\\nand Omer Levy  Emergent linguistic structure in artificial neural net-\\nworks trained by self-supervision  Proceedings of the National Academy of\\nSciences , 117(48):30046–30054, 2020 \\n41 Matilde Marcolli, Noam Chomsky, and Robert Berwick  Mathe-\\nmatical Structure of Syntactic Merge, May 2023  URL: http://arxiv \\n Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz  Build-\\ning a large annotated corpus of english: The penn treebank  1993  Vision: A computational investigation into the human rep-\\nresentation and processing of visual information   MIT press, 2010 \\n Jir ˇi Matouˇ sek  Lecture notes on metric embeddings  2013 \\n Pamela McCorduck and Cli Cfe  Machines who think: A personal inquiry\\ninto the history and prospects of artificial intelligence   CRC Press, 2004  On the Linguistic Capacity of Real-Time Counter Au-\\ntomata 06866  , April 2020  arXiv: 2004  URL:\\nhttp://arxiv \\n William Merrill and Ashish Sabharwal  The Parallelism Tradeoff: Lim-\\nitations of Log-Precision Transformers, April 2023  URL: http://arxiv \\n William Merrill, Ashish Sabharwal, and Noah A  Saturated Trans-\\nformers are Constant-Depth Threshold Circuits 16213  ,\\nApril 2022 16213  URL: http://arxiv \\n16213  \\n Marc Mezard and Andrea Montanari  Information, physics, and compu-\\ntation   Oxford University Press, 2009  Michaud, Ziming Liu, Uzay Girit, and Max Tegmark  The Quan-\\ntization Model of Neural Scaling, March 2023  URL: http://arxiv \\n Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean  Efficient\\nEstimation of Word Representations in Vector Space, September 2013  URL: http://arxiv  Society of mind   Simon and Schuster, 1988  Pattern theory: the mathematics of perception  arXiv\\npreprint math/0212400 , 2002 \\n David Mumford and Agn` es Desolneux  Pattern theory: the stochastic\\nanalysis of real-world signals   CRC Press, 2010 \\n42 Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Stein-\\nhardt  Progress measures for grokking via mechanistic interpretability,\\nJanuary 2023  URL: http://arxiv \\n Allen Newell, John Clifford Shaw, and Herbert A Simon  Empirical explo-\\nrations of the logic theory machine: a case study in heuristic  In Papers\\npresented at the February 26-28, 1957, western joint computer conference:\\nTechniques for reliability , pages 218–230, 1957  The quest for artificial intelligence   Cambridge University\\nPress, 2009  Mechanistic interpretability, variables, and the importance of\\ninterpretable bases, 2022 \\n Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das-\\nSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna\\nChen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\\nDanny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\\nLovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared\\nKaplan, Sam McCandlish, and Chris Olah  In-context Learning and\\nInduction Heads, September 2022  URL: http:\\n//arxiv \\n Jeffrey Pennington, Richard Socher, and Christopher Manning  GloVe:\\nGlobal Vectors for Word Representation  In Proceedings of the 2014 Con-\\nference on Empirical Methods in Natural Language Processing (EMNLP) ,\\npages 1532–1543, Doha, Qatar, October 2014  Association for Com-\\nputational Linguistics  URL: https://www \\n Mary Phuong and Marcus Hutter  Formal Algorithms for Transformers,\\nJuly 2022 09238   URL: http://arxiv \\n09238  \\n Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant\\nMisra  Grokking: Generalization Beyond Overfitting on Small Algorith-\\nmic Datasets 02177  , January 2022 \\nURL: http://arxiv  Smith,\\nand Mike Lewis  Measuring and Narrowing the Compositionality Gap\\nin Language Models, October 2022  URL: http:\\n//arxiv \\n Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever \\nImproving language understanding by generative pre-training  2018  Pub-\\nlisher: OpenAI \\n43 Alec Radford, Jeff Wu, Rewon Child, D  Luan, Dario Amodei, and\\nIlya Sutskever  Language Models are Unsupervised Multitask Learners \\nundefined , 2019  URL: https://www \\n Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J  Explor-\\ning the Limits of Transfer Learning with a Unified Text-to-Text Trans-\\nformer 10683  , July 2020  URL:\\nhttp://arxiv \\n Hubert Ramsauer, Bernhard Sch¨ afl, Johannes Lehner, Philipp Seidl,\\nMichael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner,\\nMilena Pavlovi´ c, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael\\nKopp, G¨ unter Klambauer, Johannes Brandstetter, and Sepp Hochreiter \\nHopfield Networks is All You Need, April 2021  URL: http://arxiv  Roberts, Sho Yaida, and Boris Hanin  The Principles of Deep\\nLearning Theory 10165  , August 2021  URL: http://arxiv  The perceptron: a probabilistic model for information\\nstorage and organization in the brain  Psychological review , 65(6):386,\\n1958  Hinton, and James L  A\\ngeneral framework for parallel distributed processing  1986  Artificial intelligence a modern approach  , 2010 \\n Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo  Are emergent\\nabilities of large language models a mirage? ArXiv , abs/2304 15004, 2023  The deep learning revolution   MIT press, 2018  programming a computer for playing chess  The\\nLondon, Edinburgh, and Dublin Philosophical Magazine and Journal of\\nScience , 41(314):256–275, 1950 \\n Hava T Siegelmann and Eduardo D Sontag  On the computational power\\nof neural nets  In Proceedings of the fifth annual workshop on Computa-\\ntional learning theory , pages 440–449, 1992  Procedural reflection in programming languages\\nvolume i  1982 \\n44 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya\\nGanguli  Deep unsupervised learning using nonequilibrium thermodynam-\\nics  In International Conference on Machine Learning , pages 2256–2265 \\nPMLR, 2015 \\n Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and\\nAri S  Beyond neural scaling laws: beating power law scaling via\\ndata pruning, June 2022  URL: http://arxiv \\n Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al  Beyond the\\nImitation Game: Quantifying and extrapolating the capabilities of lan-\\nguage models 04615, arXiv, June 2022 04615  type: article  URL: http://arxiv  The bitter lesson, 2019  URL: http://www  A promising path towards autoformalization and gen-\\neral artificial intelligence  In International Conference on Intelligent Com-\\nputer Mathematics , 2020 \\n Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gim-\\npel  Chess as a Testbed for Language Model State Tracking, May\\n2022  URL: http://arxiv  An Introduction to Transformers, July 2023  URL: http://arxiv 10557 ,doi:\\n10 \\n Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin  Atten-\\ntion Is All You Need  June 2017  URL: https:\\n//arxiv \\n Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas-\\ntian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald\\nMetzler, Ed H  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang,\\nJeff Dean, and William Fedus  Emergent Abilities of Large Language\\nModels  2022  Publisher: arXiv Version Number: 2  URL: https:\\n//arxiv \\n Gail Weiss, Yoav Goldberg, and Eran Yahav  On the Practical Compu-\\ntational Power of Finite Precision RNNs for Language Recognition, May\\n2018 04908   URL: http://arxiv \\n04908 ,doi:10 04908  \\n45 Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin\\nChoi, and Kyunghyun Cho  NaturalProofs: Mathematical Theorem Prov-\\ning in Natural Language  page 14, 2021 \\n Noam Wies, Yoav Levine, and Amnon Shashua  The Learnability of In-\\nContext Learning, March 2023  URL: http://\\narxiv  Mathematics and computation: A theory revolutionizing\\ntechnology and science   Princeton University Press, 2019 wikipedia \\n Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma  An\\nExplanation of In-context Learning as Implicit Bayesian Inference, July\\n2022  URL: http://arxiv  Hu, Igor Babuschkin, Szymon Sidor, Xiaodong\\nLiu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jian-\\nfeng Gao  Tensor Programs V: Tuning Large Neural Networks via Zero-\\nShot Hyperparameter Transfer, March 2022  URL: http://arxiv  Griffiths,\\nYuan Cao, and Karthik Narasimhan  Tree of Thoughts: Deliberate Prob-\\nlem Solving with Large Language Models, May 2023  URL: http://arxiv  HaoChen,\\nJames Zou, Percy Liang, and Serena Yeung  Beyond Positive Scaling:\\nHow Negation Impacts Scaling Trends of Language Models, May 2023  URL: http://arxiv 17311 ,doi:\\n10 \\n Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora  Do\\nTransformers Parse while Predicting the Masked Word?, March 2023  URL: http://arxiv 08117 ,doi:\\n10 \\n Kunhao Zheng, Jesse Michael Han, and Stanislas Polu  MiniF2F: a cross-\\nsystem benchmark for formal Olympiad-level mathematics, February\\n2022  URL: http://arxiv \\n46\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\soulo\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLarge Language Models\\nMichael R. Douglas\\nCMSA, Harvard University\\nDept. of Physics, Stony Brook University\\nmdouglas@cmsa.fas.harvard.edu\\nJuly 2023\\nAbstract\\nArtificial intelligence is making spectacular progress, and one of the\\nbest examples is the development of large language models (LLMs) such\\nas OpenAI’s GPT series. In these lectures, written for readers with a\\nbackground in mathematics or physics, we give a brief history and survey\\nof the state of the art, and describe the underlying transformer architec-\\nture in detail. We then explore some current ideas on how LLMs work\\nand how models trained to predict the next word in a text are able to\\nperform other tasks displaying intelligence.\\n 1arXiv:2307.05782v1    11 Jul 20231 Introduction\\nAt the end of November 2022, OpenAI released a system called ChatGPT which\\ninteracts with its users in natural language. It can answer questions, engage in\\ndialogs, translate between languages and write computer code with a fluency\\nand ability far exceeding all previous publically available systems. Although it\\nfalls well short of human abilities in many ways, still the large language model\\ntechnology of which it is an example is widely considered to be a major advance\\nin artificial intelligence.1\\nFew developments in science and technology entered the popular conscious-\\nness as quickly as ChatGPT. There is no mystery about why. The ability to use\\nlanguage is a defining property of humanity, and for the first time a computer\\nis doing this well enough to make a comparison with humans interesting. All\\nof the hopes and fears which have developed around AI, robots and technology\\nmore generally are being brought into the discussion. In my opinion this is\\njustified; the speed of recent progress makes it urgent to better understand AI,\\nto forecast its capabilities and limitations, and to make wise decisions about\\nits development and use. With great opportunities will come great challenges,\\nwhich will concern all of us.\\n In these lecture notes we give an introduction to this subject for mathe-\\nmaticians, physicists, and other scientists and readers who are mathematically\\nknowledgeable but not necessarily expert in machine learning or artificial intel-\\nligence. We begin with a very brief overview of AI in §2 to explain some ideas\\nwe consider to be essential context, the basic principles of the symbolic and\\nconnectionist approaches. In §3 we define statistical language models and relate\\nthe history of transformer-based LLMs up through GPT-4. In §4 we discuss\\nmeasures of what LLMs do and how well they do it. We then give a precise\\nexplanation of simpler language models in §5 and the transformer architecture\\nin§6.\\n It is amazing that a model defined by a few short equations, trained to go\\nthrough a text and simply predict each next word as it appears – a task which\\nseems only loosely related to any definition of intelligence – can do tasks which\\n“obviously” require intelligence, such as solving word problems like the one in\\nFigure 1 below. At present nobody really understands how this works. Even\\nthe interpretation of what LLMs are doing is controversial, ranging from the\\nbelief that they are “simply” rearranging the sentences they were trained on,\\nall the way to the belief that the LLMs are learning sophisticated models of\\nthe world and that “simply” scaling up the computations will produce artificial\\ngeneral intelligence. Any forecast for progress must take into account the current\\nmodels’ shortcomings – lack of long term memory and ability to plan, tendency\\nto make up facts and “hallucinate,” unreliability in logical reasoning, etc.. Do\\nthese problems have technical solutions which will also look simple once we have\\nthem? Or are they more significant barriers?\\nMost current work on LLMs takes an engineering and problem solving per-\\n1A few of the many other milestones in LLM development are .\\n2Question: Assume that the variance of the first nnatural numbers is 10, and\\nthe variance of the first meven natural numbers is 16. Compute m+n.\\n Model output: Letσ2be the variance of the first meven natural numbers,\\nandτ2be the variance of the first nnatural numbers. Then, we have τ2=n2−1\\n12\\nandσ2=m2−1\\n3. We solve the system of equations:\\nn2−1\\n12= 10\\nm2−1\\n3= 16.\\n This translates into the system\\nn2= 121\\nm2= 48 + 1 = 49 .\\n Therefore, n= 11 andm= 7 , son+m= 18 .\\nFigure 1: A question-answer pair solved by Minerva. From Lewkowycz et al\\n, “Solving quantitative reasoning problems with language models,” 2022.\\nspective, but there are many interesting works which focus more on understand-\\ning how LLMs work. One would think this should be far easier than understand-\\ning how human brains work, as we have full knowledge of an LLM’s microscopic\\nworkings and can do a wide variety of experiments on it.2These efforts are in\\ntheir early days, but in §7 we survey current approaches to understanding how\\nLLMs do what they do. We conclude in §8 with more general discussion, some\\nquestions and potentially important developments to watch for.\\n Before we start, let me say a little about my own background. I was trained\\nas a theoretical physicist and most of my contributions to science are in string\\ntheory and its interface with mathematics, but I have followed AI fairly closely\\nsince the 80’s and in detail since 2016. In addition I spent eight years in quan-\\ntitative finance where I gained a good deal of “hands-on” experience with ma-\\nchine learning. I have given many lectures telling computer scientists about\\nphysics and physicists about computational topics, and benefited from conver-\\nsations with many people – more than I can name here, but let me thank\\nGerald Jay Sussman, David McAllester, Yann LeCun, Sanjeev Arora, Surya\\nGanguli, Jeremy Avigad, Vijay Balasubramanian, Dmitri Chklovskii, David\\nDonoho, Steve Skiena, Christian Szegedy, Misha Tsodyks, Tony Wu and the\\n2At least, this was the case before March 2023. Currently the weights and even the de-\\nsign parameters of GPT-4, the most advanced LLM, are held in confidence by OpenAI as\\nproprietary information.\\n 3many speakers in the CMSA New Technologies seminar series,3and Josef Urban\\nand the AITP community.4Thanks to David McAllester and Sergiy Verstyuk\\nfor comments on the first draft. These notes would not have been possible with-\\nout their input and advice, and I hope their signal to noise ratio approaches\\nthat of what they shared with me.\\n 2 Symbolic and connectionist AI\\n The goal of artificial intelligence is to build computational systems which can\\nperform tasks which require intelligence. Although intelligence is hard to define\\nprecisely, an operational definition suitable for LLMs is ability at tasks requiring\\nlanguage, reasoning, and planning, as judged by humans who interact with the\\nsystem. Some famous and difficult “challenge tasks” include playing chess ,\\nproving mathematical theorems , and answering natural language questions\\nusing generally known facts and common sense.5\\nThese problems have been the subject of intense investigation since the mid-\\n50’s, and a few textbooks and histories are . Essentially from\\nthe start, two broad approaches were set out, which would later be called sym-\\nbolic and connectionist AI.6The symbolic approach originated in mathematical\\nlogic and generative linguistic theory, and tracked the development of computer\\ntechnology (both hardware and software) as a tool for solving practical and\\nscientific problems. Central topics in this approach are formal logic and lan-\\nguage theory, search and heuristics, and engineering techniques for designing\\nand building large and complex systems.\\n Symbolic AI systems are designed, meaning that their creators develop a de-\\ntailed understanding of the task, and encode this understanding into the system\\nby programming, by hardware design and otherwise. As an example, consider\\nthe task of parsing: given an input string of words, determine its grammatical\\nstructure. Most of us learned to diagram sentences in elementary school, and\\nalthough linguists have developed far more sophisticated notions of grammar,\\nthis simple notion gives the right idea. The grammar of a language is encoded\\nin rules, which belong to a formal framework – see the appendix for the example\\nof context-free grammars. Given such a framework, one can design a parsing\\nalgorithm which takes as input a rule set and an input string, and produces an\\noutput which states whether the string is a grammatical sentence and if so makes\\nits structure explicit. This is a symbolic AI approach, not because the words\\n“symbolize” anything (after all grammar does not have to come with mean-\\n3https://live-hu-cmsa-222.pantheonsite.io/\\n4http://aitp-conference.org/2023/\\n5This challenge originates with Turing’s famous test, but the restriction to question answer-\\ning makes it better defined and testable using benchmarks, standardized question-answer sets.\\n Discussion of the original test can be found at https://en.wikipedia.org/wiki/Turing test\\n6Symbolic AI is sometimes called “GOFAI” for “good old-fashioned AI.” Related terms\\ninclude “rule based,” “logic based,” “expert system” and “feature engineering.” The con-\\nnectionist approach has many other names, reflecting its mixed ancestry: “neural,” “deep\\nlearning,” “parallel distributed processing,” “differentiable,” and “representation learning.”\\n4ing), but because the grammatical rules and the parsing algorithm (including\\nits internal data structures) have a clear meaning to their designers.\\n Symbolic methods have had considerable success at many tasks requiring\\nintelligence, famously including chess playing  and symbolic algebra7as well\\nas more prosaic but very central tasks such as translating high level computer\\nlanguages to machine code (compiling). And a great deal of work has been\\ndone to broaden their scope, for example to build question answering systems\\nsuch as the well known IBM Watson. Many valuable techniques came out of\\nthis; ways to systematize rules into “knowledge bases” or “knowledge graphs,”\\nmethods for automated logical reasoning, and so on. But it was long ago realized\\nthat once one goes beyond “formal worlds” such as chess and algebra to the\\ncomplex and messy situations of real life, although one can postulate rules which\\ncapture many truths and can be used for reasoning, rules which are valid in all\\ncases are very rare. Furthermore, the sheer number of rules required to cover\\neven the likely possibilities is very large. These difficulties were addressed by\\nimplementing probabilistic reasoning and by getting teams of humans to develop\\nthe requisite enormous rule sets, leading to the “expert system” approach which\\nwas applied (for example) to medical question answering. Cyc,8an early and\\nwell known expert system, is commercially available and has a database of\\ncommonly known facts with over 25 million assertions; however this is dwarfed\\nby knowledge bases such as Wikidata (over one billion “facts”) but which do\\nnot have a systematic reasoning engine. It is clear that any approach which\\ndepends on careful human analysis of such large knowledge bases is impractical.\\n Meanwhile, a very different “connectionist” approach to AI was being cham-\\npioned by other researchers. They drew their inspiration from hypotheses about\\nhow the brain works, from information theory and statistics, from physics and\\nother natural sciences, and from applied mathematics and particularly opti-\\nmization theory. These diverse points of view came together in the 1990’s and\\nled to a great deal of interdisciplinary work,9of which the part most related to\\nAI and which lies behind LLMs is called machine learning (ML).\\n The usual starting point in modern treatments of ML is to rephrase a task\\nas a statistical inference problem based on a large dataset. A canonical example\\nis image recognition – say, given an array of pixels (light intensity values),\\nestimate the probability that the image depicts a cat. Rather than design a\\nsystem to do this, one starts with a large dataset of images with labels (cat and\\nnon-cat). One then designs a very general statistical model and “trains” it on\\nthis dataset to predict the label given the image. This is supervised learning,\\none can also do “self-supervised” learning in which the system predicts some\\npart of the data given other parts (say, filling in part of an image). A third\\nstandard ML paradigm is reinforcement learning, which applies to tasks which\\ninvolve choosing actions to fulfill a longer term goal. The classic example is\\ngame playing, as in AlphaGo and AlphaZero.\\n In any case, since the problem is formulated statistically, it is possible to\\n7https://www.sigsam.org/\\n8https://cyc.com/\\n9I first learned about this from .\\n 5consider the training dataset one item at a time, and use it to incrementally\\nimprove the model. This is almost always done by formulating the task in terms\\nof an “objective function” which measures the quality with which it is performed,\\nfor example the accuracy with which correct labels are assigned to images. One\\nthen takes a parameterized model and trains it by optimizing this function,\\nevaluated on the training dataset, with respect to the model parameters. For\\nthe classic models of statistics this can be done analytically, as in a least squares\\nfit. For more general models one uses numerical methods, such as gradient\\ndescent. Either way, a central question of statistics and machine learning is\\ngeneralization, meaning the extent to which the model well describes data not\\nin the training set but sampled from the same probability distribution. A well\\nknown principle which speaks to this question is “Occam’s razor,” that simpler\\nmodels will generalize better. This is often simplified to the rule that a model\\nshould have the minimal number of parameters needed to fit the dataset.\\n Not all machine learning systems are “deep learning”  or “connection-\\nist” . These terms generally refer to the use of neural networks with large\\nnumbers of parameters which provide effective universal function approxima-\\ntors. While the idea is very old , before 2012 it was widely believed to be\\nimpractical. One argument for this was the “dull side” of Occam’s razor – mod-\\nels with so many parameters were destined to overfit and would not generalize.\\n Evidently this is not the case, leading to concepts such as “benign overfitting.”\\n  Another argument was that the objective functions for these models\\nare highly nonconvex and optimization would get stuck at poor quality local\\nminima. This can be a problem, but turns out to be solvable for reasons that\\nare partially understood . Finally, despite the effectiveness of the trained\\nmodel in performing a task, the large number of parameters often makes it very\\nhard to understand how such a model works, and why a given input produces\\na particular output. This “interpretability problem” remains a key issue with\\ndeep learning models, and is the subject of much research .\\n There are many other variations and hybrid approaches in the story. Another\\nimportant one is the “pattern recognition” approach . This is also based\\non statistical inference but – like the symbolic approach – it emphasizes the value\\nof detailed understanding of the problem domain in designing the system. For\\nexample, one could hand-code the initial layers of an image recognition network\\nto detect lines or other significant “features” of the image. But unlike a purely\\nsymbolic approach, these features would be used as input to a general statistical\\nor neural model.\\n Another concept which illustrates the relation between the two approaches\\nis probabilistic reasoning, the use of rules such as “the chance of rain when it is\\ncloudy is 50%”. One can state and use such rules in a symbolic approach (see\\nfor example ), the essential distinction with connectionism is not the use of\\nprobabilities but rather the representation of knowledge in terms of explicit and\\nmeaningful rules.\\n As we suspect every reader has already heard, the symbolic approach was\\ndominant from the early days until 2012, and (along with many other suc-\\ncesses) led to a superhuman chess player, but seemed inadequate for our other\\n6two challenge tasks (theorem proving and question answering). In 2012 the\\nconnectionist approach surpassed other approaches to computer vision , and\\never since neural systems have gone from triumph to triumph. In 2016 the\\ndeep learning system AlphaZero surpassed the symbolic AI chess players (and\\nof course humans). Over the last few years, transformer models trained on a\\nlarge corpus of natural language to predict each next word as it appears, have\\nrevolutionized the field of natural language processing. As we write this the\\nstate of the art GPT-4 demonstrates truly remarkable performance at question\\nanswering, code generation and many other tasks .\\n The simplest and arguably deepest explanation for this history is that it is\\na consequence of the exponential growth of computational power and training\\ndatasets, which continues to the present day. Given limited computing power\\nand data, the ability of the symbolic and pattern recognition approaches to\\ndirectly incorporate human understanding into a system is a significant advan-\\ntage. On the other hand, given sufficiently large computing power and data,\\nthis advantage is nullified and may even become disadvantageous, as the human\\neffort required to code the system becomes the limiting resource. This point,\\nthat the most significant advances in AI (and computation more generally) have\\ncome from hardware improvements and replacing human engineering with data-\\ndriven methods, is forcefully made by Sutton in his “bitter lesson” essay .\\n In§3,§4 and §8 we will discuss scaling laws and evidence for and against the\\nidea that by continuing along the current path, training ever-larger models on\\never-larger datasets, we will achieve AGI (artificial general intelligence, whatever\\nthat means) and the realms beyond.\\n Up to now the symbolic and connectionist approaches have generally been\\nconsidered to be in tension.10There is another point of view which consid-\\ners them complementary, with a symbolic approach better suited for certain\\nproblems (for example logical reasoning) and connectionist for others (for ex-\\nample image recognition). Given this point of view one can seek a synthesis or\\n“neurosymbolic” approach, advocated in many works .\\n But are they in conflict at all? Another reconciliation is the hypothesis that\\nproblems which in the symbolic approach are solved using rules and algorithms,\\nare also being solved that way by neural systems and in particular by LLMs.\\n However, rather than the algorithms and rules being coded by humans, as the\\nresult of its training procedure the LLM has somehow learned them, encoded\\nin its networks in some as yet mysterious way. This vague hypothesis can be\\nsharpened in many ways, in part by proposing specific mechanisms by which\\nalgorithms and rules are encoded, in part by making general claims about the\\nalgorithms which are being learned. We discuss these ideas in §7 and §8.\\n10To better discuss this point one should refine the symbolic-connectionist dichotomy into\\nmultiple axes: system design versus learning from data; meaningful rules versus uninterpreted\\nmodels; combinatorial versus differentiable optimization; deterministic versus probabilistic.\\n 73 Language models\\nThroughout the history of linguistics, languages have been described in terms\\nof rules: rules of grammar, phonology, morphology, and so on, along with log-\\nical and other frameworks for describing meaning. This remains the case in\\nChomskyan linguistics and in much of theoretical linguistics.\\n By contrast, LLMs are statistical language models, meaning that they encode\\na probability distribution on strings of words, call this P(w1. . . w L), which\\napproximates the distribution realized by a large body (or “corpus”) of text in\\nthe language. The simplest example is the frequency or “1-gram” model defined\\nby taking the words to be independently distributed, so\\nP(w1. . . w L) =LY\\ni=1P(wi);P(w) =number of occurrences of win the corpus\\ntotal number of words in the corpus.\\n (1)\\nOf course, this model captures very little of the structure of language, which\\ninvolves dependencies between the word choices.\\n LLMs are generative models,11by which we will mean that there is a practi-\\ncal method for sampling from the distribution. To explain this, consider a word\\nprediction task in which some words in a string are given (the “input”) and\\nothers left blank (the “output”). Given a probability distribution P(w1. . . w L),\\nthere is a corresponding conditional probability distribution for the output given\\nthe input. As an example, suppose we are given the string “The cat \\noutside,” where “” is a “token” which marks the position of the missing\\nword. The relevant conditional probabilities might be\\nP(the cat went outside |the cat  outside) = 0 .5\\nP(the cat sat outside |the cat  outside) = 0 .2\\nand so on, summing to total probability 1. In the masked word prediction task,\\nthe model must determine (or sample from) this distribution.\\n A particularly convenient case is to give the conditional probability of the\\nword which follows a given string, which we denote as\\nP(wn+1|w1w2. . . w n−1wn). (2)\\nBy sampling this distribution to get a new word wn+1and appending it to the\\nend, the string can be extended one word at a time. Repeating this process\\ngives an arbitrarily long string, which by the laws of probability is a sample\\nfrom the original probability distribution P(w1. . . w L), for example\\nP(the cat went outside) = P(the) P(cat|the)P(went |the cat) P(outside |the cat went) .\\n This factorization of the probability into successive conditional probabilities\\ndefines the class of autoregressive models. One could furthermore require that\\n11Many different definitions of this term can be found in the literature.\\n 8the conditional probability Eq. 2 depends only on the kmost recent words, in\\nwhich case one would have a Markov model whose state is a string of kwords.\\n To evaluate how good a language model is, we want to quantify how well its\\nprobability distribution approximates that of the corpus (the empirical distribu-\\ntion). The standard measure of this is the cross entropy. For an autoregressive\\nmodel this is a sum of terms, one for each word in the corpus,12\\nL=−1\\nNN−nX\\ni=1logP(wi+n|wiwi+1. . . w i+n−1) (3)\\n One also refers to exp −Las “perplexity.” In a machine learning approach, we\\ncan use Eq. 3 as an objective function and minimize it as a function of the\\nnetwork parameters to train the network. We can then apply the many tools of\\nML: backpropagation, splitting the sum into batches, varying the learning rate\\nand so on, to get an efficient and effective model. While the details are an art\\nwhich depends on the particular domain and model architecture,13conceptually\\nthese are much the same for LLMs as for other machine learning models.\\n This statistical approach to modeling language has been pursued since the\\nlate 80’s  and many models were developed, such as the recurrent neu-\\nral network (RNN) which we will describe in §5. Following the general machine\\nlearning experience that supervised tasks (learning from input-output pairs)\\nare easier than unsupervised tasks, many of these works addressed machine\\ntranslation and parsing, for which there are good labeled datasets (documents\\nwith their translations; sentences with their grammatical structure). However\\nunlabeled datasets are much larger and by 2015 or so there was a sense that self-\\nsupervised learning was the next frontier , leading to more focus on masked\\nword prediction.\\n The history of transformer models starts with the 2017 proposal of Vaswani\\net al . Their model was designed for a translation task and was more com-\\nplicated than what we will explain in §6, but the essential idea to use attention\\nand positional encoding to represent all the relations between the words in a\\ntext originated here and is fully present.\\n The transformer architecture was taken up by many groups, and particularly\\ninfluential 2018 works include BERT  and GPT . BERT was trained by\\nmasking arbitrary words in a sentence (not just the next word), which allows\\nthe model to look backward and forward for context and leads to better results.\\n However it is not straightforward to sample from such a model, and eventually\\nthe simpler next word prediction approach followed by GPT won out.\\n Both of these models, and most work of this period, followed the paradigm\\nof pretraining followed by fine tuning. The idea was to first train for word\\nprediction on a very large corpus, to get a general purpose model. This would\\nthen be adapted to specific tasks such as question answering by fine tuning.\\n This means doing a second pass of supervised learning on a much smaller labeled\\n12With the sign, L ≥0 and better models have smaller L. The term “loss function” is often\\nused for an objective function with these properties.\\n 13In CS this term generally refers to the large scale arrangement of components of a system.\\n 9dataset, replacing next word prediction by the objective function for the specific\\ntask. Say we are doing question answering, this could be the accuracy of the\\nanswers. This two step procedure was justified by the notion of transfer learning,\\nmeaning that the capabilities of the general purpose model “transfer” to related\\nbut different tasks. This approach led to SOTA14results on many benchmarks\\nand motivated much further work.\\n Most importantly, a great deal of ingenuity and hard work was put into\\nsolving the engineering problems of training larger and larger models on larger\\nand larger datasets. As for the data, a lot of text is available on the web, with\\none much used archive of this data provided by Common Crawl.15Training can\\nlargely be done in parallel by dividing up this data, and the availability of large\\nclusters of GPU-enabled servers at industrial labs and through cloud computing\\nmeant that sufficient computing resources were available in principle. However,\\nthe overall cost of training scales as (at least) the product of model size and\\ndataset size, and this was becoming expensive. While the precise cost figures\\nfor the GPT series are not public, it is estimated that a single training run\\nof the largest GPT-3 models cost tens of millions of dollars. To motivate and\\nefficiently carry out such costly experiments, one needs some ability to predict in\\nadvance how changes in model and dataset size will affect the training methods\\n(for example the optimal choice of learning rate) and performance.\\n An important advance in this direction was the observation of power law\\nscaling in language model performance . Figure 2 plots the test loss16against\\nthe logarithms of the sizes and compute resources used, and these straight lines\\ncorrespond to a power law relation between size and perplexity. This scaling\\nholds over many decades in model size and, while the exponents α∼ −0.076 to\\n−0.095 are rather small, this is a strong argument that larger models will have\\nbetter performance. These ideas were also used to determine optimal model-\\ndataset size tradeoff  and the scaling of hyperparameters . These results\\nwere a significant input into the decision to do this very expensive research.\\n Year Model Number of Parameters Dataset size (tokens)\\n2018 GPT 110M 1B\\n2018 BERT 340M 3B\\n2019 GPT-2 1.5B 10B\\n2020 GPT-3 175B 500B\\n2022 PaLM 540B 780B\\n2023 GPT-4 1.4T (?) ?\\n Table 1: Large Language Models (M/B/T = million/billion/trillion). In many\\ncases several model sizes were considered; we quote the largest.\\n14State of the art, in other words an improvement over all previously evaluated models.\\n 15https://commoncrawl.org/\\n16This is Eq. 3 (minus log perplexity) evaluated on texts which were removed or “held out”\\nof the training set, to get a measure of generalization ability.\\n 10Dataset Size tokensParameters non-embeddingCompute PF-days, non-embeddingTest LossFigure 2: Language modeling performance as a function of model size, dataset\\nsize, and amount of compute used for training. From Kaplan et al, “Scaling\\nLaws for Neural Language Models,” 2020 .\\n Now it should be realized that, while the measure being improved here is\\nfairly objective, still there was no strong reason to think that improving it would\\nlead to models with qualitatively new “emergent” capabilities. But it appears\\nthat this is what happened: GPT-3 and its fine-tuned cousins (such as Codex)\\nwere able to do tasks, such as write computer code from a natural language\\ndescription, for which smaller models were almost worthless.17We will discuss\\nmore of this progress shortly, and speculate a bit in the conclusions.\\n One of the most interesting LLM phenomena is in-context learning, first\\ndiscussed in the original GPT-3 paper . This refers to the ability of an\\nLLM to carry out tasks different from its original objective without modify-\\ning its parameters, indeed without any need for additional training on the new\\ntask (fine tuning). Rather, after being given (as input text) a few examples\\nof input-output pairs, the LLM can be given another input and will generate\\na suitable output. Say the new task is question answering, then after a few\\nquestion-answer examples the LLM will answer the next question it is given.\\n While intuition based on human abilities might find this unremarkable, it is\\nactually quite unusual for an ML model and this is why the pretraining-fine\\ntuning paradigm was the usual approach in previous work. Of course the train-\\ning set already contains many examples of QA pairs. More striking are tasks\\nwhich are not much represented in the training set, such as finding anagrams\\nor rearranging letters in words. One can even do in-context “meta-learning” of\\nmachine learning tasks such as linear regression (see §4).\\n Once it is established that the model can generalize from a few examples, a\\nfurther step towards human capabilities is to try zero examples, instead simply\\nexplaining the task in natural language. At this point it becomes difficult to\\nclassify the tasks – should we consider the task of writing code from a natural\\nlanguage specification to be a form of translation, or an example of explaining\\nthe task, or something else? The relation between the input text or “prompt”\\n17A quantitative version of this claim is that performance for the “emergent” capability\\nimproves rapidly at some threshold value of the word prediction loss. This claim is disputed,\\nsee  for discussion.\\n 11and the output has many surprising features. For example, a standard tech-\\nnique in LLM question answering which measurably improves performance is to\\nprecede the question with a prompt such as “I will answer this question help-\\nfully and truthfully.” Is this somehow biasing the network towards certain texts\\nand away from others (after all the internet corpus is hardly a reliable source of\\ntruth) ? Suppose we have a theory of how this works, how can we test it? Does\\nthe model “know” anything about the truth of statements? \\n As has been much reported, one of the major difficulties in using LLMs\\nfor practical tasks is their propensity to invent facts (especially citations) and\\ntheir limited ability to do logical reasoning, algebra and other symbolic tasks.\\n A device for improving this, called “chain of thought prompting,” is to give\\nexamples (say of question answer task for definiteness) with some intermediate\\nreasoning steps spelled out. This was used in the Minerva QA system \\nwhich produced the example in Figure 1. Still the fraction of problems it solved\\ncorrectly is around 50% (the later GPT-4 is similar). Even for simpler questions,\\nthe reliability of GPT-4 is more like 90%. Much current research is devoted to\\nthis problem of reliable reasoning, as we discuss in §8.\\n 4 Phenomenology of language models\\nIn this section we discuss general claims, “non-invasive” experiments, and the-\\noretical arguments which do not depend on “microscopic details” of the models\\nsuch as the trained weights.18This includes evaluation of model capabiliities,\\nqualitative observations and scaling laws.\\n What can LLMs do? There is a huge body of work on this question, and any\\nattempt to review it would rapidly go out of date, but let us review the primary\\nmethod for studying it. This is benchmarking, the development of standardized\\nsets of test items for which model accuracy can be evaluated in a reproducible\\nway. This is in principle straightforward if the input corresponds to a single\\ncorrect output, as in multiple choice question answering.19If the answer is free-\\nform text, one can use text comparison metrics such as the ROUGE score. One\\ncurrent standard for evaluating LLMs, BIG-bench , combines 204 language\\ntasks (at first publication; they accept new tasks) including translation, QA,\\npuzzle solving, text classification and summarization, and tests of common sense\\nreasoning. A leaderboard listing the current best LLMs is at20. Another is the\\nEleutherAI “Language Model Evaluation Harness”21and leaderboard.22The\\nbenchmark suite HELM  measures additional metrics such as tendency to\\nrepeat copyrighted material, bias, toxicity and the like.\\n 18We are calling this “phenomenology” following the physics use of the term, not its use in\\npsychology and philosophy to describe the study of subjective experience.\\n 19A potential pitfall is that after a benchmark is published, the test items can find their\\nway into future training data and then be solved by memorization. Methods to detect and\\nprevent this are discussed in the references.\\n 20https://paperswithcode.com/dataset/big-bench\\n21https://github.com/EleutherAI/lm-evaluation-harness\\n22https://huggingface.co/spaces/HuggingFaceH4/open llmleaderboard\\n12Reasoning ability is of particular interest for mathematical and scientific ap-\\nplications – of course we all look forward to the day when computers will help us\\ngrade assignments, referee papers and do our research. There are many bench-\\nmarks for solving logical problems expressed in natural language. Benchmarks\\nfor mathematical theorem proving include NaturalProofs , MiniF2F \\nand ProofNet ; as of mid-2023 LLMs (and the best other systems) can find\\nmany proofs (20–80%) but still fail on some seemingly easy cases. Simpler as-\\npects of reasoning which have benchmarks are the ability to deal with negation\\n, consistency (between different phrasings of the same question) , and\\ncompositionality (the ability to analyze statements and problems into simpler\\nparts, solve these and combine the results) .\\n Natural language tasks are very complex, and benchmarks constructed from\\nreal world data cannot be used directly in theoretical considerations. For this\\npurpose one generally defines “toy worlds” and generates synthetic data. The\\npossibilities are endless, but some which have been used are arithmetic prob-\\nlems (decimal arithmetic; modular arithmetic), game play, solving systems of\\nequations, and parsing formal languages. A particularly interesting task is lin-\\near regression ; since this is the prototypical case of statistical inference, a\\nsystem which learns to do it can be said to be “learning how to learn.”\\n Coming to scaling laws, denote the model size (number of parameters) as\\nPand the dataset size (number of tokens in the corpus) as D, then there are\\ntwo general regimes. If we hold one of these (say P) fixed and take the other\\n(sayD) to infinity, then a law of large numbers applies and L ∼ 1/D. On the\\nother hand, if we take one parameter very large and study the dependence on\\nthe other, nontrivial power law scaling can emerge. In principle one can get\\ndifferent exponents for DandP, suggesting the ansatz\\nL(P, D) =\"\\x12Pc\\nP\\x13αP/αD\\n+Dc\\nD#αD\\n. (4)\\nwhere Lis test loss Eq. 3 computed in an optimally regularized model.23This\\nis a good fit to Figure 2.\\n While in Figure 2 the two exponents appear to differ, there is not really\\nconvincing evidence that this is significant. Before working hard on this, one\\nshould ask if there is any way to control the many choices involved, so as to define\\nuniversal exponents. One context in which this can be studied systematically is\\ntransfer learning, by distinguishing the dependence on the pretraining and fine\\ntuning datasets . Another relevant and practical question is whether one\\ncan prune the dataset to improve the scaling. It is intuitively plausible and can\\nbe shown in examples that sets of data items are worth more if they are diverse\\nthan if they are similar. The challenge is to find simple ways to quantify this\\nsimilarity; in  many proposals are studied.\\n 23Regularization is a standard technique in statistics and ML used to control overfitting by\\nmodels with too many parameters. If one does not regularize one sees other phenomena such\\nas double descent . For further discussion see .\\n 13Scaling laws can arise in many ways, not specific to language models. One\\nhypothesis is that the data lies on a low dimensional submanifold in a higher\\ndimensional space.24Both the number of parameters and the number of points\\nrequired to fit this manifold go as the dimension dof the manifold, and this\\nleads to αP=αD= 4/d(the precise coefficient 4 depends on assumptions\\nabout smoothness) .\\n A related hypothesis is that the spectral density of the data covariance falls\\noff as a power law, and in  Eq. 4 is derived for a random feature model with\\nthis covariance. This hypothesis follows from the low dimensional hypothesis\\nbut it is more general, for example these authors argue that additional features\\nderived from the data (as in nonlinear models such as FFN’s) generally have\\nthe same spectrum as the original data. One can also try to relate Eq. 4 and\\ncorrections to it to hypotheses about how tasks are learned .\\n What does the scaling of the information theoretic quantity Eq. 3 have to\\ndo with performance on tasks requiring intelligence? A priori , not much, but\\none way to motivate a focus on it is to draw an analogy with particle physics.\\n In the 30’s cosmic ray observations gave strong hints of new physics at higher\\nenergies, but the interesting events were too rare and uncontrolled to draw solid\\nconclusions. Thus physicists were motivated to build accelerators. These are\\nnot that expensive when they fit on a tabletop, but rapidly grow in size and\\ncost. How large does an accelerator need to be? The right measure is not its\\nsizeper se but rather the energy of the particles it can produce. The physics\\nrelating size and energy is not trivial (due to effects such as synchrotron ra-\\ndiation) but can be worked out, so one can make a good prediction of energy\\nreach. Still, as one increases energy, will one find a smooth extrapolation of\\nwhat came before, or will one discover qualitatively new phenomena? In the\\ngolden age of accelerator physics (the 50’s-70’s) much new physics was discov-\\nered, mostly associated with new particles which are produced only above sharp\\nenergy thresholds. Currently the highest energy accelerator is the Large Hadron\\nCollider at Cern, where the Higgs particle was discovered in 2012. While we\\nare still waiting for further important discoveries, the potential for discovery is\\ndetermined by measurable properties of the accelerator – by energy and secon-\\ndarily by intensity or “luminosity” – which we can judge even in the absence of\\nqualitative discoveries. In the analogy, perplexity is playing a similar role as an\\nobjective measure of language model performance defined independently of the\\nmore interesting qualitative behaviors which reflect “intelligence.”\\n How far can one push this analogy? Could perplexity be as central to lan-\\nguage as energy is to physics? Eq. 3 has a fairly objective definition, so the\\nidea is not completely crazy. But, not only was its relation to performance on\\nactual tasks not predictable in advance, even after the fact clear “thresholds” or\\nother signals for emergence of tasks have not yet been identified . Perhaps\\nif there are universal thresholds, evidence for them could be seen in humans.25\\nMore likely, additional variables (the quality and nature of the training corpus,\\n24In§5 we explain how text can be thought of embedded in a high dimensional space.\\n25Thanks to Misha Tsodyks for this suggestion.\\n 14details of the tasks, etc.) would need to be controlled to see them. This is\\nanother question probably better studied in simpler tasks using synthetic data.\\n The final topic we discuss is the behavior of the objective function (Eq. 3)\\nas a function of training time.26In almost all ML runs, such a plot shows long\\nplateaus interspersed with steep drops. This has been interpreted in many ways,\\nranging from evidence about the nature of learning, to a simple consequence of\\nrandomness of eigenvalues of the Hessian of the loss function. A more recent\\nobservation is to compare training and testing accuracy on the same plot. In\\n it was argued that these two metrics improve at two distinct stages of\\ntraining. First, the model memorizes training examples. Later, it generalizes\\nto the testing examples. This “grokking” phenomenon has been suggested as\\nevidence for learning of circuits , an idea we discuss in §7.\\n5 Simpler language models\\nHere we describe a few generative language models in detail to fix the concepts.\\n As points of notation, let Wbe the set of words (or, if the reader prefers,\\nnumbers which index a position in a list of words). We denote the cardinality of\\na setSas|S|, so|W|is the number of distinct words. The space of N-component\\nreal vectors is denoted RN.\\n The simplest model is the N-gram model defined in terms of the conditional\\nprobabilities\\nP(wN|w1w2. . . w N−1), (5)\\nwhich are all taken to be independent. Given this minimalist assumption, a\\nplausible way to estimate them from the corpus is\\nP(wN|w1w2. . . w N−1) =Number of occurrences of w1w2. . . w N−1wNP\\nwNumber of occurrences of w1w2. . . w N−1w.\\n (6)\\n This simple model with N= 3 or 4 works better than one might think (see exam-\\nples in ) and can be improved a bit by simple statistical tricks (“smoothing”).\\n But the exponential growth of the number of strings in Nmeans that there is\\nno hope of taking Nlarge enough to model even a single paragraph. The entire\\ninternet contains (in order of magnitude) 1012words, and such a corpus will\\ncontain only a vanishingly small fraction of the likely twenty word strings.27\\nA more general principle which we can take from the N-gram model is the dis-\\ntributional hypothesis, which has been pithily summarized as “you shall know\\na word by the company it keeps.”   In other words, by proper use of the\\nstatistics of neighboring words, one can define quantities which capture prop-\\nerties and even the meanings of words. The simplest expression of this idea\\n26This is roughly the time in which the gradient descent operates, see Eq. 16. In LLMs one\\noften considers each data item only once in a training run, so it is related to (but different\\nfrom) dataset size.\\n 27Statistical estimates of perplexity are in the 100’s, and the best current LLMs have per-\\nplexity ∼20.\\n 15is the co-occurrence matrix. Before explaining this, let us mention a detail of\\npractical systems, which in place of words use “tokens,” meaningful components\\nof words. A physics illustration is the word “supersymmetrization.” Even for a\\nnon-physicist reader encountering it for the first time, this word naturally breaks\\nup into “super,” “symmetry” and “ization,” pieces which appear in many words\\nand which are called tokens. And not only does this decomposition apply to\\nmany words, it helps to understand their meaning. This process of replacing\\nsingle words by strings of tokens (“tokenization”) is a first step in LLM pro-\\ncessing, and henceforth when we say “word” we will mean word or token in this\\nsense.\\n Given a corpus, we define its N-gram co-occurrence matrix MNto be the\\n|W| × |W| matrix whose ( w, w′) entry counts the number of N-grams in the\\ncorpus containing both words. This matrix defines a map from words to vectors\\nι:W →Rp(7)\\n(where the dimension p=|W|), by taking a word to the corresponding column\\nofMN. Such a map is called a word embedding.\\n Applying this map to each word independently, we can map a string of k\\nwords (in Wk) to a string of vectors, and this is the next step (after tokenization)\\nof LLM processing. One might worry that these are very high dimensional\\nvectors with many zero entries, which seems wasteful. A standard statistical\\ncure for this problem is to do principal component analysis (PCA). In words,\\ninstead of columns of MNwe use the columns of a p×|W| matrix Zchosen such\\nthatZtZis the best rank papproximation to MNin the sense that it minimizes\\ntr (ZtZ−MN)2. One can do better, but this gives the right idea.\\n Next, we feed this string of vectors into some machine learning model to get\\nan output which we use to predict the next word. If we just want the most\\nlikely next word, a good way is to output a vector v∈Rp, and choose the\\nword wwhich maximizes the inner product v·ι(w). We denote this relationship\\nasv∼ι(w). More generally, the standard inverse map from a vector to a\\nprobability distribution on words is the Boltzmann distribution on the inner\\nproducts. Explicitly, we postulate an inverse temperature β= 1/Tand take28\\nv→P(w) =eβv·ι(w)\\nP\\nw′eβv·ι(w′)(8)\\n Here is an observation  which supports the idea that word embeddings\\ncontain information about meaning. Since the embeddings are vectors, they can\\nbe added. Consider the following equation:\\nι(king) −ι(man) + ι(woman) ∼ι(?) (9)\\n28Tis the temperature parameter which can be set in (say) the GPT user interface. Also,\\nthis ratio of exponentials is usually called “softmax” in machine learning as its β→ ∞ limit\\nis the “argmax” function producing a vector whose nonzero components have the same index\\nvalues as the largest of the input(s).\\n 16One might hope that the word which maximizes this inner product is “queen,”\\nand indeed it is so. There are many more such examples; empirically one needs\\nthe dimension p≳100 for this to work. One can argue  that it follows\\nfrom relations between co-occurence statistics:29\\n∀w,MN(w,king) /#(king)\\nMN(w,queen) /#(queen)≈MN(w,man) /#(man)\\nMN(w,woman) /#(woman)(10)\\n Given these ideas and a map Ffrom a list of vectors to a vector, we can\\nnow propose a very general class of L-gram autoregressive language models as\\nthe combination of the following steps:\\n1. Map the Linput words witoLvectors ι(wi).\\n 2. Apply Fto the list of these vectors to get a prediction vector v.\\n3. Use the inverse map Eq. 8 to get a probability distribution over words.\\n Furthermore, if the map Fhas parameters, given a corpus we can determine\\nthem by optimizing the function Eq. 3 with respect to the parameters. And once\\nwe bring in optimization, we can also optimize with respect to the coefficients of\\nthe embedding map Eq. 7, so that we can dispense with co-occurence statistics.\\n This is the general prescription followed by the LLMs, and to complete it we\\njust need to specify a family of maps F. One possibility is to use a general (fully\\nconnected) feed forward neural network (FFN, also called MLP for multilayer\\nperceptron). We recall that an FFN is a composition of two general types of\\nfunctions, linear maps Wiand nonlinear maps θ, so that\\nF(v) =Wd◦θ◦Wd−1◦θ◦. . . ◦W1◦θ◦W0. (11)\\nIn more concrete terms, the maps Wiare multiplication by rectangular matrices\\nof parameters (usually called “weights” in this context), while the maps θact\\nindependently on each component of their input vector by a fixed nonlinear\\nfunction such as tanh or (more typically) ReLU (identity for x≥0 and zero\\nforx <0). The main fact we recall about FFN’s is that, in the limit that the\\nnumber of parameters becomes large, they can approximate any given function\\narbitrarily well . We refer the reader interested in learning more to .\\n We can get a very natural deep learning version of the L-gram models by\\nusing an FFN for the map Fin the prescription above . Since this asked for\\na map from a list of vectors to a vector, we need to convert the input list into a\\nsingle vector. This is easy: we can take the direct sum of the input vectors, i.e.\\nthe dimension L×pvector whose components are the concatenated lists of their\\ncomponents. Using today’s FFNs, one could implement this with L∼100 or so.\\n There does not seem to be much work on large fully connected FFN language\\nmodels, because by the time the technology advanced to this point the far more\\nefficient transformer models had taken over. Still, they illustrate the general\\n29Here #( w) denotes the number of occurences of “ w” in the corpus. These ratios can also\\nbe expressed in terms of the pairwise mutual information, PN(w, u)/P(w)P(u).\\n 17idea and also one of its most obvious limitations. Even with L∼100, often\\npredicting the next word requires remembering words which appeared farther\\nback. To solve this problem we need to incorporate some sort of memory into\\nthe model.\\n The simplest memory is an additional state variable which is updated with\\neach word and used like the other inputs. To do this, we should take the state\\nto be a vector in some Rq. This brings us to the recurrent neural network or\\nRNN. Its definition is hardly any more complex than what we saw before. With\\neach word position (say with index i) we will associate a state vector siwhich\\ncan depend on words up to wiand on the immediately previous state. Then,\\nwe let the map Fdetermine both the next word and the next state as\\n(vi+1, si+1) =F(si, vi, vi−1, vi−2, . . . , v i−k+1), (12)\\nwhere the parenthesis notation on the left hand side means that the output\\nvector of Fis the concatenation of two direct summand output vectors.\\n Mathematically, Eq. 12 is a discrete dynamical system. If we grant that\\nFcan be an arbitrary map, this is a very general class of systems. One way\\nof characterizing its generality is through computational complexity theory, by\\nasking what classes of computation it can perform. In  it was argued that\\nthe RNN is a universal computer, but this granted that the computation of F\\nin Eq. 11 could use infinite precision numbers. Under realistic assumptions\\nthe right complexity class is a finite state machine, which can recognize regular\\nlanguages . We will say more from this point of view in §7.\\n There are many variations on the RNN such as LSTM’s , each with their\\nown advantages, but we must move on.\\n 6 Recipe for an LLM\\nWe are now ready to define the transformer model.30It is simply another class\\nof maps Ffrom lists of vectors to a vector to be used in the prescription above.\\n Indeed, it is a natural generalization of the FFN which is associated to permu-\\ntational symmetry. This is in direct analogy to the use of convolutional neural\\nnetworks (CNNs) for image recognition, which are FFNs which are equivariant\\nunder the symmetry of translations in two dimensions which is natural for the\\nset of images.\\n A transformer is a composition of two types of functions (layers) taken in\\nalternation, each mapping an input list of Lvectors {ui}to an output list of L\\nvectors {vi}. One of these is an FFN as previously discussed, but now applied\\nto each embedding vector independently, so vi=FFFN(ui).\\n 30Other reviews explaining these definitions include .\\n 18The other layer type is called attention, and it is defined as follows:\\n{ui} → { vi=WiX\\nj=1ci,juj} (13)\\nci,j≡expui·B·ujPi\\nj=1expui·B·uj(14)\\nwhere Bis a learnable matrix whose elements are model parameters (equiva-\\nlently, u·B·vis a bilinear form) and Wis a linear map (also learnable).\\n In words, an item viin the output vector is (a linear transformation of) a\\nweighted sum of the inputs ujwith i≤jand can depend on any of them.31\\nThe weights ci,jare given by a “softmax” or Boltzmann weight just as in Eq.\\n8. Thus there is a very general learnable way for each output to choose which\\nof the input vectors are most useful as inputs. Suppose the product u·B·vis\\nthe dot product, then attention selects the input components ujmost similar\\nto the current unit’s input, uj∼uiin the notation of §5. The matrix Ballows\\nfor comparing different parts of the embeddings and ignoring other parts, in a\\nway determined by optimizing the objective function Eq. 3.\\n Composing these two types of functions (or layers) produces a map from\\nRp×LtoRp×L. Often one takes, instead of the pure FFN and attention func-\\ntions, sums of these with the identity function (residual connections). The FFNs\\ngenerally have a single hidden layer which can be of a different dimension, call\\nthisph.32Finally, while the language model prescription asked for a map to Rp,\\nthis is easily obtained by just taking the last vector in the final output list.\\n There are two more essential details to cover (and many minor details we\\nwill skip). The first is the concept of “attention head.” The definition Eq. 13\\nallowed for a general linear transformation Wwhose range is the output vector.\\n We are free to choose its dimension, call it q, and typically one takes this to\\nbe much less than the embedding dimension p. In return one can use many\\ncopies of Eqs. 13,14 in parallel with different choices for BandW, to produce\\nmany outputs. One can then concatenate these outputs to get a final output of\\ndimension p. These copies are called attention heads and we will denote their\\nnumber by H, sop=Hq.\\n The second essential detail is that, so far, there is nothing in the definition\\nthat keeps track of the order of the list of input vectors; the output of Eq. 13\\nwill be invariant under a general permutation of the input vectors. While this\\nis an elegant property, it is not what we want for processing language, for which\\nthe order of the words matters. The cure for this is very simple: one takes as\\ninputs not the word embeddings Eq. 7, but the direct sum (concatenation) of\\nthese with positional embedding vectors, i.e.vectors which encode the position\\n(index) of the word in the string. These can be a combination of sines and\\n31The restriction i≤jto previous or current inputs is done to get an autoregressive model;\\none can relax this for other purposes.\\n 32Explicitly, vi=W1·max(0 , W0·ui+b0) +b1, where b0,1are more learnable parameters.\\n 19cosines of various frequencies, such as \\n(e2i−1, e2i) = ( cosposition\\n100002i/dpos,sinposition\\n100002i/dpos); i∈ {1, . . . ,dpos\\n2}(15)\\n One could instead treat these vectors as learnable parameters. Still, the trig\\nfunction basis for positions may be significant. It has been generalized to rep-\\nresent other graph structures by using eigenfunctions of the graph Laplacian as\\npositional embeddings.\\n The invariance of the transformer model under permutation symmetry is\\nreminiscent of the point we mentioned earlier, that translation symmetry mo-\\ntivates the CNN. However permutation symmetry is badly broken in language,\\neven in the simplest formal languages,33and it is not obvious why this should\\nbe a useful property for the model to have. One might argue that although any\\nparticular language breaks permutation symmetry, it acts naturally on the en-\\nsemble of languages and thus should have a simple representation. For example,\\nbesides the usual infix arithmetic notation “ a+b”, one could instead use prefix\\n“+a b” or postfix “ a b+”. Translating between these notations is arguably\\neasier for permutation invariant maps using position embeddings. An oppos-\\ning view would be that permutation symmetry is just a secondary property of\\nthe simplest model using attention, and that the main point is to explain the\\nvalue of attention. In addition to its ability to select similar items, it provides\\na simple way to take products of embedding vectors. In computational com-\\nplexity terms, attention enlarges the class of circuits which can be simulated by\\na constant depth transformer . Physics analogies of Eqs. 13,14,\\nespecially to the Hopfield model, may be important .\\n A major practical advantage of the transformer over the RNN and other\\nprevious architectures is that the computations in the attention mechanism can\\nbe done in parallel, so (given sufficiently many processors) the time required does\\nnot increase with the window length L. This is by contrast with the RNN in\\nwhich information propagates from one word to the next, so a window of length\\nLrequires time Lto process. On the other hand the ability of each unit to pay\\nattention to every previous unit means that the total computation required by\\nthe transformer scales as L2. This is the limiting factor for increasing Land\\nthis is widely seen as a problem. There has been a lot of work to improve this\\nscaling, by removing some of the connections (as in sparse attention ), by\\nintroducing multiscale structure, or in other ways.\\n Let us summarize by listing the hyperparameters34and their values for the\\nlargest (175B) GPT-3 . They are\\n•Embedding dimension p= 12288 and hidden layer dimension ph= 4p.\\n •Window length L= 4096 or 8192.\\n •Depth D= 96, counting both FFN (Eq. 11) and attention (Eq. 13)\\nlayers.35\\n33Compare the logical implications A→BandB→A.\\n34This term refers to model choices which are not learned through gradient descent.\\n 35Some of the attention layers in GPT-3 are sparse.\\n 20•Number of heads H= 96 (the equality with Dis a coincidence as far as I\\nknow).\\n The total number of parameters is roughly 12 Dp2.\\n As mentioned earlier, all of these parameters, and the parameters of the em-\\nbedding map Eq. 7, are determined as follows. One generally starts with “ran-\\ndom” initial conditions, usually meaning that each parameter is drawn from a\\nnormal distribution with mean zero and variance chosen so that the linear maps\\nhave expected norm independent of the hyperparameters. As in random matrix\\ntheory, this typically means var( Wi,j)∼1/p, though there are refinements .\\n One then sequences through the training corpus and performs a step of gradient\\ndescent of Eq. 3 for each “batch” of words (here a group of ∼106words). In\\neach step, the parameters ⃗θare modified as\\n⃗θ→⃗θ−η∂Lb\\n∂⃗θ(16)\\nwhere Lbis Eq. 3 restricted to the batch, the conditional probability Pcomes\\nout of Eq. 8 applied to the output of the transformer, and ηis a positive real\\nnumber (the learning rate hyperparameter, here around 10−4).\\n The result of following this procedure on a dataset of natural language text,36\\nsupplemented by many enhancements which are described in the literature and\\nin the model source codes but which may be less important for conceptual\\nunderstanding, is an LLM with the capabiliities we described.\\n 7 Studying the internal workings\\n The success of this procedure raises many questions. Some can be asked about\\nmore or less any ML model – for example, questions about when and how\\noptimization of the objective function Eq. 3 achieves “good” local minima\\n(value near the global minimum and models which generalize well), and the\\norigin of scaling laws like Eq. 4. These are the subject of the general theory of\\nmachine learning, for which we refer to  and much other work.\\n Other questions, and understanding the many striking abilities discussed\\nearlier, sound more specific to LLMs. What would it mean to understand how\\nChatGPT writes poetry based on prompts, or solves physics word problems?\\n At present this is by no means clear and it may be that entirely new concepts\\nare needed to do this. Still, I share the belief that we can go very far towards\\nunderstanding LLMs by building on previous work in computer science, ma-\\nchine learning and AI, and many other fields. There is a well established field\\nof statistical physics and ML  which will surely contribute. Physics ideas\\nare also very relevant for tasks with spatial symmetry, such as image genera-\\ntion  and recognition . The unexpected mathematical simplicity of the\\n36As always in ML it is important that the dataset be “clean” – consistently tokenized, not\\nhaving too much garbage text or repetitions, etc.. Many later LLMs also use programming\\nlanguage code in the dataset. Besides making code generation possible, it has been reported\\nthat this improves performance on natural language reasoning tasks.\\n 21transformer model means that mathematical insights could be valuable. We can\\nalso follow approaches used in neuroscience, psychology, and cognitive science.\\n An evident observation is that the paradigm of neuroscience – careful study\\nof the microscopic workings of the system, following a reductionist philosophy\\n– is far more practical for ML models than it is for human brains, as the micro-\\nscopic workings are fully explicit. This is not to say that it is easy, as we still face\\nthe difficulty of extracting meaning from a system with billions of components\\nand parameters. How could we do this for LLMs?\\n One familiar starting point in neuroscience is to measure the activity of\\nneurons and try to correlate it with properties of the system inputs or outputs.\\n The “grandmother cell” which fires when a subject sees his or her grandmother\\nis an extreme (and controversial) example. Better established are the “place\\ncells” in the hippocampus which fire when an animal passes through a specific\\npart of its environment.\\n Generally there is no reason why the representation should be so direct; there\\nmight be some “neural code” which maps stimuli onto specific combinations\\nor patterns of activity. The details of the neural code could even be different\\nbetween one individual and the next. Analogous concepts in LLMs are the maps\\nfrom input strings to intermediate results or “activations.” The first of these\\nis the embedding map Eq. 7. Considering each layer in succession, its outputs\\n(sometimes called “contextualized embeddings”) also define such a map. The\\ndetails of these maps depend on details of the model, the training dataset and\\nthe choices made in the training procedure. Besides the hyperparameters, these\\ninclude the random initializations of the parameters, the order in which data\\nitems are considered in training and their grouping into batches. Even small\\ndifferences can be amplified by the nonlinear nature of the loss landscape.\\n One way to deal with this indeterminacy is to look for structure in the maps\\nwhich does not depend on these choices. The linear relations Eq. 9 between word\\nembeddings are a very elegant example, telling us (and presumably the model)\\nsomething about the meanings of the words they represent. Moving on to the\\nlater layers, one can ask whether contextualized embeddings carry information\\nabout the grammatical role of a word, about other words it is associated to\\n(such as the referent of a pronoun), etc.. One can go on to ask whether any\\nof the many structures which – one would think – need to be represented to\\nunderstand the real world, are visible in these embeddings.\\n Many structures are too intricate to show up in linear relations. A more\\ngeneral approach is to postulate a “target” for each training data item and\\ntrain a “probe” model (usually an FFN) to predict it from the embeddings. If\\nthis works, one can go on to modify the internal representation in a minimal way\\nwhich changes the probe prediction, and check if this leads to the corresponding\\neffects on the output (see  and references there).\\n This procedure is simpler to explain in an example. A pretty example of\\nprobing for a world model is the recent work of Li et al  (see also ) on\\nrepresentations in a transformer model trained to play the board game Othello.37\\n37For readers not familiar with this game, two players alternate in placing black and white\\n22They train a model “Othello-GPT”38to take as input a sequence of 60 legal\\nmoves, for example “E3 D3 ...” in the standard algebraic notation, and at each\\nstep to predict the next move. The trained model outputs only legal moves\\nwith very high accuracy, and the question is whether this is done using internal\\nrepresentations which reflect the state of the game board, say the presence\\nof a given color tile in a given position. Following the probe paradigm, they\\nobtain FFNs which, given intermediate activations, can predict whether a board\\nposition is occupied and by which color tile. Furthermore, after modifying\\nthe activations so that the FFN’s output has flipped a tile color, the model\\npredicts legal moves for the modified board state, confirming the identification.\\n Neuroscientists can only dream of doing such targeted experiments.\\n Numerous probe studies have been done on LLMs. One very basic ques-\\ntion is how they understand grammatical roles and relations such as subject,\\nobject and the like. This question can be sharpened to probing their internal\\nrepresentations for parse trees, a concept we review in the appendix. To get\\nthe targets for the probe, one can use a large dataset of sentences labeled with\\nparse trees, the Penn Treebank . This was done for BERT in  by\\nthe following procedure: denote the embedding (in a fixed layer) of word ias\\nui, then the model learns a projection Pon this space, such that the distances\\nd(i, j)≡ ||P(ui−uj)||in this inner product well approximate the distance be-\\ntween words iandjdefined as the length of the shortest path connecting them\\nin the parse tree. For BERT (with d∼1000) this worked well with a projection\\nPof rank ∼50.\\n Once one knows something about how information is represented by the\\nmodels, one can go on to try to understand how the computations are done. One\\napproach, also analogous to neuroscience, is to look for specific “circuits” which\\nperform specific computations. An example of a circuit which appears in trained\\ntransformer models is the induction head . This performs the following\\ntask: given a sequence such as “ A B . . . A ” it predicts a repetition, in this\\nexample “ B.” The matching between the tokens (the two A’s in the example) is\\ndone by attention. A number of works have proposed and studied such circuits,\\nwith various motivations and using various theoretical lenses: interpretability\\nand LLMs , in-context learning , formal language theory ,\\ncomputational complexity theory , etc..\\n Reverse engineering a large network ab initio ,i.e.with minimal assumptions\\nabout what it is doing, seems challenging, but maybe automated methods will be\\ndeveloped . Another approach is to first develop a detailed computational\\nmodel (CM) to perform a task without looking too much at the system under\\nstudy, and then look for evidence for or against the hypothesis that the system\\nunder study uses it. This approach also has a long history in neuroscience \\nand ways to test such hypotheses have been much discussed. As an example\\ntiles on an 8 ×8 board, and each move results in “flipping” some opponent pieces to the\\nplayer’s color. The main point for us is that the function from moves to board state is easily\\ncomputable yet very nonlocal and nonlinear.\\n 38While this model shares the GPT architecture, it is not trained on any language data,\\njust on Othello games.\\n 23of a research tactic which does not require opening the black box, one can\\nconsider illusions which fool the system in some way. The response to these\\nwill often depend on contingent and non-optimal aspects of the model, so one\\ncan distinguish different models which solve the same task. A new class of\\npredictions which becomes testable for LLMs is to look at performance as a\\nfunction of model size (depth; number of parameters). A particular CM might\\nrequire a certain model size or dataset properties in order to perform well. And\\nof course, one can open the black box: by assuming a particular CM, one can\\nmake predictions for what probe experiments should work.\\n Simple tasks studied in this approach include modular addition  and\\nlinear regression , where several CM’s (gradient descent, ridge regression and\\nexact least squares) were compared. Turning to language processing, a CM for\\nparsing by transformer LLMs was developed in Zhou et al . While this\\nis too lengthy to explain in detail here, let us give the basic idea, starting\\nfrom the PCFG framework discussed in the appendix. Rather than try to\\nrepresent a parse tree in terms of nodes and edges, it is represented by giving\\neach position iin the list of words a set of variables αi,t,j, where tindexes a\\nnonterminal (a left hand side of a rule) and jis another position. If αi,t,jis\\nturned on, this means that a rule with ton the l.h.s. was used to generate\\nthat part of the tree stretching from position ito position j. This can be\\ngeneralized to let αi,t,jbe the probability that a rule is used. These variables\\n(and additional variables βdescribing the rules used higher in the tree) satisfy\\nsimple recursion relations (the Inside-Outside parsing algorithm ). If the\\nrules have at most two symbols on the r.h.s.,39these recursion relations are\\nquadratic in the variables. By encoding the αvariables as components of the\\nembedding, they can be implemented using attention.\\n Naively, this model predicts that embedding dimension pmust be very large,\\nof order the number of nonterminals times the length of a sentence. Since\\nrealistic grammars for English have many hundreds of nonterminals, this seems\\nto contradict the good performance of transformers with p∼1000. This problem\\nis resolved by two observations, of which the first is that one can get fairly good\\nparsing with many fewer ( ∼20) nonterminals. The second is compression, that\\nembeddings and circuits which are simple and interpretable can be mapped into\\nmore “random-looking” lower dimensional forms. This is a well understood\\nconcept for metric spaces , which was implicit in the discussion of word\\nembeddings in §5. There the simplest construction (the co-occurence matrix)\\nproduced vectors with one component for each word, but by projecting on a\\nsubspace one could greatly reduce this dimension with little loss in accuracy.\\n The generalization of these ideas to neural networks seems important.\\n Once one believes an LLM is carrying out a task using a particular circuit\\nor CM, one can go on to ask how it learned this implementation from the data.\\n One can get theoretical results in the limit of infinite training data and/or for\\nsimple tasks in which the dataset is constructed by a random process. Learning\\n39One can rewrite any grammar to have this property (Chomsky normal form) by introduc-\\ning more nonterminals.\\n 24in transformer models trained on realistic amounts of data is mostly studied\\nempirically and using synthetic data. A few recent interesting works are . Intuitively one expects that simpler instances of a task are learned first,\\nallowing the model to learn features which are needed to analyze more complex\\ninstances, and there is a lot of evidence for this. The idea that many submodels\\ncan be learned simultaneously, including straight memorization and submodels\\nwhich rely on structure, also seems important. Ultimately learnability is crucial\\nbut we should keep in mind that in analogous questions in physics, evolution,\\nand so on, it is much easier to understand optimal and critical points in the\\nlandscape than to understand dynamics.\\n This brings us to in-context learning, the ability of an LLM to perform\\ndiverse tasks given only a few examples of input-output pairs. The simplest\\nhypothesis is that the model has learned the individual tasks, and the examples\\nare selecting a particular task from this repertoire. It has been argued that\\nthis is guaranteed to happen (in the infinite data limit) for a model trained\\non a mixture of tasks . If the many tasks have common aspects (for\\nexample parsing might be used in any linguistic task), one can ask how the\\nmodel takes advantage of this, a question discussed in .\\n Understanding LLMs is a very active research area and there is much more\\nwe could say, but let us finish by summarizing the two main approaches we\\ndescribed. One can postulate a representation and a computation designed to\\nperform a task, and look for evidence that the LLM actually uses the postu-\\nlated structure. Alternatively, one can look for a function in some simpler class\\n(such as digital circuits) which well approximates the function computed by the\\ntransformer model, and then “reverse engineer” the simpler function to find\\nout what it is doing. Either or both of these procedures could lead to inter-\\npretable systems and if so, are answers to the question “what has the LLM\\nlearned.” There is no guarantee that they will work and it might turn out that\\none cannot understand LLMs without new ideas, but they deserve to be tried.\\n 8 Questions and discussion\\nLarge language models have revolutionized computational linguistics and opened\\nup many new applications of AI. Understanding how they work is both straight-\\nforward (we explained it in §6) and at the same time an outstanding scientific\\nchallenge. This is because the question “how do they work” has multiple mean-\\nings. On the one hand, LLMs are a relatively simple solution to the task of\\npredicting the likely next word in a text. On the other hand, they also seem to\\nperform many other tasks which require intelligence, such as solving the physics\\nword problem in Figure 1. While we do not have a strong understanding of\\nwhat a system which can perform these tasks must do, a vast body of work in\\ncognitive science and AI supports one’s first naive intuition that such a system\\nmust be doing sophisticated analyses of language, must contain models of the\\nreal world, and must be able to do fairly general logical reasoning. Before it\\nwas demonstrated, the idea that all this could be learned as a byproduct of\\n25word prediction would have seemed hopelessly optimistic, had anyone dared to\\nsuggest it.\\n Extraordinary claims should be greeted with skepticism. One must guard\\nagainst the possibility that a successful ML system is actually picking up on\\nsuperficial aspects or statistical regularities of the inputs, the “clever Hans”\\neffect. Addressing this is an important function of the benchmark evaluations\\ndiscussed in §4. Of course as LLMs get good at performing tasks of practical\\nvalue, the skeptical position becomes hard to maintain.\\n Intelligence and language are incredibly complex and diverse. According to\\nMinsky,40this diversity is a defining feature of intelligence. The goal of under-\\nstanding LLMs (or any general AI) will not be accomplished by understanding\\nall of the content in their training data, the “entire internet.” Rather, the trick\\nwe need to understand is how a single system can learn from this diverse corpus\\nto perform a wide range of tasks. Theories of “what is learnable” are a central\\npart of computer science . Although theoretical understanding has a long\\nway to go to catch up with LLM capabilities, for simpler and better understood\\ntasks much is known.\\n In these notes we mostly looked at this question through the lens of computer\\nscience, and took as the gold standard for explaining how an LLM learns and\\nperforms a task, a computational model expressed as an algorithm or a circuit\\ntogether with arguments that the trained LLM realizes this model. This point of\\nview has many more insights to offer, but before we discuss them let us consider\\nsome other points of view. In §7 we drew the analogy between detailed study\\nof transformer circuits and neuroscience – what others can we consider?\\n Another analogy is with cognitive psychology. LLMs are sufficiently human-\\nlike to make this interesting, and there is a growing literature which applies tests\\nand experimental protocols from psychology to LLMs, see for example  and\\nthe many references there. When discussing this, we should keep in mind the\\nvast differences between how humans and LLMs function. Human brains are\\nnot believed to use the backpropagation learning algorithm, indeed it has been\\nargued that biological neural systems cannot use it . Perhaps related to this,\\nbrains are not feed-forward networks but have many bidirectional connections.\\n Whatever brains are doing, it works very well: LLMs (like other current deep\\nlearning systems) need far more training data than humans. Furthermore, the\\nLLMs we discussed do not interact with the world. Some argue that on philo-\\nsophical grounds, a model trained only on language prediction can never learn\\nmeaning . While I do not find this particular claim convincing, I agree that\\nwe should not assume that LLMs perform tasks the same way humans do. Still\\nboth similarities and differences are interesting; can we make the analogies with\\ncognitive psychology more precise?\\n One analogy , is with the well known concept of “fast and slow think-\\ning” in behavioral psychology . To summarize, humans are postulated to\\nhave two modes of thought, “system 1” which makes fast, intuitive judgments,\\n40What magical trick makes us intelligent? The trick is that there is no trick. The power\\nof intelligence stems from our vast diversity, not from any single, perfect principle. \\n 26and “system 2” which can focus attention and do calculations, logic, and plan-\\nning. While system 2 is more general and less error-prone, using it requires\\nconscious attention and effort. According to the analogy, LLMs implement sys-\\ntem 1 thinking, and are weak at system 2 thinking.\\n In  it is argued that LLMs have “formal linguistic competence” but not\\n“functional competence.” In plainer terms, they are solving problems by manip-\\nulating language using rules, but they lack other mechanisms of human thought.\\n While it may be surprising that a purely rule-based system could do all that\\nLLMs can do, we do not have a good intuition about what rule-based systems\\nwith billions of rules can do.\\n What are the other mechanisms? There is a long-standing hypothesis in cog-\\nnitive science, modularity of mind , according to which the human brain has\\nmany “mental modules” with different capabilities. These include a language\\nmodule of the sort that Chomsky famously advocated and many others, includ-\\ning one for geometric and physical reasoning, another for social reasoning and\\ntheory of mind, and perhaps others. Notably, formal logic and mathematical\\nreasoning seem to call upon different brain regions from those which specialize\\nin language , suggesting that these functions are performed by different men-\\ntal modules. One can thus hypothesize that LLMs have commonalities with the\\nhuman language module and might be useful scientific models for it,41but that\\nprogress towards human level capability will eventually stall without analogs of\\nthe other modules. \\n A related claim is that current LLMs, even when they perform well on bench-\\nmarks, do not construct models of the world. Consider reasoning about spatial\\nrelations – for example if A is in front of B is in front of C, then A is in front of\\nC. Such reasoning is greatly facilitated by representing the locations of objects\\nin space, perhaps in terms of coordinates, perhaps using “place cells” or in some\\nother non-linguistic way. If distance from the observer is explicitly represented\\nand used in reasoning, then it becomes hard to get this type of question wrong.\\n Conversely, to the extent that LLMs do get it wrong, this might be evidence\\nthat they lack this type of world model or cannot effectively use it.\\n There are many papers exhibiting LLM errors and suggesting such inter-\\npretations, but often one finds that next years’ model does not make the same\\nerrors. At the present rate of progress it seems premature to draw any strong\\nconclusions. My own opinion is that there is no barrier in principle to LLMs\\nconstructing internal non-linguistic models of the world, and the work  on\\nOthello-GPT discussed in §7 is a nice demonstration of what is possible. This\\nis not to say that any and all models can be learned, but rather that it might\\nbe better for now to focus on other significant differences between LLM and\\nhuman reasoning, of which there are many. I will come back to this below.\\n If LLMs and other connectionist systems do not work in the same way as\\nbrains, what other guidance do we have? In §7 we discussed one answer, the\\nhypothesis that they work much like the algorithms and circuits studied in\\n41Chomsky rejects this idea, saying that “The child’s operating system is completely differ-\\nent from that of a machine learning program.” (New York Times, March 8, 2023).\\n 27computer science. Perhaps trained LLMs implement algorithms like those de-\\nsigned by computational linguists, or perhaps new algorithms which were not\\npreviously thought up but which can be understood in similar terms. In either\\nversion this is still a hypothesis, but if we grant it we can draw on insights from\\ntheoretical computer science which apply to all such algorithms.\\n Computational complexity theory  makes many statements and con-\\njectures about how the time and space required by a particular computation\\ndepends on the size of the problem (usually meaning the length of the input).\\n The most famous of these, the P̸=NPconjecture, states (very loosely) that for\\nproblems which involve satisfying general logical statements, finding a solution\\ncan be much harder than checking that the solution is correct.\\n From this point of view, a central question is the complexity class of circuits\\nwhich can be realized by constant depth transformers, meaning that the number\\nof layers does not grow with the window size. Roughly, this is the complexity\\nclass TC0of constant depth circuits with threshold gates . Of\\ncourse in an autoregressive LLM one can repeat this operation to compute a\\nsequence of words: thus the circuit defines the transition function of a finite\\nstate machine (FSM) where the state is the window, and the LLM has learned\\nto simulate this FSM. If a natural algorithm to perform a task is in a more\\ndifficult complexity class than the FSM can handle, this is a reason to think the\\ntask cannot be learned by this type of LLM. Conversely, one might conjecture\\nthat any task for which there is an algorithm in this class can be learned, at\\nleast in the limit of an infinite amount of training data.\\n What about the lenses of pure mathematics, theoretical physics and allied\\nfields? Besides my own personal interest in them, these fields have made sub-\\nstantial contributions to statistics and machine learning, especially the interface\\nbetween statistical physics and machine learning is a vibrant field of research\\n. Spin glass theory made a very deep impact, starting with the Hopfield\\nmodel and developing into a far-reaching theory of optimization landscapes and\\ncomplexity. Random matrix theory is central to high dimensional statistics \\nand in many approaches to understanding deep learning . Mathematical\\napproaches to language such as  can reveal new structure and\\nprovide deeper understanding.\\n Another reason to think pure mathematics and theoretical physics have more\\nto contribute is that neural networks, transformers, and many of the models of\\nneuroscience, are formulated in terms of real variables and continuous mathe-\\nmatics. By contrast, computer science is largely based on discrete mathematics,\\nappropriate for some but not all questions. Perhaps word embeddings have im-\\nportant geometric properties, or perhaps the dynamics of gradient descent are\\nbest understood through the intuitions of continuous mathematics and physics.\\n Arguments such as those in §7 which reduce neural networks to digital circuits,\\neven if they do explain their functioning, may not be adequate to explain how\\nthey are learned.\\n Having at least mentioned some of the many points of view, let me combine\\nthese insights and speculate a bit on where this is going. Let me focus on\\nthree capabilities which seem lacking in current LLMs: planning, confidence\\n28judgments, and reflection.\\n Planning, solving problems whose solution requires choosing a series of ac-\\ntions and/or the consideration of future actions by other agents, is one of the\\ncore problems of AI. Making plans generally requires search, and in general\\nsearch is hard (assuming P̸=NP). A familiar example is a chess program,\\nwhich searches through a game tree to judge the longer term value of a candi-\\ndate move by hypothesizing possible future moves. While much of the success\\nof AlphaGo and AlphaZero is attributed to reinforcement learning by self-play,\\nthey also search through game trees; indeed the Monte Carlo tree search algo-\\nrithm on which they built  was considered a key enabling breakthrough.\\n By contrast, LLMs have no component dedicated to search. While it does\\nnot seem impossible that search trees or other structures could be learned inter-\\nnally (like world models), it seems intuitively clear that an autoregressive model\\nwhich predicts one word at a time and cannot go back to revise its predictions\\nin light of what comes later will be seriously handicapped in planning. This\\nobservation is motivating a fair amount of current work on ways to incorporate\\nsearch. LeCun has suggested adding a dynamic programming component to\\nsearch through multiword predictions, as part of his “path towards autonomous\\nmachine intelligence” . Another proposal, the “tree of thoughts” model ,\\nworks with a search tree of LLM responses. A system which uses hierarchical\\nplanning for mathematical theorem proving was developed in .\\n The next capability on my list, making and working with confidence judg-\\nments, has to do with the well known “hallucination” problem, that LLMs often\\nsimply invent statements, including untrue facts and imaginary citations. While\\nadvantageous for a poetry generator, and bearable for a system which makes\\nsuggestions which an expert human user will verify, this is a huge obstacle to\\nmany practical applications. Thus it is the subject of a great deal of research\\n– a few of this month’s papers are . Perhaps by the time you read\\nthese words there will have already been major progress.\\n Why are LLMs producing these hallucinations? One intuition is that they\\nare doing some sort of compression, analogous to JPEG image compression,\\nwhich introduces errors . This point of view suggests that the problem will\\neventually be solved with larger models and perhaps better training protocols\\nwhich focus on the more informative data items .\\n A related intuition is that the problems follow from inability to properly\\ngeneralize. This comes back to the point about “world models” – a correct\\nmodel, for example an internal encoding of place information, by definition\\ncorrectly treats the properties being modeled. Now suppose we grant that the\\nLLM is solving some class of problems, not by constructing such a model, but by\\nrule-based reasoning. In other words, the LLM somehow learns rules from the\\ncorpus which it uses to make particular inferences which agree with the model.\\n While such rules can cover any number of cases, there is no clear reason for such\\na rule set to ever cover all cases.\\n Another intuition is that the training data contains errors and this is reflected\\nin the results. Certainly the internet is not known for being a completely reliable\\nsource of truth. This intuition also fits with the observation that adding code\\n29(computer programs) to the training set improves natural language reasoning.\\n Code is a good source of rules because almost all of it has been debugged, leading\\nto rules which are correct in their original context (of course they might not be\\ncorrectly applied). It is a longstanding question whether internal representations\\n(both in AI and in humans) are shared between different natural languages; it\\nwould be truly fascinating to know how much they are also shared with code.\\n If this intuition is right, then LLMs reasoning capability might be improved by\\ntraining on far more code and other content which is guaranteed to be correct.\\n Such content could be generated synthetically as tautologies, or even better as\\nformal verified mathematics (as proposed in ).\\n Here is a different point of view: the problem is not that the systems make\\nthings up, after all creativity has value. Rather, it is that they do not provide\\nmuch indication about the confidence to place in a particular output, and do\\nnot have ways to adapt their reasoning to statements known at different levels of\\nconfidence. Much of our reasoning involves uncertain claims and claims which\\nturn out to be false, the point is to distinguish these from justified claims and\\nkeep track of our confidence in each belief. While it is possible to extract\\nconfidence scores from LLMs , there is also a philosophical point to make\\nhere: not all facts have the same epistemological status. Some facts are grounded\\nin evidence; others are true by definition.\\n LLMs are of course statistical models. Even for a completely deterministic\\ntask, say doing arithmetic, a statistical approach to learning is very powerful.\\n This is because learning based on inputs which consist of finitely many training\\nexamples, given in a random order, is naturally formulated in statistical terms.\\n But without making additional non-statistical assumptions, one can never go\\nfrom almost 100% confidence to 100% confidence.\\n This difference is crucial in many aspects of human thought. Of course,\\nlogical reasoning and mathematics stand out as prime examples. Long chains\\nof reasoning are only possible if the individual links are reliable. But it is also\\ncrucial in social reasoning. There is an essential difference between statistical\\nand evidence-based statements, say “Michael is a popular name,” and tauto-\\nlogical, definitional and descriptive statements such as “My name is Michael.”\\n While the first statement might be a subject of discussion, a model which can\\nget confused about the second statement is clearly missing a defining aspect of\\nhuman thought, and will lose the confidence of its interlocutor. Perhaps episte-\\nmological status and tautological correctness need to be somehow represented\\nin the model. It need not be designed in, but the model needs to be given\\nadditional signals beyond next word prediction to learn it.\\n The third point on my list, reflection, does not seem to be much discussed,\\nbut to me seems just as important. In computer science, reflection is the ca-\\npability of a system to work with its programs as a form of data .\\n This is naturally possible for a computer programmed in assembly language, in\\nwhich instructions are encoded in integers. To some extent it is also possible\\nin Lisp, in which programs are encoded in a universal list data structure. As\\ntype systems and other programming language refinements are introduced, re-\\nflection becomes more difficult to provide, but it is necessary for systems-level\\n30programming and makes various standard tasks easier to implement.\\n Since an LLM operates on language, reflection for an LLM is the ability to\\nwork with its internal model in linguistic terms. This is related to ML inter-\\npretability, the ability to translate a model into understandable terms. In §7 we\\ndiscussed interpretability of LLMs in terms of circuits and computational mod-\\nels, implicitly leaving these for a human to interpret and understand. One can\\nimagine an “interpretation engine” which given a model, automatically produces\\na more interpretable description, in terms of circuits, rules, or even a description\\nof the model’s functioning in natural language. Given such an interpretation\\nengine, by applying it to an LLM and sending its output as an input to the\\nLLM, we can implement a form of reflection.\\n A basic human capability which corresponds to this process is the translation\\nfrom procedural or other implicit forms of memory to linguistic, explicit mem-\\nory. Very often, we learn by doing – riding a bicycle, solving math problems,\\ninteracting socially. We then reflect on what we have learned – in some uncon-\\nscious way – and occasionally come up with verbal observations, summaries, in\\na word reflections. It is fascinating that combining the ideas we discussed brings\\nus into contact with such topics.\\n To conclude, and for what it is worth, out of the forty years I have followed\\nAI, this is by far the most exciting period. I agree with those who think LLMs\\nare a major milestone and believe the ideas behind them – including the trans-\\nformer architecture – will remain important even in the light of future progress.\\n The questions they raise are interesting and important enough that – even as\\nthe specialists make remarkable progress – we need not leave the field to them,\\nbut as scientists and thinkers we should engage and try to contribute.\\n A Grammars and parsing\\nMost readers will have encountered the idea of “sentence diagram,” which graph-\\nically represents the decomposition of a sentence into clauses with a subject,\\nverb and object, the assignment of adjectives and prepositional phrases to the\\nnouns and verbs they modify, and so on. Formal versions of this concept are\\nfoundational in linguistics and computer science, and a short introduction (or\\nreview) is a good way to bring the general ideas we are discussing to life.\\n A formal grammar can be given by a set of “production rules” which can\\nbe used to generate grammatical strings. A simple example is in Figure 3.\\n Each line is a rule, which is made up of two strings of symbols separated by\\n→, the left hand side or lhs and right hand side or rhs. These symbols can be\\n“terminals” which appear in the language (such as +, ∗,x, 0 and so on in our\\nexample) or “nonterminals” which do not (such as TERM).\\n These rules are used as follows: We start with a string Scontaining a dis-\\ntinguished “start” symbol (here EXPR). We then iterate the following process:\\nchoose a rule whose lhs occurs in S, and apply it by substituting one occurrence\\nof this lhs in Swith the rhs. Every string Swhich can be obtained by a finite\\nsequence of these operations is considered grammatical, and by keeping track\\n31EXPR →TERM + EXPR (17)\\nEXPR →( EXPR )\\n EXPR →TERM\\nTERM →VALUE ∗TERM\\nTERM →( EXPR )\\n TERM →VALUE\\nVALUE →x\\nVALUE →y\\nVALUE →1\\nFigure 3: A grammar for arithmetic expressions.\\nof the rule applications we get a parse tree. This is a graph whose nodes are\\nsymbols and whose edges connect a symbol which appears on the lhs of a rule\\napplication with the nodes for each of the symbols which appear on the rhs.42\\nA good exercise is to work out the parse tree for the expression y+ 1∗xand\\ncheck that multiplication takes precedence over addition.\\n Our example of a grammar is a context-free grammar, meaning that the\\nleft hand side of each rule consists of a single symbol. If we do not put this\\nrestriction, the resulting class of languages are universal computers (and thus\\nsuffer from potential undecidability). There is also a more restricted class of\\ngrammars called regular grammars (this hierarchy was found by Chomsky), but\\nthese cannot describe nested structures such as the parentheses of Eq. 17. The\\ncontext-free grammars are the right degree of complexity for many purposes. In\\nparticular, programming languages and the formal languages of mathematical\\nlogic can be described using CFG’s and thus the algorithms for working with\\nthem and associated theory are well developed.\\n Besides recognizing and parsing languages, one can describe other linguistic\\ntasks in similar terms. A trivial example would be word replacement, with\\nrules such as OLD i→NEW i. Realistic tasks benefit from frameworks with\\nmore structure. For example, to use the grammar in Eq. 17 to do arithmetic,\\nwe would be much better off with a framework in which the token VALUE\\ncarries an associated numerical or symbolic value. This can be done with the\\nframework of attribute grammars. When we suggest in §8 that LLMs perform\\nnatural language tasks using systems of large numbers of rules, we have this\\nsort of extended grammatical framework in mind.\\n CFG’s are not really adequate for natural languages, with their inherent\\nambiguity and their many special cases and exceptions. A more general for-\\nmalism is the probabilistic CFG. This is obtained by associating a probability\\ndistribution to each symbol which appears on the left hand side of a rule (the\\nnonterminals). For example, we might stipulate that a VALUE has a 75% chance\\n42One can see examples for English sentences in the Wikipedia article “Parse tree.”\\n 32to be a number and a 25% chance to be a variable. With this information, a\\nPCFG defines a probability distribution on strings, which gives zero probability\\nto nongrammatical strings.\\n A symbolic approach to parsing would propose two primary algorithms. One\\nis a parser, which given a grammar and an input produces the parse tree. An-\\nother would be an algorithm for learning a grammar from a corpus. Since any\\nfinite corpus can be described by many grammars, PCFG’s are better suited\\nthan CFG’s to this problem. In any case the learning and parsing algorithms\\nare not necessarily related.\\n In the connectionist approach followed by LLMs, these two algorithms are\\nsubsumed into the definition of a model which can parse any PCFG whose rules\\nare encoded in its weights. By training this on a corpus, the model learns a\\nparticular PCFG which generates the corpus. Interpretability as discussed in\\n§7 then means reversing this relation, by extracting a parser and PCFG from\\nthe trained model.\\n References\\n Reproduced under the cc by 4.0 license. https://creativecommons.org/\\nlicenses/by/4.0/ .\\n  Ekin Aky¨ urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny\\nZhou. What learning algorithm is in-context learning? Investigations\\nwith linear models, November 2022. arXiv:2211.15661 . URL: http:\\n//arxiv.org/abs/2211.15661 ,doi:10.48550/arXiv.2211.15661 .\\n Marie Amalric and Stanislas Dehaene. A distinct cortical network for\\nmathematical knowledge in the human brain. NeuroImage , 189:19–31,\\nApril 2019. URL: https://www.sciencedirect.com/science/article/\\npii/S1053811919300011 ,doi:10.1016/j.neuroimage.2019.01.001 .\\n Sanjeev Arora and Boaz Barak. Computational complexity: a modern\\napproach . Cambridge University Press, 2009.\\n Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Ris-\\nteski. A Latent Variable Model Approach to PMI-based Word Embed-\\ndings. arXiv:1502.03520  , June 2019. arXiv: 1502.03520. URL:\\nhttp://arxiv.org/abs/1502.03520 .\\n  Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W.\\nAyers, Dragomir Radev, and Jeremy Avigad. ProofNet: Autoformalizing\\nand Formally Proving Undergraduate-Level Mathematics, February 2023.\\narXiv:2302.12433 . URL: http://arxiv.org/abs/2302.12433 ,doi:\\n10.48550/arXiv.2302.12433 .\\n  Sebastian Bader and Pascal Hitzler. Dimensions of Neural-symbolic In-\\ntegration - A Structured Survey, November 2005. arXiv:cs/0511042.\\n33URL: http://arxiv.org/abs/cs/0511042 ,doi:10.48550/arXiv.cs/\\n0511042 .\\n  Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh\\nSharma. Explaining Neural Scaling Laws. February 2021. URL: https:\\n//arxiv.org/abs/2102.06701v1 .\\n  Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran\\nMalach, and Cyril Zhang. Hidden Progress in Deep Learning: SGD Learns\\nParities Near the Computational Limit, July 2022. arXiv:2207.08799 . URL: http://arxiv.org/abs/2207.08799 ,doi:10.48550/\\narXiv.2207.08799 .\\n  Peter L. Bartlett, Philip M. Long, G´ abor Lugosi, and Alexander Tsigler.\\n Benign overfitting in linear regression. Proceedings of the National\\nAcademy of Sciences , 117(48):30063–30070, 2020. Publisher: National\\nAcad Sciences.\\n  Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learn-\\ning: a statistical viewpoint. arXiv:2103.09177  , March\\n2021. arXiv: 2103.09177. URL: http://arxiv.org/abs/2103.09177 .\\n  Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and\\nadvances. Computational Linguistics , 48:207–219, 2021. URL: http:\\n//arxiv.org/abs/2102.12452 .\\n  Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of\\ndeep learning through the prism of interpolation. arXiv:2105.14368  , May 2021. arXiv: 2105.14368. URL: http://arxiv.org/\\nabs/2105.14368 .\\n  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Man-\\ndal. Reconciling modern machine-learning practice and the classical\\nbias–variance trade-off. Proceedings of the National Academy of Sci-\\nences , 116(32):15849–15854, 2019. Publisher: National Academy of\\nSciences eprint: https://www.pnas.org/content/116/32/15849.full.pdf.\\n URL: https://www.pnas.org/content/116/32/15849 ,doi:10.1073/\\npnas.1903070116 .\\n  Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep\\nlearning we need to understand kernel learning. February 2018. URL:\\nhttps://arxiv.org/abs/1802.01396v3 .\\n  Emily M. Bender and Alexander Koller. Climbing towards NLU: On mean-\\ning, form, and understanding in the age of data. In Proceedings of the\\n58th Annual Meeting of the Association for Computational Linguistics ,\\npages 5185–5198, Online, July 2020. Association for Computational Lin-\\nguistics. URL: https://aclanthology.org/2020.acl-main.463 ,doi:\\n10.18653/v1/2020.acl-main.463 .\\n 34 Yoshua Bengio. From system 1 deep learning to system 2 deep\\nlearning, December 2019. URL: https://slideslive.com/38922304/\\nfrom-system-1-deep-learning-to-system-2-deep-learning .\\n  Yoshua Bengio, R´ ejean Ducharme, and Pascal Vincent. A neural proba-\\nbilistic language model. Advances in neural information processing sys-\\ntems, 13, 2000.\\n  Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and\\nmachine learning , volume 4. Springer, 2006.\\n  Tai-Danae Bradley, John Terilla, and Yiannis Vlassopoulos. An enriched\\ncategory theory of language: from syntax to semantics. arXiv:2106.07890\\n , June 2021. arXiv: 2106.07890. URL: http://arxiv.org/abs/\\n2106.07890 .\\n  Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Robert L\\nMercer, et al. The mathematics of statistical machine translation: Pa-\\nrameter estimation. 1993.\\n  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. Language Models are Few-Shot Learners. arXiv:2005.14165\\n, June 2020. arXiv: 2005.14165. URL: http://arxiv.org/abs/2005.\\n14165 .\\n  Cameron B. Browne, Edward Powley, Daniel Whitehouse, Simon M. Lu-\\ncas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez,\\nSpyridon Samothrakis, and Simon Colton. A survey of monte carlo tree\\nsearch methods. IEEE Transactions on Computational Intelligence and\\nAI in games , 4(1):1–43, 2012.\\n  S´ ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes\\nGehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi\\nLi, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\\nand Yi Zhang. Sparks of Artificial General Intelligence: Early experi-\\nments with GPT-4, March 2023. URL: https://arxiv.org/abs/2303.\\n12712v1 .\\n  Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering\\nLatent Knowledge in Language Models Without Supervision, December\\n2022. arXiv:2212.03827 . URL: http://arxiv.org/abs/2212.03827 .\\n 35 Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin\\nKnight. Recurrent Neural Networks as Weighted Language Recognizers,\\nMarch 2018. arXiv:1711.05408 . URL: http://arxiv.org/abs/1711.\\n05408 ,doi:10.48550/arXiv.1711.05408 .\\n  Ethan A. Chi, John Hewitt, and Christopher D. Manning. Finding Uni-\\nversal Grammatical Relations in Multilingual BERT. arXiv:2005.04511\\n, May 2020. arXiv: 2005.04511. URL: http://arxiv.org/abs/2005.\\n04511 .\\n David Chiang, Peter Cholak, and Anand Pillay. Tighter Bounds on\\nthe Expressivity of Transformer Encoders, May 2023. arXiv:2301.10743\\n. URL: http://arxiv.org/abs/2301.10743 ,doi:10.48550/arXiv.\\n 2301.10743 .\\n Ted Chiang. Chatgpt is a blurry jpeg of the web. The New Yorker ,\\nFebruary 2023.\\n  Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating\\nLong Sequences with Sparse Transformers. April 2019. URL: https:\\n//arxiv.org/abs/1904.10509v1 .\\n  Anna Choromanska, Mikael Henaff, Michael Mathieu, G´ erard Ben Arous,\\nand Yann LeCun. The Loss Surfaces of Multilayer Networks, January\\n2015. arXiv:1412.0233 . URL: http://arxiv.org/abs/1412.0233 ,\\ndoi:10.48550/arXiv.1412.0233 .\\n  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. PaLM: Scal-\\ning Language Modeling with Pathways. arXiv:2204.02311  , April 2022.\\n arXiv: 2204.02311. URL: http://arxiv.org/abs/2204.02311 .\\n  Bilal Chughtai, Lawrence Chan, and Neel Nanda. A Toy Model of Uni-\\nversality: Reverse Engineering How Networks Learn Group Operations,\\nMay 2023. arXiv:2302.03025 . URL: http://arxiv.org/abs/\\n2302.03025 ,doi:10.48550/arXiv.2302.03025 .\\n  Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical\\nFoundations for a Compositional Distributional Model of Meaning, March\\n2010. arXiv:1003.4394 . URL: http://arxiv.org/abs/1003.\\n4394 ,doi:10.48550/arXiv.1003.4394 .\\n  Taco Cohen and Max Welling. Group equivariant convolutional net-\\nworks. In International conference on machine learning , pages 2990–2999.\\nPMLR, 2016. arXiv:1602.07576.\\n  R´ emi Coulom. Efficient selectivity and backup operators in monte-carlo\\ntree search. In International conference on computers and games , pages\\n72–83. Springer, 2006.\\n 36 Francis Crick. The recent excitement about neural networks. Nature ,\\n337:129–132, 1989.\\n  George V. Cybenko. Approximation by superpositions of a sigmoidal\\nfunction. Mathematics of Control, Signals and Systems , 2:303–314, 1989.\\n  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBERT: Pre-training of Deep Bidirectional Transformers for Language Un-\\nderstanding. October 2018. arXiv: 1810.04805. URL: https://arxiv.\\norg/abs/1810.04805v1 .\\n  Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural Network\\nApproximation. arXiv:2012.14501  , December 2020. arXiv:\\n2012.14501. URL: http://arxiv.org/abs/2012.14501 .\\n  Benjamin L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.\\n Inductive Biases and Variable Creation in Self-Attention Mechanisms.\\n arXiv:2110.10090  , October 2021. arXiv: 2110.10090. URL:\\nhttp://arxiv.org/abs/2110.10090 .\\n  Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom\\nHenighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn\\nDrain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario\\nAmodei, Martin Wattenberg, and Christopher Olah. Toy Models of\\nSuperposition, September 2022. arXiv:2209.10652 . URL: http:\\n//arxiv.org/abs/2209.10652 ,doi:10.48550/arXiv.2209.10652 .\\n  Philip Feldman, James R. Foulds, and Shimei Pan. Trapping LLM Hal-\\nlucinations Using Tagged Context Prompts, June 2023. arXiv:2306.06085\\n. URL: http://arxiv.org/abs/2306.06085 ,doi:10.48550/arXiv.\\n2306.06085 .\\n  John Rupert Firth. Studies in linguistic analysis . Wiley-Blackwell, 1957.\\n  Jerry A Fodor. The modularity of mind . MIT press, 1983.\\n  Dan Friedman, Alexander Wettig, and Danqi Chen. Learning Transformer\\nPrograms, June 2023. arXiv:2306.01128 . URL: http://arxiv.org/\\nabs/2306.01128 ,doi:10.48550/arXiv.2306.01128 .\\n  Artur d’Avila Garcez and Luis C. Lamb. Neurosymbolic AI: The 3rd\\nWave, December 2020. arXiv:2012.05876 . URL: http://arxiv.org/\\nabs/2012.05876 .\\n  Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What\\nCan Transformers Learn In-Context? A Case Study of Simple Function\\nClasses, January 2023. arXiv:2208.01066 . URL: http://arxiv.org/\\nabs/2208.01066 ,doi:10.48550/arXiv.2208.01066 .\\n37 Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning . MIT\\npress, 2016.\\n  Anirudh Goyal and Yoshua Bengio. Inductive Biases for Deep Learn-\\ning of Higher-Level Cognition, August 2022. arXiv:2011.15091 . URL: http://arxiv.org/abs/2011.15091 ,doi:10.48550/arXiv.\\n 2011.15091 .\\n  Andrey Gromov. Grokking modular arithmetic, January 2023.\\narXiv:2301.02679 . URL: http://arxiv.org/abs/2301.\\n02679 ,doi:10.48550/arXiv.2301.02679 .\\n  Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini,\\nFosca Giannotti, and Dino Pedreschi. A Survey of Methods for Explain-\\ning Black Box Models. ACM Computing Surveys , 51(5):1–42, September\\n2019. URL: https://dl.acm.org/doi/10.1145/3236009 ,doi:10.1145/\\n3236009 .\\n  Thilo Hagendorff. Machine Psychology: Investigating Emergent Capabil-\\nities and Behavior in Large Language Models Using Psychological Meth-\\nods, April 2023. arXiv:2303.13988 . URL: http://arxiv.org/abs/\\n2303.13988 ,doi:10.48550/arXiv.2303.13988 .\\n  Michael Hahn and Navin Goyal. A Theory of Emergent In-Context\\nLearning as Implicit Structure Induction, March 2023. arXiv:2303.07971\\n. URL: http://arxiv.org/abs/2303.07971 ,doi:10.48550/arXiv.\\n2303.07971 .\\n  Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCan-\\ndlish. Scaling Laws for Transfer, February 2021. arXiv:2102.01293\\n. URL: http://arxiv.org/abs/2102.01293 ,doi:10.48550/arXiv.\\n2102.01293 .\\n  John Hewitt and Christopher D Manning. A Structural Probe for Finding\\nSyntax in Word Representations. page 10, 2019.\\n  Sepp Hochreiter and J¨ urgen Schmidhuber. Long short-term memory. Neu-\\nral computation , 9(8):1735–1780, 1997.\\n  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne\\nHendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\\nVinyals, and Laurent Sifre. Training Compute-Optimal Large Language\\nModels, March 2022. arXiv:2203.15556 . URL: http://arxiv.org/\\nabs/2203.15556 ,doi:10.48550/arXiv.2203.15556 .\\n 38 Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik\\nStrobelt, Duen Horng Chau, Mohammed J. Zaki, and Dmitry Krotov.\\n Energy Transformer, February 2023. arXiv:2302.07253 . URL: http://arxiv.org/abs/2302.07253 ,doi:10.48550/arXiv.\\n2302.07253 .\\n  Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated\\nthe world chess champion . Princeton University Press, 2002.\\n  Myeongjun Jang and Thomas Lukasiewicz. Consistency Analysis of Chat-\\nGPT, March 2023. arXiv:2303.06273 . URL: http://arxiv.org/abs/\\n2303.06273 ,doi:10.48550/arXiv.2303.06273 .\\n  Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu,\\nMateja Jamnik, Timoth´ ee Lacroix, Yuhuai Wu, and Guillaume Lample.\\n Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal\\nProofs, November 2022. arXiv:2210.12283 . URL: http://arxiv.org/\\nabs/2210.12283 ,doi:10.48550/arXiv.2210.12283 .\\n  Iain M. Johnstone. High Dimensional Statistical Inference and Ran-\\ndom Matrices, November 2006. URL: https://arxiv.org/abs/math/\\n0611589v1 .\\n  Dan Jurafsky and James H Martin. Speech and language processing, 2009.\\n  Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn\\nDrain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova Das-\\nSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones,\\nNelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman,\\nStanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson\\nKernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson,\\nSam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben\\nMann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language Mod-\\nels (Mostly) Know What They Know, July 2022. arXiv:2207.05221 .\\n URL: http://arxiv.org/abs/2207.05221 .\\n  Daniel Kahneman. Fast and slow thinking. Allen Lane and Penguin\\nBooks, New York , 2011.\\n  Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Ben-\\njamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and\\nDario Amodei. Scaling Laws for Neural Language Models, January 2020.\\n arXiv:2001.08361 . URL: http://arxiv.org/abs/2001.08361 ,\\ndoi:10.48550/arXiv.2001.08361 .\\n  Michael J Kearns and Umesh Vazirani. An introduction to computational\\nlearning theory . MIT press, 1994.\\n  Daphne Koller and Nir Friedman. Probabilistic graphical models: princi-\\nples and techniques . MIT press, 2009.\\n39 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi-\\nfication with deep convolutional neural networks. Communications of the\\nACM , 60(6):84–90, 2017.\\n  Florent Krzakala, Federico Ricci-Tersenghi, Lenka Zdeborova, Eric W\\nTramel, Riccardo Zecchina, and Leticia F Cugliandolo. Statistical Physics,\\nOptimization, Inference, and Message-Passing Algorithms: Lecture Notes\\nof the Les Houches School of Physics: Special Issue, October 2013 . Num-\\nber 2013. Oxford University Press, 2016.\\n  B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level\\nconcept learning through probabilistic program induction. Science ,\\n350(6266):1332–1338, December 2015. URL: https://www.sciencemag.\\norg/lookup/doi/10.1126/science.aab3050 ,doi:10.1126/science.\\naab3050 .\\n  Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J\\nGershman. Building Machines That Learn and Think Like People. April\\n2016. URL: http://arxiv.org/abs/1604.00289 .\\n  Yann LeCun. Popular talks and private discussion, 2015.\\n  Yann LeCun. A path towards autonomous machine intelligence, 2022.\\n URL: https://openreview.net/forum?id=BZ5a1r-kVsf .\\n  Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature ,\\n521:436–444, 2015.\\n  Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk\\nMichalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and\\nVedant Misra. Solving Quantitative Reasoning Problems with Lan-\\nguage Models, June 2022. Number: arXiv:2206.14858 arXiv:2206.14858\\n. URL: http://arxiv.org/abs/2206.14858 ,doi:10.48550/arXiv.\\n 2206.14858 .\\n  Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi´ egas, Hanspeter\\nPfister, and Martin Wattenberg. Emergent World Representations: Ex-\\nploring a Sequence Model Trained on a Synthetic Task, February 2023.\\narXiv:2210.13382 . URL: http://arxiv.org/abs/2210.13382 ,doi:\\n10.48550/arXiv.2210.13382 .\\n  Kenneth Li, Oam Patel, Fernanda Vi´ egas, Hanspeter Pfister, and Mar-\\ntin Wattenberg. Inference-Time Intervention: Eliciting Truthful Answers\\nfrom a Language Model, June 2023. arXiv:2306.03341 . URL: http:\\n//arxiv.org/abs/2306.03341 ,doi:10.48550/arXiv.2306.03341 .\\n  Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara\\nSoylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai\\n40Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan,\\nCe Zhang, Christian Cosgrove, Christopher D. Manning, Christopher\\nR´ e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus,\\nFaisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav\\nSanthanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,\\nNathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Hender-\\nson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\\nGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav\\nChaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and\\nYuta Koreeda. Holistic Evaluation of Language Models, November 2022.\\n arXiv:2211.09110 . URL: http://arxiv.org/abs/2211.09110 ,doi:\\n10.48550/arXiv.2211.09110 .\\n  Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen\\nBaker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and\\nKarl Cobbe. Let’s Verify Step by Step, May 2023. arXiv:2305.20050\\n. URL: http://arxiv.org/abs/2305.20050 ,doi:10.48550/arXiv.\\n2305.20050 .\\n  Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and\\nCyril Zhang. Transformers Learn Shortcuts to Automata, October 2022.\\n arXiv:2210.10749 . URL: http://arxiv.org/abs/2210.10749 .\\n  David JC MacKay. Information theory, inference and learning algorithms .\\n Cambridge university press, 2003.\\n  Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher,\\nJoshua B. Tenenbaum, and Evelina Fedorenko. Dissociating language\\nand thought in large language models: a cognitive perspective, January\\n2023. arXiv:2301.06627 . URL: http://arxiv.org/abs/2301.06627 ,\\ndoi:10.48550/arXiv.2301.06627 .\\n  Alexander Maloney, Daniel A. Roberts, and James Sully. A Solvable\\nModel of Neural Scaling Laws, October 2022. arXiv:2210.16859 . URL: http://arxiv.org/abs/2210.16859 ,doi:10.48550/arXiv.\\n 2210.16859 .\\n  Yuri Manin and Matilde Marcolli. Semantic Spaces. arXiv.org , May 2016.\\n arXiv: 1605.04238v1. URL: http://arxiv.org/abs/1605.04238v1 .\\n  Christopher Manning and Hinrich Schutze. Foundations of statistical nat-\\nural language processing . MIT press, 1999.\\n  Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal,\\nand Omer Levy. Emergent linguistic structure in artificial neural net-\\nworks trained by self-supervision. Proceedings of the National Academy of\\nSciences , 117(48):30046–30054, 2020.\\n 41 Matilde Marcolli, Noam Chomsky, and Robert Berwick. Mathe-\\nmatical Structure of Syntactic Merge, May 2023. arXiv:2305.18278\\n. URL: http://arxiv.org/abs/2305.18278 ,doi:10.48550/\\narXiv.2305.18278 .\\n  Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Build-\\ning a large annotated corpus of english: The penn treebank. 1993.\\n  David Marr. Vision: A computational investigation into the human rep-\\nresentation and processing of visual information . MIT press, 2010.\\n  Jir ˇi Matouˇ sek. Lecture notes on metric embeddings. 2013. URL: https:\\n//kam.mff.cuni.cz/ ~matousek/ba-a4.pdf .\\n  Pamela McCorduck and Cli Cfe. Machines who think: A personal inquiry\\ninto the history and prospects of artificial intelligence . CRC Press, 2004.\\n  William Merrill. On the Linguistic Capacity of Real-Time Counter Au-\\ntomata. arXiv:2004.06866  , April 2020. arXiv: 2004.06866. URL:\\nhttp://arxiv.org/abs/2004.06866 .\\n  William Merrill and Ashish Sabharwal. The Parallelism Tradeoff: Lim-\\nitations of Log-Precision Transformers, April 2023. arXiv:2207.00729\\n. URL: http://arxiv.org/abs/2207.00729 ,doi:10.48550/arXiv.\\n2207.00729 .\\n  William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated Trans-\\nformers are Constant-Depth Threshold Circuits. arXiv:2106.16213  ,\\nApril 2022. arXiv: 2106.16213. URL: http://arxiv.org/abs/2106.\\n16213 .\\n  Marc Mezard and Andrea Montanari. Information, physics, and compu-\\ntation . Oxford University Press, 2009.\\n  Eric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The Quan-\\ntization Model of Neural Scaling, March 2023. arXiv:2303.13506 . URL: http://arxiv.org/abs/2303.13506 ,doi:10.48550/arXiv.\\n2303.13506 .\\n  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient\\nEstimation of Word Representations in Vector Space, September 2013.\\n arXiv:1301.3781 . URL: http://arxiv.org/abs/1301.3781 .\\n  Marvin Minsky. Society of mind . Simon and Schuster, 1988.\\n  David Mumford. Pattern theory: the mathematics of perception. arXiv\\npreprint math/0212400 , 2002.\\n  David Mumford and Agn` es Desolneux. Pattern theory: the stochastic\\nanalysis of real-world signals . CRC Press, 2010.\\n 42 Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Stein-\\nhardt. Progress measures for grokking via mechanistic interpretability,\\nJanuary 2023. arXiv:2301.05217 . URL: http://arxiv.org/abs/\\n2301.05217 ,doi:10.48550/arXiv.2301.05217 .\\n  Allen Newell, John Clifford Shaw, and Herbert A Simon. Empirical explo-\\nrations of the logic theory machine: a case study in heuristic. In Papers\\npresented at the February 26-28, 1957, western joint computer conference:\\nTechniques for reliability , pages 218–230, 1957.\\n  Nils J Nilsson. The quest for artificial intelligence . Cambridge University\\nPress, 2009.\\n  Chris Olah. Mechanistic interpretability, variables, and the importance of\\ninterpretable bases, 2022. URL: https://transformer-circuits.pub/\\n2022/mech-interp-essay/index.html .\\n  Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das-\\nSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna\\nChen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\\nDanny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\\nLovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared\\nKaplan, Sam McCandlish, and Chris Olah. In-context Learning and\\nInduction Heads, September 2022. arXiv:2209.11895 . URL: http:\\n//arxiv.org/abs/2209.11895 ,doi:10.48550/arXiv.2209.11895 .\\n  Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe:\\nGlobal Vectors for Word Representation. In Proceedings of the 2014 Con-\\nference on Empirical Methods in Natural Language Processing (EMNLP) ,\\npages 1532–1543, Doha, Qatar, October 2014. Association for Com-\\nputational Linguistics. URL: https://www.aclweb.org/anthology/\\nD14-1162 ,doi:10.3115/v1/D14-1162 .\\n  Mary Phuong and Marcus Hutter. Formal Algorithms for Transformers,\\nJuly 2022. arXiv:2207.09238 . URL: http://arxiv.org/abs/2207.\\n09238 .\\n  Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant\\nMisra. Grokking: Generalization Beyond Overfitting on Small Algorith-\\nmic Datasets. arXiv:2201.02177  , January 2022. arXiv: 2201.02177.\\nURL: http://arxiv.org/abs/2201.02177 .\\n  Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith,\\nand Mike Lewis. Measuring and Narrowing the Compositionality Gap\\nin Language Models, October 2022. arXiv:2210.03350 . URL: http:\\n//arxiv.org/abs/2210.03350 ,doi:10.48550/arXiv.2210.03350 .\\n  Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\\n Improving language understanding by generative pre-training. 2018. Pub-\\nlisher: OpenAI.\\n43 Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and\\nIlya Sutskever. Language Models are Unsupervised Multitask Learners.\\nundefined , 2019. URL: https://www.semanticscholar.org/paper/\\nLanguage-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/\\n9405cc0d6169988371b2755e573cc28650d14dfe .\\n  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Explor-\\ning the Limits of Transfer Learning with a Unified Text-to-Text Trans-\\nformer. arXiv:1910.10683  , July 2020. arXiv: 1910.10683. URL:\\nhttp://arxiv.org/abs/1910.10683 .\\n  Hubert Ramsauer, Bernhard Sch¨ afl, Johannes Lehner, Philipp Seidl,\\nMichael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner,\\nMilena Pavlovi´ c, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael\\nKopp, G¨ unter Klambauer, Johannes Brandstetter, and Sepp Hochreiter.\\n Hopfield Networks is All You Need, April 2021. arXiv:2008.02217 . URL: http://arxiv.org/abs/2008.02217 ,doi:10.48550/arXiv.\\n2008.02217 .\\n  Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep\\nLearning Theory. arXiv:2106.10165  , August 2021. arXiv:\\n2106.10165. URL: http://arxiv.org/abs/2106.10165 .\\n Frank Rosenblatt. The perceptron: a probabilistic model for information\\nstorage and organization in the brain. Psychological review , 65(6):386,\\n1958.\\n  David E. Rumelhart, Geoffrey E. Hinton, and James L. McClelland. A\\ngeneral framework for parallel distributed processing. 1986.\\n  Stuart J Russell. Artificial intelligence a modern approach . Pearson Ed-\\nucation, Inc., 2010.\\n  Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo. Are emergent\\nabilities of large language models a mirage? ArXiv , abs/2304.15004, 2023.\\n  Terrence J Sejnowski. The deep learning revolution . MIT press, 2018.\\n  Claude E Shannon. Xxii. programming a computer for playing chess. The\\nLondon, Edinburgh, and Dublin Philosophical Magazine and Journal of\\nScience , 41(314):256–275, 1950.\\n  Hava T Siegelmann and Eduardo D Sontag. On the computational power\\nof neural nets. In Proceedings of the fifth annual workshop on Computa-\\ntional learning theory , pages 440–449, 1992.\\n  Brian Cantwell Smith. Procedural reflection in programming languages\\nvolume i. 1982.\\n 44 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya\\nGanguli. Deep unsupervised learning using nonequilibrium thermodynam-\\nics. In International Conference on Machine Learning , pages 2256–2265.\\nPMLR, 2015. arXiv:1503.03585.\\n  Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and\\nAri S. Morcos. Beyond neural scaling laws: beating power law scaling via\\ndata pruning, June 2022. Number: arXiv:2206.14486 arXiv:2206.14486\\n. URL: http://arxiv.org/abs/2206.14486 ,doi:10.48550/\\narXiv.2206.14486 .\\n  Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. Beyond the\\nImitation Game: Quantifying and extrapolating the capabilities of lan-\\nguage models. Technical Report arXiv:2206.04615, arXiv, June 2022.\\narXiv:2206.04615  type: article. URL: http://arxiv.org/abs/\\n2206.04615 .\\n  Richard Sutton. The bitter lesson, 2019. URL: http://www.\\nincompleteideas.net/IncIdeas/BitterLesson.html .\\n  Christian Szegedy. A promising path towards autoformalization and gen-\\neral artificial intelligence. In International Conference on Intelligent Com-\\nputer Mathematics , 2020.\\n  Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gim-\\npel. Chess as a Testbed for Language Model State Tracking, May\\n2022. arXiv:2102.13249 . URL: http://arxiv.org/abs/2102.13249 ,\\ndoi:10.48550/arXiv.2102.13249 .\\n  Richard E. Turner. An Introduction to Transformers, July 2023.\\narXiv:2304.10557 . URL: http://arxiv.org/abs/2304.10557 ,doi:\\n10.48550/arXiv.2304.10557 .\\n  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Atten-\\ntion Is All You Need. June 2017. arXiv: 1706.03762. URL: https:\\n//arxiv.org/abs/1706.03762 .\\n  Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas-\\ntian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald\\nMetzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang,\\nJeff Dean, and William Fedus. Emergent Abilities of Large Language\\nModels. 2022. Publisher: arXiv Version Number: 2. URL: https:\\n//arxiv.org/abs/2206.07682 ,doi:10.48550/ARXIV.2206.07682 .\\n  Gail Weiss, Yoav Goldberg, and Eran Yahav. On the Practical Compu-\\ntational Power of Finite Precision RNNs for Language Recognition, May\\n2018. arXiv:1805.04908 . URL: http://arxiv.org/abs/1805.\\n04908 ,doi:10.48550/arXiv.1805.04908 .\\n 45 Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin\\nChoi, and Kyunghyun Cho. NaturalProofs: Mathematical Theorem Prov-\\ning in Natural Language. page 14, 2021.\\n  Noam Wies, Yoav Levine, and Amnon Shashua. The Learnability of In-\\nContext Learning, March 2023. arXiv:2303.07895 . URL: http://\\narxiv.org/abs/2303.07895 ,doi:10.48550/arXiv.2303.07895 .\\n  Avi Wigderson. Mathematics and computation: A theory revolutionizing\\ntechnology and science . Princeton University Press, 2019.\\n  Wikipedia. URL: https://en.wikipedia.org/wiki/Reflective_\\nprogramming .\\n  Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An\\nExplanation of In-context Learning as Implicit Bayesian Inference, July\\n2022. arXiv:2111.02080 . URL: http://arxiv.org/abs/2111.02080 ,\\ndoi:10.48550/arXiv.2111.02080 .\\n  Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong\\nLiu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jian-\\nfeng Gao. Tensor Programs V: Tuning Large Neural Networks via Zero-\\nShot Hyperparameter Transfer, March 2022. arXiv:2203.03466 . URL: http://arxiv.org/abs/2203.03466 ,doi:10.48550/arXiv.\\n 2203.03466 .\\n  Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,\\nYuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Prob-\\nlem Solving with Large Language Models, May 2023. arXiv:2305.10601\\n. URL: http://arxiv.org/abs/2305.10601 ,doi:10.48550/arXiv.\\n2305.10601 .\\n  Yuhui Zhang, Michihiro Yasunaga, Zhengping Zhou, Jeff Z. HaoChen,\\nJames Zou, Percy Liang, and Serena Yeung. Beyond Positive Scaling:\\nHow Negation Impacts Scaling Trends of Language Models, May 2023.\\narXiv:2305.17311 . URL: http://arxiv.org/abs/2305.17311 ,doi:\\n10.48550/arXiv.2305.17311 .\\n  Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do\\nTransformers Parse while Predicting the Masked Word?, March 2023.\\n arXiv:2303.08117 . URL: http://arxiv.org/abs/2303.08117 ,doi:\\n10.48550/arXiv.2303.08117 .\\n  Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F: a cross-\\nsystem benchmark for formal Olympiad-level mathematics, February\\n2022. arXiv:2109.00110 . URL: http://arxiv.org/abs/2109.00110 ,\\ndoi:10.48550/arXiv.2109.00110 .\\n46\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "def clean_research_paper(paper_text):\n",
    "  \"\"\"Cleans a scientific research paper for text summarization.\n",
    "\n",
    "  Args:\n",
    "    paper_text: The text of the research paper.\n",
    "\n",
    "  Returns:\n",
    "    The cleaned text of the research paper.\n",
    "  \"\"\"\n",
    "\n",
    "  # Remove all formatting.\n",
    "  paper_text = re.sub(r\"\\[[^\\]]+\\]\", \"\", paper_text)\n",
    "  paper_text = re.sub(r\"<.*?>\", \"\", paper_text)\n",
    "\n",
    "  # Tokenize the text.\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  doc = nlp(paper_text)\n",
    "  tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "  # Remove stop words.\n",
    "  stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "  tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "  # Remove punctuation.\n",
    "  tokens = [token for token in tokens if not token in string.punctuation]\n",
    "\n",
    "  # Calculate the frequency of each word.\n",
    "  word_counts = nltk.FreqDist(tokens)\n",
    "\n",
    "  # Identify the key sentences.\n",
    "  key_sentences = []\n",
    "  for sentence in doc.sents:\n",
    "    sentence_tokens = [token.lemma_ for token in sentence]\n",
    "    sentence_score = 0\n",
    "    for token in sentence_tokens:\n",
    "      sentence_score += word_counts[token]\n",
    "    if sentence_score > 0:\n",
    "      key_sentences.append(sentence)\n",
    "\n",
    "  # Generate the summary.\n",
    "  summary = \" \".join([str(sentence) for sentence in key_sentences])\n",
    "\n",
    "  return summary\n",
    "\n",
    "clean_research_paper(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpreprocess_and_summarize_research_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36mpreprocess_and_summarize_research_paper\u001b[1;34m(paper_text)\u001b[0m\n\u001b[0;32m     46\u001b[0m sentence_scores \u001b[38;5;241m=\u001b[39m sentence_vectors\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m key_sentence_indices \u001b[38;5;241m=\u001b[39m sentence_scores\u001b[38;5;241m.\u001b[39margsort(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 48\u001b[0m key_sentences \u001b[38;5;241m=\u001b[39m [cleaned_sentences[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m key_sentence_indices]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key_sentences, extracted_entities\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m sentence_scores \u001b[38;5;241m=\u001b[39m sentence_vectors\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m key_sentence_indices \u001b[38;5;241m=\u001b[39m sentence_scores\u001b[38;5;241m.\u001b[39margsort(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 48\u001b[0m key_sentences \u001b[38;5;241m=\u001b[39m [\u001b[43mcleaned_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m key_sentence_indices]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key_sentences, extracted_entities\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "preprocess_and_summarize_research_paper(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLarge Language Models\\nMichael R. Douglas\\nCMSA, Harvard University\\nDept. of Physics, Stony Brook University\\nmdouglas@cmsa.fas.harvard.edu\\nJuly 2023\\nAbstract\\nArtificial intelligence is making spectacular progress, and one of the\\nbest examples is the development of large language models (LLMs) such\\nas OpenAI’s GPT series. In these lectures, written for readers with a\\nbackground in mathematics or physics, we give a brief history and survey\\nof the state of the art, and describe the underlying transformer architec-\\nture in detail. We then explore some current ideas on how LLMs work\\nand how models trained to predict the next word in a text are able to\\nperform other tasks displaying intelligence.\\n 1arXiv:2307.05782v1    11 Jul 20231 Introduction\\nAt the end of November 2022, OpenAI released a system called ChatGPT which\\ninteracts with its users in natural language. It can answer questions, engage in\\ndialogs, translate between languages and write computer code with a fluency\\nand ability far exceeding all previous publically available systems. Although it\\nfalls well short of human abilities in many ways, still the large language model\\ntechnology of which it is an example is widely considered to be a major advance\\nin artificial intelligence.1\\nFew developments in science and technology entered the popular conscious-\\nness as quickly as ChatGPT. There is no mystery about why. The ability to use\\nlanguage is a defining property of humanity, and for the first time a computer\\nis doing this well enough to make a comparison with humans interesting. All\\nof the hopes and fears which have developed around AI, robots and technology\\nmore generally are being brought into the discussion. In my opinion this is\\njustified; the speed of recent progress makes it urgent to better understand AI,\\nto forecast its capabilities and limitations, and to make wise decisions about\\nits development and use. With great opportunities will come great challenges,\\nwhich will concern all of us.\\n In these lecture notes we give an introduction to this subject for mathe-\\nmaticians, physicists, and other scientists and readers who are mathematically\\nknowledgeable but not necessarily expert in machine learning or artificial intel-\\nligence. We begin with a very brief overview of AI in §2 to explain some ideas\\nwe consider to be essential context, the basic principles of the symbolic and\\nconnectionist approaches. In §3 we define statistical language models and relate\\nthe history of transformer-based LLMs up through GPT-4. In §4 we discuss\\nmeasures of what LLMs do and how well they do it. We then give a precise\\nexplanation of simpler language models in §5 and the transformer architecture\\nin§6.\\n It is amazing that a model defined by a few short equations, trained to go\\nthrough a text and simply predict each next word as it appears – a task which\\nseems only loosely related to any definition of intelligence – can do tasks which\\n“obviously” require intelligence, such as solving word problems like the one in\\nFigure 1 below. At present nobody really understands how this works. Even\\nthe interpretation of what LLMs are doing is controversial, ranging from the\\nbelief that they are “simply” rearranging the sentences they were trained on,\\nall the way to the belief that the LLMs are learning sophisticated models of\\nthe world and that “simply” scaling up the computations will produce artificial\\ngeneral intelligence. Any forecast for progress must take into account the current\\nmodels’ shortcomings – lack of long term memory and ability to plan, tendency\\nto make up facts and “hallucinate,” unreliability in logical reasoning, etc.. Do\\nthese problems have technical solutions which will also look simple once we have\\nthem? Or are they more significant barriers?\\nMost current work on LLMs takes an engineering and problem solving per-\\n1A few of the many other milestones in LLM development are .\\n2Question: Assume that the variance of the first nnatural numbers is 10, and\\nthe variance of the first meven natural numbers is 16. Compute m+n.\\n Model output: Letσ2be the variance of the first meven natural numbers,\\nandτ2be the variance of the first nnatural numbers. Then, we have τ2=n2−1\\n12\\nandσ2=m2−1\\n3. We solve the system of equations:\\nn2−1\\n12= 10\\nm2−1\\n3= 16.\\n This translates into the system\\nn2= 121\\nm2= 48 + 1 = 49 .\\n Therefore, n= 11 andm= 7 , son+m= 18 .\\nFigure 1: A question-answer pair solved by Minerva. From Lewkowycz et al\\n, “Solving quantitative reasoning problems with language models,” 2022.\\nspective, but there are many interesting works which focus more on understand-\\ning how LLMs work. One would think this should be far easier than understand-\\ning how human brains work, as we have full knowledge of an LLM’s microscopic\\nworkings and can do a wide variety of experiments on it.2These efforts are in\\ntheir early days, but in §7 we survey current approaches to understanding how\\nLLMs do what they do. We conclude in §8 with more general discussion, some\\nquestions and potentially important developments to watch for.\\n Before we start, let me say a little about my own background. I was trained\\nas a theoretical physicist and most of my contributions to science are in string\\ntheory and its interface with mathematics, but I have followed AI fairly closely\\nsince the 80’s and in detail since 2016. In addition I spent eight years in quan-\\ntitative finance where I gained a good deal of “hands-on” experience with ma-\\nchine learning. I have given many lectures telling computer scientists about\\nphysics and physicists about computational topics, and benefited from conver-\\nsations with many people – more than I can name here, but let me thank\\nGerald Jay Sussman, David McAllester, Yann LeCun, Sanjeev Arora, Surya\\nGanguli, Jeremy Avigad, Vijay Balasubramanian, Dmitri Chklovskii, David\\nDonoho, Steve Skiena, Christian Szegedy, Misha Tsodyks, Tony Wu and the\\n2At least, this was the case before March 2023. Currently the weights and even the de-\\nsign parameters of GPT-4, the most advanced LLM, are held in confidence by OpenAI as\\nproprietary information.\\n 3many speakers in the CMSA New Technologies seminar series,3and Josef Urban\\nand the AITP community.4Thanks to David McAllester and Sergiy Verstyuk\\nfor comments on the first draft. These notes would not have been possible with-\\nout their input and advice, and I hope their signal to noise ratio approaches\\nthat of what they shared with me.\\n 2 Symbolic and connectionist AI\\n The goal of artificial intelligence is to build computational systems which can\\nperform tasks which require intelligence. Although intelligence is hard to define\\nprecisely, an operational definition suitable for LLMs is ability at tasks requiring\\nlanguage, reasoning, and planning, as judged by humans who interact with the\\nsystem. Some famous and difficult “challenge tasks” include playing chess ,\\nproving mathematical theorems , and answering natural language questions\\nusing generally known facts and common sense.5\\nThese problems have been the subject of intense investigation since the mid-\\n50’s, and a few textbooks and histories are . Essentially from\\nthe start, two broad approaches were set out, which would later be called sym-\\nbolic and connectionist AI.6The symbolic approach originated in mathematical\\nlogic and generative linguistic theory, and tracked the development of computer\\ntechnology (both hardware and software) as a tool for solving practical and\\nscientific problems. Central topics in this approach are formal logic and lan-\\nguage theory, search and heuristics, and engineering techniques for designing\\nand building large and complex systems.\\n Symbolic AI systems are designed, meaning that their creators develop a de-\\ntailed understanding of the task, and encode this understanding into the system\\nby programming, by hardware design and otherwise. As an example, consider\\nthe task of parsing: given an input string of words, determine its grammatical\\nstructure. Most of us learned to diagram sentences in elementary school, and\\nalthough linguists have developed far more sophisticated notions of grammar,\\nthis simple notion gives the right idea. The grammar of a language is encoded\\nin rules, which belong to a formal framework – see the appendix for the example\\nof context-free grammars. Given such a framework, one can design a parsing\\nalgorithm which takes as input a rule set and an input string, and produces an\\noutput which states whether the string is a grammatical sentence and if so makes\\nits structure explicit. This is a symbolic AI approach, not because the words\\n“symbolize” anything (after all grammar does not have to come with mean-\\n3https://live-hu-cmsa-222.pantheonsite.io/\\n4http://aitp-conference.org/2023/\\n5This challenge originates with Turing’s famous test, but the restriction to question answer-\\ning makes it better defined and testable using benchmarks, standardized question-answer sets.\\n Discussion of the original test can be found at https://en.wikipedia.org/wiki/Turing test\\n6Symbolic AI is sometimes called “GOFAI” for “good old-fashioned AI.” Related terms\\ninclude “rule based,” “logic based,” “expert system” and “feature engineering.” The con-\\nnectionist approach has many other names, reflecting its mixed ancestry: “neural,” “deep\\nlearning,” “parallel distributed processing,” “differentiable,” and “representation learning.”\\n4ing), but because the grammatical rules and the parsing algorithm (including\\nits internal data structures) have a clear meaning to their designers.\\n Symbolic methods have had considerable success at many tasks requiring\\nintelligence, famously including chess playing  and symbolic algebra7as well\\nas more prosaic but very central tasks such as translating high level computer\\nlanguages to machine code (compiling). And a great deal of work has been\\ndone to broaden their scope, for example to build question answering systems\\nsuch as the well known IBM Watson. Many valuable techniques came out of\\nthis; ways to systematize rules into “knowledge bases” or “knowledge graphs,”\\nmethods for automated logical reasoning, and so on. But it was long ago realized\\nthat once one goes beyond “formal worlds” such as chess and algebra to the\\ncomplex and messy situations of real life, although one can postulate rules which\\ncapture many truths and can be used for reasoning, rules which are valid in all\\ncases are very rare. Furthermore, the sheer number of rules required to cover\\neven the likely possibilities is very large. These difficulties were addressed by\\nimplementing probabilistic reasoning and by getting teams of humans to develop\\nthe requisite enormous rule sets, leading to the “expert system” approach which\\nwas applied (for example) to medical question answering. Cyc,8an early and\\nwell known expert system, is commercially available and has a database of\\ncommonly known facts with over 25 million assertions; however this is dwarfed\\nby knowledge bases such as Wikidata (over one billion “facts”) but which do\\nnot have a systematic reasoning engine. It is clear that any approach which\\ndepends on careful human analysis of such large knowledge bases is impractical.\\n Meanwhile, a very different “connectionist” approach to AI was being cham-\\npioned by other researchers. They drew their inspiration from hypotheses about\\nhow the brain works, from information theory and statistics, from physics and\\nother natural sciences, and from applied mathematics and particularly opti-\\nmization theory. These diverse points of view came together in the 1990’s and\\nled to a great deal of interdisciplinary work,9of which the part most related to\\nAI and which lies behind LLMs is called machine learning (ML).\\n The usual starting point in modern treatments of ML is to rephrase a task\\nas a statistical inference problem based on a large dataset. A canonical example\\nis image recognition – say, given an array of pixels (light intensity values),\\nestimate the probability that the image depicts a cat. Rather than design a\\nsystem to do this, one starts with a large dataset of images with labels (cat and\\nnon-cat). One then designs a very general statistical model and “trains” it on\\nthis dataset to predict the label given the image. This is supervised learning,\\none can also do “self-supervised” learning in which the system predicts some\\npart of the data given other parts (say, filling in part of an image). A third\\nstandard ML paradigm is reinforcement learning, which applies to tasks which\\ninvolve choosing actions to fulfill a longer term goal. The classic example is\\ngame playing, as in AlphaGo and AlphaZero.\\n In any case, since the problem is formulated statistically, it is possible to\\n7https://www.sigsam.org/\\n8https://cyc.com/\\n9I first learned about this from .\\n 5consider the training dataset one item at a time, and use it to incrementally\\nimprove the model. This is almost always done by formulating the task in terms\\nof an “objective function” which measures the quality with which it is performed,\\nfor example the accuracy with which correct labels are assigned to images. One\\nthen takes a parameterized model and trains it by optimizing this function,\\nevaluated on the training dataset, with respect to the model parameters. For\\nthe classic models of statistics this can be done analytically, as in a least squares\\nfit. For more general models one uses numerical methods, such as gradient\\ndescent. Either way, a central question of statistics and machine learning is\\ngeneralization, meaning the extent to which the model well describes data not\\nin the training set but sampled from the same probability distribution. A well\\nknown principle which speaks to this question is “Occam’s razor,” that simpler\\nmodels will generalize better. This is often simplified to the rule that a model\\nshould have the minimal number of parameters needed to fit the dataset.\\n Not all machine learning systems are “deep learning”  or “connection-\\nist” . These terms generally refer to the use of neural networks with large\\nnumbers of parameters which provide effective universal function approxima-\\ntors. While the idea is very old , before 2012 it was widely believed to be\\nimpractical. One argument for this was the “dull side” of Occam’s razor – mod-\\nels with so many parameters were destined to overfit and would not generalize.\\n Evidently this is not the case, leading to concepts such as “benign overfitting.”\\n  Another argument was that the objective functions for these models\\nare highly nonconvex and optimization would get stuck at poor quality local\\nminima. This can be a problem, but turns out to be solvable for reasons that\\nare partially understood . Finally, despite the effectiveness of the trained\\nmodel in performing a task, the large number of parameters often makes it very\\nhard to understand how such a model works, and why a given input produces\\na particular output. This “interpretability problem” remains a key issue with\\ndeep learning models, and is the subject of much research .\\n There are many other variations and hybrid approaches in the story. Another\\nimportant one is the “pattern recognition” approach . This is also based\\non statistical inference but – like the symbolic approach – it emphasizes the value\\nof detailed understanding of the problem domain in designing the system. For\\nexample, one could hand-code the initial layers of an image recognition network\\nto detect lines or other significant “features” of the image. But unlike a purely\\nsymbolic approach, these features would be used as input to a general statistical\\nor neural model.\\n Another concept which illustrates the relation between the two approaches\\nis probabilistic reasoning, the use of rules such as “the chance of rain when it is\\ncloudy is 50%”. One can state and use such rules in a symbolic approach (see\\nfor example ), the essential distinction with connectionism is not the use of\\nprobabilities but rather the representation of knowledge in terms of explicit and\\nmeaningful rules.\\n As we suspect every reader has already heard, the symbolic approach was\\ndominant from the early days until 2012, and (along with many other suc-\\ncesses) led to a superhuman chess player, but seemed inadequate for our other\\n6two challenge tasks (theorem proving and question answering). In 2012 the\\nconnectionist approach surpassed other approaches to computer vision , and\\never since neural systems have gone from triumph to triumph. In 2016 the\\ndeep learning system AlphaZero surpassed the symbolic AI chess players (and\\nof course humans). Over the last few years, transformer models trained on a\\nlarge corpus of natural language to predict each next word as it appears, have\\nrevolutionized the field of natural language processing. As we write this the\\nstate of the art GPT-4 demonstrates truly remarkable performance at question\\nanswering, code generation and many other tasks .\\n The simplest and arguably deepest explanation for this history is that it is\\na consequence of the exponential growth of computational power and training\\ndatasets, which continues to the present day. Given limited computing power\\nand data, the ability of the symbolic and pattern recognition approaches to\\ndirectly incorporate human understanding into a system is a significant advan-\\ntage. On the other hand, given sufficiently large computing power and data,\\nthis advantage is nullified and may even become disadvantageous, as the human\\neffort required to code the system becomes the limiting resource. This point,\\nthat the most significant advances in AI (and computation more generally) have\\ncome from hardware improvements and replacing human engineering with data-\\ndriven methods, is forcefully made by Sutton in his “bitter lesson” essay .\\n In§3,§4 and §8 we will discuss scaling laws and evidence for and against the\\nidea that by continuing along the current path, training ever-larger models on\\never-larger datasets, we will achieve AGI (artificial general intelligence, whatever\\nthat means) and the realms beyond.\\n Up to now the symbolic and connectionist approaches have generally been\\nconsidered to be in tension.10There is another point of view which consid-\\ners them complementary, with a symbolic approach better suited for certain\\nproblems (for example logical reasoning) and connectionist for others (for ex-\\nample image recognition). Given this point of view one can seek a synthesis or\\n“neurosymbolic” approach, advocated in many works .\\n But are they in conflict at all? Another reconciliation is the hypothesis that\\nproblems which in the symbolic approach are solved using rules and algorithms,\\nare also being solved that way by neural systems and in particular by LLMs.\\n However, rather than the algorithms and rules being coded by humans, as the\\nresult of its training procedure the LLM has somehow learned them, encoded\\nin its networks in some as yet mysterious way. This vague hypothesis can be\\nsharpened in many ways, in part by proposing specific mechanisms by which\\nalgorithms and rules are encoded, in part by making general claims about the\\nalgorithms which are being learned. We discuss these ideas in §7 and §8.\\n10To better discuss this point one should refine the symbolic-connectionist dichotomy into\\nmultiple axes: system design versus learning from data; meaningful rules versus uninterpreted\\nmodels; combinatorial versus differentiable optimization; deterministic versus probabilistic.\\n 73 Language models\\nThroughout the history of linguistics, languages have been described in terms\\nof rules: rules of grammar, phonology, morphology, and so on, along with log-\\nical and other frameworks for describing meaning. This remains the case in\\nChomskyan linguistics and in much of theoretical linguistics.\\n By contrast, LLMs are statistical language models, meaning that they encode\\na probability distribution on strings of words, call this P(w1. . . w L), which\\napproximates the distribution realized by a large body (or “corpus”) of text in\\nthe language. The simplest example is the frequency or “1-gram” model defined\\nby taking the words to be independently distributed, so\\nP(w1. . . w L) =LY\\ni=1P(wi);P(w) =number of occurrences of win the corpus\\ntotal number of words in the corpus.\\n (1)\\nOf course, this model captures very little of the structure of language, which\\ninvolves dependencies between the word choices.\\n LLMs are generative models,11by which we will mean that there is a practi-\\ncal method for sampling from the distribution. To explain this, consider a word\\nprediction task in which some words in a string are given (the “input”) and\\nothers left blank (the “output”). Given a probability distribution P(w1. . . w L),\\nthere is a corresponding conditional probability distribution for the output given\\nthe input. As an example, suppose we are given the string “The cat \\noutside,” where “” is a “token” which marks the position of the missing\\nword. The relevant conditional probabilities might be\\nP(the cat went outside |the cat  outside) = 0 .5\\nP(the cat sat outside |the cat  outside) = 0 .2\\nand so on, summing to total probability 1. In the masked word prediction task,\\nthe model must determine (or sample from) this distribution.\\n A particularly convenient case is to give the conditional probability of the\\nword which follows a given string, which we denote as\\nP(wn+1|w1w2. . . w n−1wn). (2)\\nBy sampling this distribution to get a new word wn+1and appending it to the\\nend, the string can be extended one word at a time. Repeating this process\\ngives an arbitrarily long string, which by the laws of probability is a sample\\nfrom the original probability distribution P(w1. . . w L), for example\\nP(the cat went outside) = P(the) P(cat|the)P(went |the cat) P(outside |the cat went) .\\n This factorization of the probability into successive conditional probabilities\\ndefines the class of autoregressive models. One could furthermore require that\\n11Many different definitions of this term can be found in the literature.\\n 8the conditional probability Eq. 2 depends only on the kmost recent words, in\\nwhich case one would have a Markov model whose state is a string of kwords.\\n To evaluate how good a language model is, we want to quantify how well its\\nprobability distribution approximates that of the corpus (the empirical distribu-\\ntion). The standard measure of this is the cross entropy. For an autoregressive\\nmodel this is a sum of terms, one for each word in the corpus,12\\nL=−1\\nNN−nX\\ni=1logP(wi+n|wiwi+1. . . w i+n−1) (3)\\n One also refers to exp −Las “perplexity.” In a machine learning approach, we\\ncan use Eq. 3 as an objective function and minimize it as a function of the\\nnetwork parameters to train the network. We can then apply the many tools of\\nML: backpropagation, splitting the sum into batches, varying the learning rate\\nand so on, to get an efficient and effective model. While the details are an art\\nwhich depends on the particular domain and model architecture,13conceptually\\nthese are much the same for LLMs as for other machine learning models.\\n This statistical approach to modeling language has been pursued since the\\nlate 80’s  and many models were developed, such as the recurrent neu-\\nral network (RNN) which we will describe in §5. Following the general machine\\nlearning experience that supervised tasks (learning from input-output pairs)\\nare easier than unsupervised tasks, many of these works addressed machine\\ntranslation and parsing, for which there are good labeled datasets (documents\\nwith their translations; sentences with their grammatical structure). However\\nunlabeled datasets are much larger and by 2015 or so there was a sense that self-\\nsupervised learning was the next frontier , leading to more focus on masked\\nword prediction.\\n The history of transformer models starts with the 2017 proposal of Vaswani\\net al . Their model was designed for a translation task and was more com-\\nplicated than what we will explain in §6, but the essential idea to use attention\\nand positional encoding to represent all the relations between the words in a\\ntext originated here and is fully present.\\n The transformer architecture was taken up by many groups, and particularly\\ninfluential 2018 works include BERT  and GPT . BERT was trained by\\nmasking arbitrary words in a sentence (not just the next word), which allows\\nthe model to look backward and forward for context and leads to better results.\\n However it is not straightforward to sample from such a model, and eventually\\nthe simpler next word prediction approach followed by GPT won out.\\n Both of these models, and most work of this period, followed the paradigm\\nof pretraining followed by fine tuning. The idea was to first train for word\\nprediction on a very large corpus, to get a general purpose model. This would\\nthen be adapted to specific tasks such as question answering by fine tuning.\\n This means doing a second pass of supervised learning on a much smaller labeled\\n12With the sign, L ≥0 and better models have smaller L. The term “loss function” is often\\nused for an objective function with these properties.\\n 13In CS this term generally refers to the large scale arrangement of components of a system.\\n 9dataset, replacing next word prediction by the objective function for the specific\\ntask. Say we are doing question answering, this could be the accuracy of the\\nanswers. This two step procedure was justified by the notion of transfer learning,\\nmeaning that the capabilities of the general purpose model “transfer” to related\\nbut different tasks. This approach led to SOTA14results on many benchmarks\\nand motivated much further work.\\n Most importantly, a great deal of ingenuity and hard work was put into\\nsolving the engineering problems of training larger and larger models on larger\\nand larger datasets. As for the data, a lot of text is available on the web, with\\none much used archive of this data provided by Common Crawl.15Training can\\nlargely be done in parallel by dividing up this data, and the availability of large\\nclusters of GPU-enabled servers at industrial labs and through cloud computing\\nmeant that sufficient computing resources were available in principle. However,\\nthe overall cost of training scales as (at least) the product of model size and\\ndataset size, and this was becoming expensive. While the precise cost figures\\nfor the GPT series are not public, it is estimated that a single training run\\nof the largest GPT-3 models cost tens of millions of dollars. To motivate and\\nefficiently carry out such costly experiments, one needs some ability to predict in\\nadvance how changes in model and dataset size will affect the training methods\\n(for example the optimal choice of learning rate) and performance.\\n An important advance in this direction was the observation of power law\\nscaling in language model performance . Figure 2 plots the test loss16against\\nthe logarithms of the sizes and compute resources used, and these straight lines\\ncorrespond to a power law relation between size and perplexity. This scaling\\nholds over many decades in model size and, while the exponents α∼ −0.076 to\\n−0.095 are rather small, this is a strong argument that larger models will have\\nbetter performance. These ideas were also used to determine optimal model-\\ndataset size tradeoff  and the scaling of hyperparameters . These results\\nwere a significant input into the decision to do this very expensive research.\\n Year Model Number of Parameters Dataset size (tokens)\\n2018 GPT 110M 1B\\n2018 BERT 340M 3B\\n2019 GPT-2 1.5B 10B\\n2020 GPT-3 175B 500B\\n2022 PaLM 540B 780B\\n2023 GPT-4 1.4T (?) ?\\n Table 1: Large Language Models (M/B/T = million/billion/trillion). In many\\ncases several model sizes were considered; we quote the largest.\\n14State of the art, in other words an improvement over all previously evaluated models.\\n 15https://commoncrawl.org/\\n16This is Eq. 3 (minus log perplexity) evaluated on texts which were removed or “held out”\\nof the training set, to get a measure of generalization ability.\\n 10Dataset Size tokensParameters non-embeddingCompute PF-days, non-embeddingTest LossFigure 2: Language modeling performance as a function of model size, dataset\\nsize, and amount of compute used for training. From Kaplan et al, “Scaling\\nLaws for Neural Language Models,” 2020 .\\n Now it should be realized that, while the measure being improved here is\\nfairly objective, still there was no strong reason to think that improving it would\\nlead to models with qualitatively new “emergent” capabilities. But it appears\\nthat this is what happened: GPT-3 and its fine-tuned cousins (such as Codex)\\nwere able to do tasks, such as write computer code from a natural language\\ndescription, for which smaller models were almost worthless.17We will discuss\\nmore of this progress shortly, and speculate a bit in the conclusions.\\n One of the most interesting LLM phenomena is in-context learning, first\\ndiscussed in the original GPT-3 paper . This refers to the ability of an\\nLLM to carry out tasks different from its original objective without modify-\\ning its parameters, indeed without any need for additional training on the new\\ntask (fine tuning). Rather, after being given (as input text) a few examples\\nof input-output pairs, the LLM can be given another input and will generate\\na suitable output. Say the new task is question answering, then after a few\\nquestion-answer examples the LLM will answer the next question it is given.\\n While intuition based on human abilities might find this unremarkable, it is\\nactually quite unusual for an ML model and this is why the pretraining-fine\\ntuning paradigm was the usual approach in previous work. Of course the train-\\ning set already contains many examples of QA pairs. More striking are tasks\\nwhich are not much represented in the training set, such as finding anagrams\\nor rearranging letters in words. One can even do in-context “meta-learning” of\\nmachine learning tasks such as linear regression (see §4).\\n Once it is established that the model can generalize from a few examples, a\\nfurther step towards human capabilities is to try zero examples, instead simply\\nexplaining the task in natural language. At this point it becomes difficult to\\nclassify the tasks – should we consider the task of writing code from a natural\\nlanguage specification to be a form of translation, or an example of explaining\\nthe task, or something else? The relation between the input text or “prompt”\\n17A quantitative version of this claim is that performance for the “emergent” capability\\nimproves rapidly at some threshold value of the word prediction loss. This claim is disputed,\\nsee  for discussion.\\n 11and the output has many surprising features. For example, a standard tech-\\nnique in LLM question answering which measurably improves performance is to\\nprecede the question with a prompt such as “I will answer this question help-\\nfully and truthfully.” Is this somehow biasing the network towards certain texts\\nand away from others (after all the internet corpus is hardly a reliable source of\\ntruth) ? Suppose we have a theory of how this works, how can we test it? Does\\nthe model “know” anything about the truth of statements? \\n As has been much reported, one of the major difficulties in using LLMs\\nfor practical tasks is their propensity to invent facts (especially citations) and\\ntheir limited ability to do logical reasoning, algebra and other symbolic tasks.\\n A device for improving this, called “chain of thought prompting,” is to give\\nexamples (say of question answer task for definiteness) with some intermediate\\nreasoning steps spelled out. This was used in the Minerva QA system \\nwhich produced the example in Figure 1. Still the fraction of problems it solved\\ncorrectly is around 50% (the later GPT-4 is similar). Even for simpler questions,\\nthe reliability of GPT-4 is more like 90%. Much current research is devoted to\\nthis problem of reliable reasoning, as we discuss in §8.\\n 4 Phenomenology of language models\\nIn this section we discuss general claims, “non-invasive” experiments, and the-\\noretical arguments which do not depend on “microscopic details” of the models\\nsuch as the trained weights.18This includes evaluation of model capabiliities,\\nqualitative observations and scaling laws.\\n What can LLMs do? There is a huge body of work on this question, and any\\nattempt to review it would rapidly go out of date, but let us review the primary\\nmethod for studying it. This is benchmarking, the development of standardized\\nsets of test items for which model accuracy can be evaluated in a reproducible\\nway. This is in principle straightforward if the input corresponds to a single\\ncorrect output, as in multiple choice question answering.19If the answer is free-\\nform text, one can use text comparison metrics such as the ROUGE score. One\\ncurrent standard for evaluating LLMs, BIG-bench , combines 204 language\\ntasks (at first publication; they accept new tasks) including translation, QA,\\npuzzle solving, text classification and summarization, and tests of common sense\\nreasoning. A leaderboard listing the current best LLMs is at20. Another is the\\nEleutherAI “Language Model Evaluation Harness”21and leaderboard.22The\\nbenchmark suite HELM  measures additional metrics such as tendency to\\nrepeat copyrighted material, bias, toxicity and the like.\\n 18We are calling this “phenomenology” following the physics use of the term, not its use in\\npsychology and philosophy to describe the study of subjective experience.\\n 19A potential pitfall is that after a benchmark is published, the test items can find their\\nway into future training data and then be solved by memorization. Methods to detect and\\nprevent this are discussed in the references.\\n 20https://paperswithcode.com/dataset/big-bench\\n21https://github.com/EleutherAI/lm-evaluation-harness\\n22https://huggingface.co/spaces/HuggingFaceH4/open llmleaderboard\\n12Reasoning ability is of particular interest for mathematical and scientific ap-\\nplications – of course we all look forward to the day when computers will help us\\ngrade assignments, referee papers and do our research. There are many bench-\\nmarks for solving logical problems expressed in natural language. Benchmarks\\nfor mathematical theorem proving include NaturalProofs , MiniF2F \\nand ProofNet ; as of mid-2023 LLMs (and the best other systems) can find\\nmany proofs (20–80%) but still fail on some seemingly easy cases. Simpler as-\\npects of reasoning which have benchmarks are the ability to deal with negation\\n, consistency (between different phrasings of the same question) , and\\ncompositionality (the ability to analyze statements and problems into simpler\\nparts, solve these and combine the results) .\\n Natural language tasks are very complex, and benchmarks constructed from\\nreal world data cannot be used directly in theoretical considerations. For this\\npurpose one generally defines “toy worlds” and generates synthetic data. The\\npossibilities are endless, but some which have been used are arithmetic prob-\\nlems (decimal arithmetic; modular arithmetic), game play, solving systems of\\nequations, and parsing formal languages. A particularly interesting task is lin-\\near regression ; since this is the prototypical case of statistical inference, a\\nsystem which learns to do it can be said to be “learning how to learn.”\\n Coming to scaling laws, denote the model size (number of parameters) as\\nPand the dataset size (number of tokens in the corpus) as D, then there are\\ntwo general regimes. If we hold one of these (say P) fixed and take the other\\n(sayD) to infinity, then a law of large numbers applies and L ∼ 1/D. On the\\nother hand, if we take one parameter very large and study the dependence on\\nthe other, nontrivial power law scaling can emerge. In principle one can get\\ndifferent exponents for DandP, suggesting the ansatz\\nL(P, D) =\"\\x12Pc\\nP\\x13αP/αD\\n+Dc\\nD#αD\\n. (4)\\nwhere Lis test loss Eq. 3 computed in an optimally regularized model.23This\\nis a good fit to Figure 2.\\n While in Figure 2 the two exponents appear to differ, there is not really\\nconvincing evidence that this is significant. Before working hard on this, one\\nshould ask if there is any way to control the many choices involved, so as to define\\nuniversal exponents. One context in which this can be studied systematically is\\ntransfer learning, by distinguishing the dependence on the pretraining and fine\\ntuning datasets . Another relevant and practical question is whether one\\ncan prune the dataset to improve the scaling. It is intuitively plausible and can\\nbe shown in examples that sets of data items are worth more if they are diverse\\nthan if they are similar. The challenge is to find simple ways to quantify this\\nsimilarity; in  many proposals are studied.\\n 23Regularization is a standard technique in statistics and ML used to control overfitting by\\nmodels with too many parameters. If one does not regularize one sees other phenomena such\\nas double descent . For further discussion see .\\n 13Scaling laws can arise in many ways, not specific to language models. One\\nhypothesis is that the data lies on a low dimensional submanifold in a higher\\ndimensional space.24Both the number of parameters and the number of points\\nrequired to fit this manifold go as the dimension dof the manifold, and this\\nleads to αP=αD= 4/d(the precise coefficient 4 depends on assumptions\\nabout smoothness) .\\n A related hypothesis is that the spectral density of the data covariance falls\\noff as a power law, and in  Eq. 4 is derived for a random feature model with\\nthis covariance. This hypothesis follows from the low dimensional hypothesis\\nbut it is more general, for example these authors argue that additional features\\nderived from the data (as in nonlinear models such as FFN’s) generally have\\nthe same spectrum as the original data. One can also try to relate Eq. 4 and\\ncorrections to it to hypotheses about how tasks are learned .\\n What does the scaling of the information theoretic quantity Eq. 3 have to\\ndo with performance on tasks requiring intelligence? A priori , not much, but\\none way to motivate a focus on it is to draw an analogy with particle physics.\\n In the 30’s cosmic ray observations gave strong hints of new physics at higher\\nenergies, but the interesting events were too rare and uncontrolled to draw solid\\nconclusions. Thus physicists were motivated to build accelerators. These are\\nnot that expensive when they fit on a tabletop, but rapidly grow in size and\\ncost. How large does an accelerator need to be? The right measure is not its\\nsizeper se but rather the energy of the particles it can produce. The physics\\nrelating size and energy is not trivial (due to effects such as synchrotron ra-\\ndiation) but can be worked out, so one can make a good prediction of energy\\nreach. Still, as one increases energy, will one find a smooth extrapolation of\\nwhat came before, or will one discover qualitatively new phenomena? In the\\ngolden age of accelerator physics (the 50’s-70’s) much new physics was discov-\\nered, mostly associated with new particles which are produced only above sharp\\nenergy thresholds. Currently the highest energy accelerator is the Large Hadron\\nCollider at Cern, where the Higgs particle was discovered in 2012. While we\\nare still waiting for further important discoveries, the potential for discovery is\\ndetermined by measurable properties of the accelerator – by energy and secon-\\ndarily by intensity or “luminosity” – which we can judge even in the absence of\\nqualitative discoveries. In the analogy, perplexity is playing a similar role as an\\nobjective measure of language model performance defined independently of the\\nmore interesting qualitative behaviors which reflect “intelligence.”\\n How far can one push this analogy? Could perplexity be as central to lan-\\nguage as energy is to physics? Eq. 3 has a fairly objective definition, so the\\nidea is not completely crazy. But, not only was its relation to performance on\\nactual tasks not predictable in advance, even after the fact clear “thresholds” or\\nother signals for emergence of tasks have not yet been identified . Perhaps\\nif there are universal thresholds, evidence for them could be seen in humans.25\\nMore likely, additional variables (the quality and nature of the training corpus,\\n24In§5 we explain how text can be thought of embedded in a high dimensional space.\\n25Thanks to Misha Tsodyks for this suggestion.\\n 14details of the tasks, etc.) would need to be controlled to see them. This is\\nanother question probably better studied in simpler tasks using synthetic data.\\n The final topic we discuss is the behavior of the objective function (Eq. 3)\\nas a function of training time.26In almost all ML runs, such a plot shows long\\nplateaus interspersed with steep drops. This has been interpreted in many ways,\\nranging from evidence about the nature of learning, to a simple consequence of\\nrandomness of eigenvalues of the Hessian of the loss function. A more recent\\nobservation is to compare training and testing accuracy on the same plot. In\\n it was argued that these two metrics improve at two distinct stages of\\ntraining. First, the model memorizes training examples. Later, it generalizes\\nto the testing examples. This “grokking” phenomenon has been suggested as\\nevidence for learning of circuits , an idea we discuss in §7.\\n5 Simpler language models\\nHere we describe a few generative language models in detail to fix the concepts.\\n As points of notation, let Wbe the set of words (or, if the reader prefers,\\nnumbers which index a position in a list of words). We denote the cardinality of\\na setSas|S|, so|W|is the number of distinct words. The space of N-component\\nreal vectors is denoted RN.\\n The simplest model is the N-gram model defined in terms of the conditional\\nprobabilities\\nP(wN|w1w2. . . w N−1), (5)\\nwhich are all taken to be independent. Given this minimalist assumption, a\\nplausible way to estimate them from the corpus is\\nP(wN|w1w2. . . w N−1) =Number of occurrences of w1w2. . . w N−1wNP\\nwNumber of occurrences of w1w2. . . w N−1w.\\n (6)\\n This simple model with N= 3 or 4 works better than one might think (see exam-\\nples in ) and can be improved a bit by simple statistical tricks (“smoothing”).\\n But the exponential growth of the number of strings in Nmeans that there is\\nno hope of taking Nlarge enough to model even a single paragraph. The entire\\ninternet contains (in order of magnitude) 1012words, and such a corpus will\\ncontain only a vanishingly small fraction of the likely twenty word strings.27\\nA more general principle which we can take from the N-gram model is the dis-\\ntributional hypothesis, which has been pithily summarized as “you shall know\\na word by the company it keeps.”   In other words, by proper use of the\\nstatistics of neighboring words, one can define quantities which capture prop-\\nerties and even the meanings of words. The simplest expression of this idea\\n26This is roughly the time in which the gradient descent operates, see Eq. 16. In LLMs one\\noften considers each data item only once in a training run, so it is related to (but different\\nfrom) dataset size.\\n 27Statistical estimates of perplexity are in the 100’s, and the best current LLMs have per-\\nplexity ∼20.\\n 15is the co-occurrence matrix. Before explaining this, let us mention a detail of\\npractical systems, which in place of words use “tokens,” meaningful components\\nof words. A physics illustration is the word “supersymmetrization.” Even for a\\nnon-physicist reader encountering it for the first time, this word naturally breaks\\nup into “super,” “symmetry” and “ization,” pieces which appear in many words\\nand which are called tokens. And not only does this decomposition apply to\\nmany words, it helps to understand their meaning. This process of replacing\\nsingle words by strings of tokens (“tokenization”) is a first step in LLM pro-\\ncessing, and henceforth when we say “word” we will mean word or token in this\\nsense.\\n Given a corpus, we define its N-gram co-occurrence matrix MNto be the\\n|W| × |W| matrix whose ( w, w′) entry counts the number of N-grams in the\\ncorpus containing both words. This matrix defines a map from words to vectors\\nι:W →Rp(7)\\n(where the dimension p=|W|), by taking a word to the corresponding column\\nofMN. Such a map is called a word embedding.\\n Applying this map to each word independently, we can map a string of k\\nwords (in Wk) to a string of vectors, and this is the next step (after tokenization)\\nof LLM processing. One might worry that these are very high dimensional\\nvectors with many zero entries, which seems wasteful. A standard statistical\\ncure for this problem is to do principal component analysis (PCA). In words,\\ninstead of columns of MNwe use the columns of a p×|W| matrix Zchosen such\\nthatZtZis the best rank papproximation to MNin the sense that it minimizes\\ntr (ZtZ−MN)2. One can do better, but this gives the right idea.\\n Next, we feed this string of vectors into some machine learning model to get\\nan output which we use to predict the next word. If we just want the most\\nlikely next word, a good way is to output a vector v∈Rp, and choose the\\nword wwhich maximizes the inner product v·ι(w). We denote this relationship\\nasv∼ι(w). More generally, the standard inverse map from a vector to a\\nprobability distribution on words is the Boltzmann distribution on the inner\\nproducts. Explicitly, we postulate an inverse temperature β= 1/Tand take28\\nv→P(w) =eβv·ι(w)\\nP\\nw′eβv·ι(w′)(8)\\n Here is an observation  which supports the idea that word embeddings\\ncontain information about meaning. Since the embeddings are vectors, they can\\nbe added. Consider the following equation:\\nι(king) −ι(man) + ι(woman) ∼ι(?) (9)\\n28Tis the temperature parameter which can be set in (say) the GPT user interface. Also,\\nthis ratio of exponentials is usually called “softmax” in machine learning as its β→ ∞ limit\\nis the “argmax” function producing a vector whose nonzero components have the same index\\nvalues as the largest of the input(s).\\n 16One might hope that the word which maximizes this inner product is “queen,”\\nand indeed it is so. There are many more such examples; empirically one needs\\nthe dimension p≳100 for this to work. One can argue  that it follows\\nfrom relations between co-occurence statistics:29\\n∀w,MN(w,king) /#(king)\\nMN(w,queen) /#(queen)≈MN(w,man) /#(man)\\nMN(w,woman) /#(woman)(10)\\n Given these ideas and a map Ffrom a list of vectors to a vector, we can\\nnow propose a very general class of L-gram autoregressive language models as\\nthe combination of the following steps:\\n1. Map the Linput words witoLvectors ι(wi).\\n 2. Apply Fto the list of these vectors to get a prediction vector v.\\n3. Use the inverse map Eq. 8 to get a probability distribution over words.\\n Furthermore, if the map Fhas parameters, given a corpus we can determine\\nthem by optimizing the function Eq. 3 with respect to the parameters. And once\\nwe bring in optimization, we can also optimize with respect to the coefficients of\\nthe embedding map Eq. 7, so that we can dispense with co-occurence statistics.\\n This is the general prescription followed by the LLMs, and to complete it we\\njust need to specify a family of maps F. One possibility is to use a general (fully\\nconnected) feed forward neural network (FFN, also called MLP for multilayer\\nperceptron). We recall that an FFN is a composition of two general types of\\nfunctions, linear maps Wiand nonlinear maps θ, so that\\nF(v) =Wd◦θ◦Wd−1◦θ◦. . . ◦W1◦θ◦W0. (11)\\nIn more concrete terms, the maps Wiare multiplication by rectangular matrices\\nof parameters (usually called “weights” in this context), while the maps θact\\nindependently on each component of their input vector by a fixed nonlinear\\nfunction such as tanh or (more typically) ReLU (identity for x≥0 and zero\\nforx <0). The main fact we recall about FFN’s is that, in the limit that the\\nnumber of parameters becomes large, they can approximate any given function\\narbitrarily well . We refer the reader interested in learning more to .\\n We can get a very natural deep learning version of the L-gram models by\\nusing an FFN for the map Fin the prescription above . Since this asked for\\na map from a list of vectors to a vector, we need to convert the input list into a\\nsingle vector. This is easy: we can take the direct sum of the input vectors, i.e.\\nthe dimension L×pvector whose components are the concatenated lists of their\\ncomponents. Using today’s FFNs, one could implement this with L∼100 or so.\\n There does not seem to be much work on large fully connected FFN language\\nmodels, because by the time the technology advanced to this point the far more\\nefficient transformer models had taken over. Still, they illustrate the general\\n29Here #( w) denotes the number of occurences of “ w” in the corpus. These ratios can also\\nbe expressed in terms of the pairwise mutual information, PN(w, u)/P(w)P(u).\\n 17idea and also one of its most obvious limitations. Even with L∼100, often\\npredicting the next word requires remembering words which appeared farther\\nback. To solve this problem we need to incorporate some sort of memory into\\nthe model.\\n The simplest memory is an additional state variable which is updated with\\neach word and used like the other inputs. To do this, we should take the state\\nto be a vector in some Rq. This brings us to the recurrent neural network or\\nRNN. Its definition is hardly any more complex than what we saw before. With\\neach word position (say with index i) we will associate a state vector siwhich\\ncan depend on words up to wiand on the immediately previous state. Then,\\nwe let the map Fdetermine both the next word and the next state as\\n(vi+1, si+1) =F(si, vi, vi−1, vi−2, . . . , v i−k+1), (12)\\nwhere the parenthesis notation on the left hand side means that the output\\nvector of Fis the concatenation of two direct summand output vectors.\\n Mathematically, Eq. 12 is a discrete dynamical system. If we grant that\\nFcan be an arbitrary map, this is a very general class of systems. One way\\nof characterizing its generality is through computational complexity theory, by\\nasking what classes of computation it can perform. In  it was argued that\\nthe RNN is a universal computer, but this granted that the computation of F\\nin Eq. 11 could use infinite precision numbers. Under realistic assumptions\\nthe right complexity class is a finite state machine, which can recognize regular\\nlanguages . We will say more from this point of view in §7.\\n There are many variations on the RNN such as LSTM’s , each with their\\nown advantages, but we must move on.\\n 6 Recipe for an LLM\\nWe are now ready to define the transformer model.30It is simply another class\\nof maps Ffrom lists of vectors to a vector to be used in the prescription above.\\n Indeed, it is a natural generalization of the FFN which is associated to permu-\\ntational symmetry. This is in direct analogy to the use of convolutional neural\\nnetworks (CNNs) for image recognition, which are FFNs which are equivariant\\nunder the symmetry of translations in two dimensions which is natural for the\\nset of images.\\n A transformer is a composition of two types of functions (layers) taken in\\nalternation, each mapping an input list of Lvectors {ui}to an output list of L\\nvectors {vi}. One of these is an FFN as previously discussed, but now applied\\nto each embedding vector independently, so vi=FFFN(ui).\\n 30Other reviews explaining these definitions include .\\n 18The other layer type is called attention, and it is defined as follows:\\n{ui} → { vi=WiX\\nj=1ci,juj} (13)\\nci,j≡expui·B·ujPi\\nj=1expui·B·uj(14)\\nwhere Bis a learnable matrix whose elements are model parameters (equiva-\\nlently, u·B·vis a bilinear form) and Wis a linear map (also learnable).\\n In words, an item viin the output vector is (a linear transformation of) a\\nweighted sum of the inputs ujwith i≤jand can depend on any of them.31\\nThe weights ci,jare given by a “softmax” or Boltzmann weight just as in Eq.\\n8. Thus there is a very general learnable way for each output to choose which\\nof the input vectors are most useful as inputs. Suppose the product u·B·vis\\nthe dot product, then attention selects the input components ujmost similar\\nto the current unit’s input, uj∼uiin the notation of §5. The matrix Ballows\\nfor comparing different parts of the embeddings and ignoring other parts, in a\\nway determined by optimizing the objective function Eq. 3.\\n Composing these two types of functions (or layers) produces a map from\\nRp×LtoRp×L. Often one takes, instead of the pure FFN and attention func-\\ntions, sums of these with the identity function (residual connections). The FFNs\\ngenerally have a single hidden layer which can be of a different dimension, call\\nthisph.32Finally, while the language model prescription asked for a map to Rp,\\nthis is easily obtained by just taking the last vector in the final output list.\\n There are two more essential details to cover (and many minor details we\\nwill skip). The first is the concept of “attention head.” The definition Eq. 13\\nallowed for a general linear transformation Wwhose range is the output vector.\\n We are free to choose its dimension, call it q, and typically one takes this to\\nbe much less than the embedding dimension p. In return one can use many\\ncopies of Eqs. 13,14 in parallel with different choices for BandW, to produce\\nmany outputs. One can then concatenate these outputs to get a final output of\\ndimension p. These copies are called attention heads and we will denote their\\nnumber by H, sop=Hq.\\n The second essential detail is that, so far, there is nothing in the definition\\nthat keeps track of the order of the list of input vectors; the output of Eq. 13\\nwill be invariant under a general permutation of the input vectors. While this\\nis an elegant property, it is not what we want for processing language, for which\\nthe order of the words matters. The cure for this is very simple: one takes as\\ninputs not the word embeddings Eq. 7, but the direct sum (concatenation) of\\nthese with positional embedding vectors, i.e.vectors which encode the position\\n(index) of the word in the string. These can be a combination of sines and\\n31The restriction i≤jto previous or current inputs is done to get an autoregressive model;\\none can relax this for other purposes.\\n 32Explicitly, vi=W1·max(0 , W0·ui+b0) +b1, where b0,1are more learnable parameters.\\n 19cosines of various frequencies, such as \\n(e2i−1, e2i) = ( cosposition\\n100002i/dpos,sinposition\\n100002i/dpos); i∈ {1, . . . ,dpos\\n2}(15)\\n One could instead treat these vectors as learnable parameters. Still, the trig\\nfunction basis for positions may be significant. It has been generalized to rep-\\nresent other graph structures by using eigenfunctions of the graph Laplacian as\\npositional embeddings.\\n The invariance of the transformer model under permutation symmetry is\\nreminiscent of the point we mentioned earlier, that translation symmetry mo-\\ntivates the CNN. However permutation symmetry is badly broken in language,\\neven in the simplest formal languages,33and it is not obvious why this should\\nbe a useful property for the model to have. One might argue that although any\\nparticular language breaks permutation symmetry, it acts naturally on the en-\\nsemble of languages and thus should have a simple representation. For example,\\nbesides the usual infix arithmetic notation “ a+b”, one could instead use prefix\\n“+a b” or postfix “ a b+”. Translating between these notations is arguably\\neasier for permutation invariant maps using position embeddings. An oppos-\\ning view would be that permutation symmetry is just a secondary property of\\nthe simplest model using attention, and that the main point is to explain the\\nvalue of attention. In addition to its ability to select similar items, it provides\\na simple way to take products of embedding vectors. In computational com-\\nplexity terms, attention enlarges the class of circuits which can be simulated by\\na constant depth transformer . Physics analogies of Eqs. 13,14,\\nespecially to the Hopfield model, may be important .\\n A major practical advantage of the transformer over the RNN and other\\nprevious architectures is that the computations in the attention mechanism can\\nbe done in parallel, so (given sufficiently many processors) the time required does\\nnot increase with the window length L. This is by contrast with the RNN in\\nwhich information propagates from one word to the next, so a window of length\\nLrequires time Lto process. On the other hand the ability of each unit to pay\\nattention to every previous unit means that the total computation required by\\nthe transformer scales as L2. This is the limiting factor for increasing Land\\nthis is widely seen as a problem. There has been a lot of work to improve this\\nscaling, by removing some of the connections (as in sparse attention ), by\\nintroducing multiscale structure, or in other ways.\\n Let us summarize by listing the hyperparameters34and their values for the\\nlargest (175B) GPT-3 . They are\\n•Embedding dimension p= 12288 and hidden layer dimension ph= 4p.\\n •Window length L= 4096 or 8192.\\n •Depth D= 96, counting both FFN (Eq. 11) and attention (Eq. 13)\\nlayers.35\\n33Compare the logical implications A→BandB→A.\\n34This term refers to model choices which are not learned through gradient descent.\\n 35Some of the attention layers in GPT-3 are sparse.\\n 20•Number of heads H= 96 (the equality with Dis a coincidence as far as I\\nknow).\\n The total number of parameters is roughly 12 Dp2.\\n As mentioned earlier, all of these parameters, and the parameters of the em-\\nbedding map Eq. 7, are determined as follows. One generally starts with “ran-\\ndom” initial conditions, usually meaning that each parameter is drawn from a\\nnormal distribution with mean zero and variance chosen so that the linear maps\\nhave expected norm independent of the hyperparameters. As in random matrix\\ntheory, this typically means var( Wi,j)∼1/p, though there are refinements .\\n One then sequences through the training corpus and performs a step of gradient\\ndescent of Eq. 3 for each “batch” of words (here a group of ∼106words). In\\neach step, the parameters ⃗θare modified as\\n⃗θ→⃗θ−η∂Lb\\n∂⃗θ(16)\\nwhere Lbis Eq. 3 restricted to the batch, the conditional probability Pcomes\\nout of Eq. 8 applied to the output of the transformer, and ηis a positive real\\nnumber (the learning rate hyperparameter, here around 10−4).\\n The result of following this procedure on a dataset of natural language text,36\\nsupplemented by many enhancements which are described in the literature and\\nin the model source codes but which may be less important for conceptual\\nunderstanding, is an LLM with the capabiliities we described.\\n 7 Studying the internal workings\\n The success of this procedure raises many questions. Some can be asked about\\nmore or less any ML model – for example, questions about when and how\\noptimization of the objective function Eq. 3 achieves “good” local minima\\n(value near the global minimum and models which generalize well), and the\\norigin of scaling laws like Eq. 4. These are the subject of the general theory of\\nmachine learning, for which we refer to  and much other work.\\n Other questions, and understanding the many striking abilities discussed\\nearlier, sound more specific to LLMs. What would it mean to understand how\\nChatGPT writes poetry based on prompts, or solves physics word problems?\\n At present this is by no means clear and it may be that entirely new concepts\\nare needed to do this. Still, I share the belief that we can go very far towards\\nunderstanding LLMs by building on previous work in computer science, ma-\\nchine learning and AI, and many other fields. There is a well established field\\nof statistical physics and ML  which will surely contribute. Physics ideas\\nare also very relevant for tasks with spatial symmetry, such as image genera-\\ntion  and recognition . The unexpected mathematical simplicity of the\\n36As always in ML it is important that the dataset be “clean” – consistently tokenized, not\\nhaving too much garbage text or repetitions, etc.. Many later LLMs also use programming\\nlanguage code in the dataset. Besides making code generation possible, it has been reported\\nthat this improves performance on natural language reasoning tasks.\\n 21transformer model means that mathematical insights could be valuable. We can\\nalso follow approaches used in neuroscience, psychology, and cognitive science.\\n An evident observation is that the paradigm of neuroscience – careful study\\nof the microscopic workings of the system, following a reductionist philosophy\\n– is far more practical for ML models than it is for human brains, as the micro-\\nscopic workings are fully explicit. This is not to say that it is easy, as we still face\\nthe difficulty of extracting meaning from a system with billions of components\\nand parameters. How could we do this for LLMs?\\n One familiar starting point in neuroscience is to measure the activity of\\nneurons and try to correlate it with properties of the system inputs or outputs.\\n The “grandmother cell” which fires when a subject sees his or her grandmother\\nis an extreme (and controversial) example. Better established are the “place\\ncells” in the hippocampus which fire when an animal passes through a specific\\npart of its environment.\\n Generally there is no reason why the representation should be so direct; there\\nmight be some “neural code” which maps stimuli onto specific combinations\\nor patterns of activity. The details of the neural code could even be different\\nbetween one individual and the next. Analogous concepts in LLMs are the maps\\nfrom input strings to intermediate results or “activations.” The first of these\\nis the embedding map Eq. 7. Considering each layer in succession, its outputs\\n(sometimes called “contextualized embeddings”) also define such a map. The\\ndetails of these maps depend on details of the model, the training dataset and\\nthe choices made in the training procedure. Besides the hyperparameters, these\\ninclude the random initializations of the parameters, the order in which data\\nitems are considered in training and their grouping into batches. Even small\\ndifferences can be amplified by the nonlinear nature of the loss landscape.\\n One way to deal with this indeterminacy is to look for structure in the maps\\nwhich does not depend on these choices. The linear relations Eq. 9 between word\\nembeddings are a very elegant example, telling us (and presumably the model)\\nsomething about the meanings of the words they represent. Moving on to the\\nlater layers, one can ask whether contextualized embeddings carry information\\nabout the grammatical role of a word, about other words it is associated to\\n(such as the referent of a pronoun), etc.. One can go on to ask whether any\\nof the many structures which – one would think – need to be represented to\\nunderstand the real world, are visible in these embeddings.\\n Many structures are too intricate to show up in linear relations. A more\\ngeneral approach is to postulate a “target” for each training data item and\\ntrain a “probe” model (usually an FFN) to predict it from the embeddings. If\\nthis works, one can go on to modify the internal representation in a minimal way\\nwhich changes the probe prediction, and check if this leads to the corresponding\\neffects on the output (see  and references there).\\n This procedure is simpler to explain in an example. A pretty example of\\nprobing for a world model is the recent work of Li et al  (see also ) on\\nrepresentations in a transformer model trained to play the board game Othello.37\\n37For readers not familiar with this game, two players alternate in placing black and white\\n22They train a model “Othello-GPT”38to take as input a sequence of 60 legal\\nmoves, for example “E3 D3 ...” in the standard algebraic notation, and at each\\nstep to predict the next move. The trained model outputs only legal moves\\nwith very high accuracy, and the question is whether this is done using internal\\nrepresentations which reflect the state of the game board, say the presence\\nof a given color tile in a given position. Following the probe paradigm, they\\nobtain FFNs which, given intermediate activations, can predict whether a board\\nposition is occupied and by which color tile. Furthermore, after modifying\\nthe activations so that the FFN’s output has flipped a tile color, the model\\npredicts legal moves for the modified board state, confirming the identification.\\n Neuroscientists can only dream of doing such targeted experiments.\\n Numerous probe studies have been done on LLMs. One very basic ques-\\ntion is how they understand grammatical roles and relations such as subject,\\nobject and the like. This question can be sharpened to probing their internal\\nrepresentations for parse trees, a concept we review in the appendix. To get\\nthe targets for the probe, one can use a large dataset of sentences labeled with\\nparse trees, the Penn Treebank . This was done for BERT in  by\\nthe following procedure: denote the embedding (in a fixed layer) of word ias\\nui, then the model learns a projection Pon this space, such that the distances\\nd(i, j)≡ ||P(ui−uj)||in this inner product well approximate the distance be-\\ntween words iandjdefined as the length of the shortest path connecting them\\nin the parse tree. For BERT (with d∼1000) this worked well with a projection\\nPof rank ∼50.\\n Once one knows something about how information is represented by the\\nmodels, one can go on to try to understand how the computations are done. One\\napproach, also analogous to neuroscience, is to look for specific “circuits” which\\nperform specific computations. An example of a circuit which appears in trained\\ntransformer models is the induction head . This performs the following\\ntask: given a sequence such as “ A B . . . A ” it predicts a repetition, in this\\nexample “ B.” The matching between the tokens (the two A’s in the example) is\\ndone by attention. A number of works have proposed and studied such circuits,\\nwith various motivations and using various theoretical lenses: interpretability\\nand LLMs , in-context learning , formal language theory ,\\ncomputational complexity theory , etc..\\n Reverse engineering a large network ab initio ,i.e.with minimal assumptions\\nabout what it is doing, seems challenging, but maybe automated methods will be\\ndeveloped . Another approach is to first develop a detailed computational\\nmodel (CM) to perform a task without looking too much at the system under\\nstudy, and then look for evidence for or against the hypothesis that the system\\nunder study uses it. This approach also has a long history in neuroscience \\nand ways to test such hypotheses have been much discussed. As an example\\ntiles on an 8 ×8 board, and each move results in “flipping” some opponent pieces to the\\nplayer’s color. The main point for us is that the function from moves to board state is easily\\ncomputable yet very nonlocal and nonlinear.\\n 38While this model shares the GPT architecture, it is not trained on any language data,\\njust on Othello games.\\n 23of a research tactic which does not require opening the black box, one can\\nconsider illusions which fool the system in some way. The response to these\\nwill often depend on contingent and non-optimal aspects of the model, so one\\ncan distinguish different models which solve the same task. A new class of\\npredictions which becomes testable for LLMs is to look at performance as a\\nfunction of model size (depth; number of parameters). A particular CM might\\nrequire a certain model size or dataset properties in order to perform well. And\\nof course, one can open the black box: by assuming a particular CM, one can\\nmake predictions for what probe experiments should work.\\n Simple tasks studied in this approach include modular addition  and\\nlinear regression , where several CM’s (gradient descent, ridge regression and\\nexact least squares) were compared. Turning to language processing, a CM for\\nparsing by transformer LLMs was developed in Zhou et al . While this\\nis too lengthy to explain in detail here, let us give the basic idea, starting\\nfrom the PCFG framework discussed in the appendix. Rather than try to\\nrepresent a parse tree in terms of nodes and edges, it is represented by giving\\neach position iin the list of words a set of variables αi,t,j, where tindexes a\\nnonterminal (a left hand side of a rule) and jis another position. If αi,t,jis\\nturned on, this means that a rule with ton the l.h.s. was used to generate\\nthat part of the tree stretching from position ito position j. This can be\\ngeneralized to let αi,t,jbe the probability that a rule is used. These variables\\n(and additional variables βdescribing the rules used higher in the tree) satisfy\\nsimple recursion relations (the Inside-Outside parsing algorithm ). If the\\nrules have at most two symbols on the r.h.s.,39these recursion relations are\\nquadratic in the variables. By encoding the αvariables as components of the\\nembedding, they can be implemented using attention.\\n Naively, this model predicts that embedding dimension pmust be very large,\\nof order the number of nonterminals times the length of a sentence. Since\\nrealistic grammars for English have many hundreds of nonterminals, this seems\\nto contradict the good performance of transformers with p∼1000. This problem\\nis resolved by two observations, of which the first is that one can get fairly good\\nparsing with many fewer ( ∼20) nonterminals. The second is compression, that\\nembeddings and circuits which are simple and interpretable can be mapped into\\nmore “random-looking” lower dimensional forms. This is a well understood\\nconcept for metric spaces , which was implicit in the discussion of word\\nembeddings in §5. There the simplest construction (the co-occurence matrix)\\nproduced vectors with one component for each word, but by projecting on a\\nsubspace one could greatly reduce this dimension with little loss in accuracy.\\n The generalization of these ideas to neural networks seems important.\\n Once one believes an LLM is carrying out a task using a particular circuit\\nor CM, one can go on to ask how it learned this implementation from the data.\\n One can get theoretical results in the limit of infinite training data and/or for\\nsimple tasks in which the dataset is constructed by a random process. Learning\\n39One can rewrite any grammar to have this property (Chomsky normal form) by introduc-\\ning more nonterminals.\\n 24in transformer models trained on realistic amounts of data is mostly studied\\nempirically and using synthetic data. A few recent interesting works are . Intuitively one expects that simpler instances of a task are learned first,\\nallowing the model to learn features which are needed to analyze more complex\\ninstances, and there is a lot of evidence for this. The idea that many submodels\\ncan be learned simultaneously, including straight memorization and submodels\\nwhich rely on structure, also seems important. Ultimately learnability is crucial\\nbut we should keep in mind that in analogous questions in physics, evolution,\\nand so on, it is much easier to understand optimal and critical points in the\\nlandscape than to understand dynamics.\\n This brings us to in-context learning, the ability of an LLM to perform\\ndiverse tasks given only a few examples of input-output pairs. The simplest\\nhypothesis is that the model has learned the individual tasks, and the examples\\nare selecting a particular task from this repertoire. It has been argued that\\nthis is guaranteed to happen (in the infinite data limit) for a model trained\\non a mixture of tasks . If the many tasks have common aspects (for\\nexample parsing might be used in any linguistic task), one can ask how the\\nmodel takes advantage of this, a question discussed in .\\n Understanding LLMs is a very active research area and there is much more\\nwe could say, but let us finish by summarizing the two main approaches we\\ndescribed. One can postulate a representation and a computation designed to\\nperform a task, and look for evidence that the LLM actually uses the postu-\\nlated structure. Alternatively, one can look for a function in some simpler class\\n(such as digital circuits) which well approximates the function computed by the\\ntransformer model, and then “reverse engineer” the simpler function to find\\nout what it is doing. Either or both of these procedures could lead to inter-\\npretable systems and if so, are answers to the question “what has the LLM\\nlearned.” There is no guarantee that they will work and it might turn out that\\none cannot understand LLMs without new ideas, but they deserve to be tried.\\n 8 Questions and discussion\\nLarge language models have revolutionized computational linguistics and opened\\nup many new applications of AI. Understanding how they work is both straight-\\nforward (we explained it in §6) and at the same time an outstanding scientific\\nchallenge. This is because the question “how do they work” has multiple mean-\\nings. On the one hand, LLMs are a relatively simple solution to the task of\\npredicting the likely next word in a text. On the other hand, they also seem to\\nperform many other tasks which require intelligence, such as solving the physics\\nword problem in Figure 1. While we do not have a strong understanding of\\nwhat a system which can perform these tasks must do, a vast body of work in\\ncognitive science and AI supports one’s first naive intuition that such a system\\nmust be doing sophisticated analyses of language, must contain models of the\\nreal world, and must be able to do fairly general logical reasoning. Before it\\nwas demonstrated, the idea that all this could be learned as a byproduct of\\n25word prediction would have seemed hopelessly optimistic, had anyone dared to\\nsuggest it.\\n Extraordinary claims should be greeted with skepticism. One must guard\\nagainst the possibility that a successful ML system is actually picking up on\\nsuperficial aspects or statistical regularities of the inputs, the “clever Hans”\\neffect. Addressing this is an important function of the benchmark evaluations\\ndiscussed in §4. Of course as LLMs get good at performing tasks of practical\\nvalue, the skeptical position becomes hard to maintain.\\n Intelligence and language are incredibly complex and diverse. According to\\nMinsky,40this diversity is a defining feature of intelligence. The goal of under-\\nstanding LLMs (or any general AI) will not be accomplished by understanding\\nall of the content in their training data, the “entire internet.” Rather, the trick\\nwe need to understand is how a single system can learn from this diverse corpus\\nto perform a wide range of tasks. Theories of “what is learnable” are a central\\npart of computer science . Although theoretical understanding has a long\\nway to go to catch up with LLM capabilities, for simpler and better understood\\ntasks much is known.\\n In these notes we mostly looked at this question through the lens of computer\\nscience, and took as the gold standard for explaining how an LLM learns and\\nperforms a task, a computational model expressed as an algorithm or a circuit\\ntogether with arguments that the trained LLM realizes this model. This point of\\nview has many more insights to offer, but before we discuss them let us consider\\nsome other points of view. In §7 we drew the analogy between detailed study\\nof transformer circuits and neuroscience – what others can we consider?\\n Another analogy is with cognitive psychology. LLMs are sufficiently human-\\nlike to make this interesting, and there is a growing literature which applies tests\\nand experimental protocols from psychology to LLMs, see for example  and\\nthe many references there. When discussing this, we should keep in mind the\\nvast differences between how humans and LLMs function. Human brains are\\nnot believed to use the backpropagation learning algorithm, indeed it has been\\nargued that biological neural systems cannot use it . Perhaps related to this,\\nbrains are not feed-forward networks but have many bidirectional connections.\\n Whatever brains are doing, it works very well: LLMs (like other current deep\\nlearning systems) need far more training data than humans. Furthermore, the\\nLLMs we discussed do not interact with the world. Some argue that on philo-\\nsophical grounds, a model trained only on language prediction can never learn\\nmeaning . While I do not find this particular claim convincing, I agree that\\nwe should not assume that LLMs perform tasks the same way humans do. Still\\nboth similarities and differences are interesting; can we make the analogies with\\ncognitive psychology more precise?\\n One analogy , is with the well known concept of “fast and slow think-\\ning” in behavioral psychology . To summarize, humans are postulated to\\nhave two modes of thought, “system 1” which makes fast, intuitive judgments,\\n40What magical trick makes us intelligent? The trick is that there is no trick. The power\\nof intelligence stems from our vast diversity, not from any single, perfect principle. \\n 26and “system 2” which can focus attention and do calculations, logic, and plan-\\nning. While system 2 is more general and less error-prone, using it requires\\nconscious attention and effort. According to the analogy, LLMs implement sys-\\ntem 1 thinking, and are weak at system 2 thinking.\\n In  it is argued that LLMs have “formal linguistic competence” but not\\n“functional competence.” In plainer terms, they are solving problems by manip-\\nulating language using rules, but they lack other mechanisms of human thought.\\n While it may be surprising that a purely rule-based system could do all that\\nLLMs can do, we do not have a good intuition about what rule-based systems\\nwith billions of rules can do.\\n What are the other mechanisms? There is a long-standing hypothesis in cog-\\nnitive science, modularity of mind , according to which the human brain has\\nmany “mental modules” with different capabilities. These include a language\\nmodule of the sort that Chomsky famously advocated and many others, includ-\\ning one for geometric and physical reasoning, another for social reasoning and\\ntheory of mind, and perhaps others. Notably, formal logic and mathematical\\nreasoning seem to call upon different brain regions from those which specialize\\nin language , suggesting that these functions are performed by different men-\\ntal modules. One can thus hypothesize that LLMs have commonalities with the\\nhuman language module and might be useful scientific models for it,41but that\\nprogress towards human level capability will eventually stall without analogs of\\nthe other modules. \\n A related claim is that current LLMs, even when they perform well on bench-\\nmarks, do not construct models of the world. Consider reasoning about spatial\\nrelations – for example if A is in front of B is in front of C, then A is in front of\\nC. Such reasoning is greatly facilitated by representing the locations of objects\\nin space, perhaps in terms of coordinates, perhaps using “place cells” or in some\\nother non-linguistic way. If distance from the observer is explicitly represented\\nand used in reasoning, then it becomes hard to get this type of question wrong.\\n Conversely, to the extent that LLMs do get it wrong, this might be evidence\\nthat they lack this type of world model or cannot effectively use it.\\n There are many papers exhibiting LLM errors and suggesting such inter-\\npretations, but often one finds that next years’ model does not make the same\\nerrors. At the present rate of progress it seems premature to draw any strong\\nconclusions. My own opinion is that there is no barrier in principle to LLMs\\nconstructing internal non-linguistic models of the world, and the work  on\\nOthello-GPT discussed in §7 is a nice demonstration of what is possible. This\\nis not to say that any and all models can be learned, but rather that it might\\nbe better for now to focus on other significant differences between LLM and\\nhuman reasoning, of which there are many. I will come back to this below.\\n If LLMs and other connectionist systems do not work in the same way as\\nbrains, what other guidance do we have? In §7 we discussed one answer, the\\nhypothesis that they work much like the algorithms and circuits studied in\\n41Chomsky rejects this idea, saying that “The child’s operating system is completely differ-\\nent from that of a machine learning program.” (New York Times, March 8, 2023).\\n 27computer science. Perhaps trained LLMs implement algorithms like those de-\\nsigned by computational linguists, or perhaps new algorithms which were not\\npreviously thought up but which can be understood in similar terms. In either\\nversion this is still a hypothesis, but if we grant it we can draw on insights from\\ntheoretical computer science which apply to all such algorithms.\\n Computational complexity theory  makes many statements and con-\\njectures about how the time and space required by a particular computation\\ndepends on the size of the problem (usually meaning the length of the input).\\n The most famous of these, the P̸=NPconjecture, states (very loosely) that for\\nproblems which involve satisfying general logical statements, finding a solution\\ncan be much harder than checking that the solution is correct.\\n From this point of view, a central question is the complexity class of circuits\\nwhich can be realized by constant depth transformers, meaning that the number\\nof layers does not grow with the window size. Roughly, this is the complexity\\nclass TC0of constant depth circuits with threshold gates . Of\\ncourse in an autoregressive LLM one can repeat this operation to compute a\\nsequence of words: thus the circuit defines the transition function of a finite\\nstate machine (FSM) where the state is the window, and the LLM has learned\\nto simulate this FSM. If a natural algorithm to perform a task is in a more\\ndifficult complexity class than the FSM can handle, this is a reason to think the\\ntask cannot be learned by this type of LLM. Conversely, one might conjecture\\nthat any task for which there is an algorithm in this class can be learned, at\\nleast in the limit of an infinite amount of training data.\\n What about the lenses of pure mathematics, theoretical physics and allied\\nfields? Besides my own personal interest in them, these fields have made sub-\\nstantial contributions to statistics and machine learning, especially the interface\\nbetween statistical physics and machine learning is a vibrant field of research\\n. Spin glass theory made a very deep impact, starting with the Hopfield\\nmodel and developing into a far-reaching theory of optimization landscapes and\\ncomplexity. Random matrix theory is central to high dimensional statistics \\nand in many approaches to understanding deep learning . Mathematical\\napproaches to language such as  can reveal new structure and\\nprovide deeper understanding.\\n Another reason to think pure mathematics and theoretical physics have more\\nto contribute is that neural networks, transformers, and many of the models of\\nneuroscience, are formulated in terms of real variables and continuous mathe-\\nmatics. By contrast, computer science is largely based on discrete mathematics,\\nappropriate for some but not all questions. Perhaps word embeddings have im-\\nportant geometric properties, or perhaps the dynamics of gradient descent are\\nbest understood through the intuitions of continuous mathematics and physics.\\n Arguments such as those in §7 which reduce neural networks to digital circuits,\\neven if they do explain their functioning, may not be adequate to explain how\\nthey are learned.\\n Having at least mentioned some of the many points of view, let me combine\\nthese insights and speculate a bit on where this is going. Let me focus on\\nthree capabilities which seem lacking in current LLMs: planning, confidence\\n28judgments, and reflection.\\n Planning, solving problems whose solution requires choosing a series of ac-\\ntions and/or the consideration of future actions by other agents, is one of the\\ncore problems of AI. Making plans generally requires search, and in general\\nsearch is hard (assuming P̸=NP). A familiar example is a chess program,\\nwhich searches through a game tree to judge the longer term value of a candi-\\ndate move by hypothesizing possible future moves. While much of the success\\nof AlphaGo and AlphaZero is attributed to reinforcement learning by self-play,\\nthey also search through game trees; indeed the Monte Carlo tree search algo-\\nrithm on which they built  was considered a key enabling breakthrough.\\n By contrast, LLMs have no component dedicated to search. While it does\\nnot seem impossible that search trees or other structures could be learned inter-\\nnally (like world models), it seems intuitively clear that an autoregressive model\\nwhich predicts one word at a time and cannot go back to revise its predictions\\nin light of what comes later will be seriously handicapped in planning. This\\nobservation is motivating a fair amount of current work on ways to incorporate\\nsearch. LeCun has suggested adding a dynamic programming component to\\nsearch through multiword predictions, as part of his “path towards autonomous\\nmachine intelligence” . Another proposal, the “tree of thoughts” model ,\\nworks with a search tree of LLM responses. A system which uses hierarchical\\nplanning for mathematical theorem proving was developed in .\\n The next capability on my list, making and working with confidence judg-\\nments, has to do with the well known “hallucination” problem, that LLMs often\\nsimply invent statements, including untrue facts and imaginary citations. While\\nadvantageous for a poetry generator, and bearable for a system which makes\\nsuggestions which an expert human user will verify, this is a huge obstacle to\\nmany practical applications. Thus it is the subject of a great deal of research\\n– a few of this month’s papers are . Perhaps by the time you read\\nthese words there will have already been major progress.\\n Why are LLMs producing these hallucinations? One intuition is that they\\nare doing some sort of compression, analogous to JPEG image compression,\\nwhich introduces errors . This point of view suggests that the problem will\\neventually be solved with larger models and perhaps better training protocols\\nwhich focus on the more informative data items .\\n A related intuition is that the problems follow from inability to properly\\ngeneralize. This comes back to the point about “world models” – a correct\\nmodel, for example an internal encoding of place information, by definition\\ncorrectly treats the properties being modeled. Now suppose we grant that the\\nLLM is solving some class of problems, not by constructing such a model, but by\\nrule-based reasoning. In other words, the LLM somehow learns rules from the\\ncorpus which it uses to make particular inferences which agree with the model.\\n While such rules can cover any number of cases, there is no clear reason for such\\na rule set to ever cover all cases.\\n Another intuition is that the training data contains errors and this is reflected\\nin the results. Certainly the internet is not known for being a completely reliable\\nsource of truth. This intuition also fits with the observation that adding code\\n29(computer programs) to the training set improves natural language reasoning.\\n Code is a good source of rules because almost all of it has been debugged, leading\\nto rules which are correct in their original context (of course they might not be\\ncorrectly applied). It is a longstanding question whether internal representations\\n(both in AI and in humans) are shared between different natural languages; it\\nwould be truly fascinating to know how much they are also shared with code.\\n If this intuition is right, then LLMs reasoning capability might be improved by\\ntraining on far more code and other content which is guaranteed to be correct.\\n Such content could be generated synthetically as tautologies, or even better as\\nformal verified mathematics (as proposed in ).\\n Here is a different point of view: the problem is not that the systems make\\nthings up, after all creativity has value. Rather, it is that they do not provide\\nmuch indication about the confidence to place in a particular output, and do\\nnot have ways to adapt their reasoning to statements known at different levels of\\nconfidence. Much of our reasoning involves uncertain claims and claims which\\nturn out to be false, the point is to distinguish these from justified claims and\\nkeep track of our confidence in each belief. While it is possible to extract\\nconfidence scores from LLMs , there is also a philosophical point to make\\nhere: not all facts have the same epistemological status. Some facts are grounded\\nin evidence; others are true by definition.\\n LLMs are of course statistical models. Even for a completely deterministic\\ntask, say doing arithmetic, a statistical approach to learning is very powerful.\\n This is because learning based on inputs which consist of finitely many training\\nexamples, given in a random order, is naturally formulated in statistical terms.\\n But without making additional non-statistical assumptions, one can never go\\nfrom almost 100% confidence to 100% confidence.\\n This difference is crucial in many aspects of human thought. Of course,\\nlogical reasoning and mathematics stand out as prime examples. Long chains\\nof reasoning are only possible if the individual links are reliable. But it is also\\ncrucial in social reasoning. There is an essential difference between statistical\\nand evidence-based statements, say “Michael is a popular name,” and tauto-\\nlogical, definitional and descriptive statements such as “My name is Michael.”\\n While the first statement might be a subject of discussion, a model which can\\nget confused about the second statement is clearly missing a defining aspect of\\nhuman thought, and will lose the confidence of its interlocutor. Perhaps episte-\\nmological status and tautological correctness need to be somehow represented\\nin the model. It need not be designed in, but the model needs to be given\\nadditional signals beyond next word prediction to learn it.\\n The third point on my list, reflection, does not seem to be much discussed,\\nbut to me seems just as important. In computer science, reflection is the ca-\\npability of a system to work with its programs as a form of data .\\n This is naturally possible for a computer programmed in assembly language, in\\nwhich instructions are encoded in integers. To some extent it is also possible\\nin Lisp, in which programs are encoded in a universal list data structure. As\\ntype systems and other programming language refinements are introduced, re-\\nflection becomes more difficult to provide, but it is necessary for systems-level\\n30programming and makes various standard tasks easier to implement.\\n Since an LLM operates on language, reflection for an LLM is the ability to\\nwork with its internal model in linguistic terms. This is related to ML inter-\\npretability, the ability to translate a model into understandable terms. In §7 we\\ndiscussed interpretability of LLMs in terms of circuits and computational mod-\\nels, implicitly leaving these for a human to interpret and understand. One can\\nimagine an “interpretation engine” which given a model, automatically produces\\na more interpretable description, in terms of circuits, rules, or even a description\\nof the model’s functioning in natural language. Given such an interpretation\\nengine, by applying it to an LLM and sending its output as an input to the\\nLLM, we can implement a form of reflection.\\n A basic human capability which corresponds to this process is the translation\\nfrom procedural or other implicit forms of memory to linguistic, explicit mem-\\nory. Very often, we learn by doing – riding a bicycle, solving math problems,\\ninteracting socially. We then reflect on what we have learned – in some uncon-\\nscious way – and occasionally come up with verbal observations, summaries, in\\na word reflections. It is fascinating that combining the ideas we discussed brings\\nus into contact with such topics.\\n To conclude, and for what it is worth, out of the forty years I have followed\\nAI, this is by far the most exciting period. I agree with those who think LLMs\\nare a major milestone and believe the ideas behind them – including the trans-\\nformer architecture – will remain important even in the light of future progress.\\n The questions they raise are interesting and important enough that – even as\\nthe specialists make remarkable progress – we need not leave the field to them,\\nbut as scientists and thinkers we should engage and try to contribute.\\n A Grammars and parsing\\nMost readers will have encountered the idea of “sentence diagram,” which graph-\\nically represents the decomposition of a sentence into clauses with a subject,\\nverb and object, the assignment of adjectives and prepositional phrases to the\\nnouns and verbs they modify, and so on. Formal versions of this concept are\\nfoundational in linguistics and computer science, and a short introduction (or\\nreview) is a good way to bring the general ideas we are discussing to life.\\n A formal grammar can be given by a set of “production rules” which can\\nbe used to generate grammatical strings. A simple example is in Figure 3.\\n Each line is a rule, which is made up of two strings of symbols separated by\\n→, the left hand side or lhs and right hand side or rhs. These symbols can be\\n“terminals” which appear in the language (such as +, ∗,x, 0 and so on in our\\nexample) or “nonterminals” which do not (such as TERM).\\n These rules are used as follows: We start with a string Scontaining a dis-\\ntinguished “start” symbol (here EXPR). We then iterate the following process:\\nchoose a rule whose lhs occurs in S, and apply it by substituting one occurrence\\nof this lhs in Swith the rhs. Every string Swhich can be obtained by a finite\\nsequence of these operations is considered grammatical, and by keeping track\\n31EXPR →TERM + EXPR (17)\\nEXPR →( EXPR )\\n EXPR →TERM\\nTERM →VALUE ∗TERM\\nTERM →( EXPR )\\n TERM →VALUE\\nVALUE →x\\nVALUE →y\\nVALUE →1\\nFigure 3: A grammar for arithmetic expressions.\\nof the rule applications we get a parse tree. This is a graph whose nodes are\\nsymbols and whose edges connect a symbol which appears on the lhs of a rule\\napplication with the nodes for each of the symbols which appear on the rhs.42\\nA good exercise is to work out the parse tree for the expression y+ 1∗xand\\ncheck that multiplication takes precedence over addition.\\n Our example of a grammar is a context-free grammar, meaning that the\\nleft hand side of each rule consists of a single symbol. If we do not put this\\nrestriction, the resulting class of languages are universal computers (and thus\\nsuffer from potential undecidability). There is also a more restricted class of\\ngrammars called regular grammars (this hierarchy was found by Chomsky), but\\nthese cannot describe nested structures such as the parentheses of Eq. 17. The\\ncontext-free grammars are the right degree of complexity for many purposes. In\\nparticular, programming languages and the formal languages of mathematical\\nlogic can be described using CFG’s and thus the algorithms for working with\\nthem and associated theory are well developed.\\n Besides recognizing and parsing languages, one can describe other linguistic\\ntasks in similar terms. A trivial example would be word replacement, with\\nrules such as OLD i→NEW i. Realistic tasks benefit from frameworks with\\nmore structure. For example, to use the grammar in Eq. 17 to do arithmetic,\\nwe would be much better off with a framework in which the token VALUE\\ncarries an associated numerical or symbolic value. This can be done with the\\nframework of attribute grammars. When we suggest in §8 that LLMs perform\\nnatural language tasks using systems of large numbers of rules, we have this\\nsort of extended grammatical framework in mind.\\n CFG’s are not really adequate for natural languages, with their inherent\\nambiguity and their many special cases and exceptions. A more general for-\\nmalism is the probabilistic CFG. This is obtained by associating a probability\\ndistribution to each symbol which appears on the left hand side of a rule (the\\nnonterminals). For example, we might stipulate that a VALUE has a 75% chance\\n42One can see examples for English sentences in the Wikipedia article “Parse tree.”\\n 32to be a number and a 25% chance to be a variable. With this information, a\\nPCFG defines a probability distribution on strings, which gives zero probability\\nto nongrammatical strings.\\n A symbolic approach to parsing would propose two primary algorithms. One\\nis a parser, which given a grammar and an input produces the parse tree. An-\\nother would be an algorithm for learning a grammar from a corpus. Since any\\nfinite corpus can be described by many grammars, PCFG’s are better suited\\nthan CFG’s to this problem. In any case the learning and parsing algorithms\\nare not necessarily related.\\n In the connectionist approach followed by LLMs, these two algorithms are\\nsubsumed into the definition of a model which can parse any PCFG whose rules\\nare encoded in its weights. By training this on a corpus, the model learns a\\nparticular PCFG which generates the corpus. Interpretability as discussed in\\n§7 then means reversing this relation, by extracting a parser and PCFG from\\nthe trained model.\\n\\nlicenses/by/4.0/ .\\n  Ekin Aky¨ urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny\\nZhou. What learning algorithm is in-context learning? Investigations\\nwith linear models, November 2022. arXiv:2211.15661 . URL: http:\\n//arxiv.org/abs/2211.15661 ,doi:10.48550/arXiv.2211.15661 .\\n Marie Amalric and Stanislas Dehaene. A distinct cortical network for\\nmathematical knowledge in the human brain. NeuroImage , 189:19–31,\\nApril 2019. URL: https://www.sciencedirect.com/science/article/\\npii/S1053811919300011 ,doi:10.1016/j.neuroimage.2019.01.001 .\\n Sanjeev Arora and Boaz Barak. Computational complexity: a modern\\napproach . Cambridge University Press, 2009.\\n Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Ris-\\nteski. A Latent Variable Model Approach to PMI-based Word Embed-\\ndings. arXiv:1502.03520  , June 2019. arXiv: 1502.03520. URL:\\nhttp://arxiv.org/abs/1502.03520 .\\n  Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W.\\nAyers, Dragomir Radev, and Jeremy Avigad. ProofNet: Autoformalizing\\nand Formally Proving Undergraduate-Level Mathematics, February 2023.\\narXiv:2302.12433 . URL: http://arxiv.org/abs/2302.12433 ,doi:\\n10.48550/arXiv.2302.12433 .\\n  Sebastian Bader and Pascal Hitzler. Dimensions of Neural-symbolic In-\\ntegration - A Structured Survey, November 2005. arXiv:cs/0511042.\\n33URL: http://arxiv.org/abs/cs/0511042 ,doi:10.48550/arXiv.cs/\\n0511042 .\\n  Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh\\nSharma. Explaining Neural Scaling Laws. February 2021. URL: https:\\n//arxiv.org/abs/2102.06701v1 .\\n  Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran\\nMalach, and Cyril Zhang. Hidden Progress in Deep Learning: SGD Learns\\nParities Near the Computational Limit, July 2022. arXiv:2207.08799 . URL: http://arxiv.org/abs/2207.08799 ,doi:10.48550/\\narXiv.2207.08799 .\\n  Peter L. Bartlett, Philip M. Long, G´ abor Lugosi, and Alexander Tsigler.\\n Benign overfitting in linear regression. Proceedings of the National\\nAcademy of Sciences , 117(48):30063–30070, 2020. Publisher: National\\nAcad Sciences.\\n  Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learn-\\ning: a statistical viewpoint. arXiv:2103.09177  , March\\n2021. arXiv: 2103.09177. URL: http://arxiv.org/abs/2103.09177 .\\n  Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and\\nadvances. Computational Linguistics , 48:207–219, 2021. URL: http:\\n//arxiv.org/abs/2102.12452 .\\n  Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of\\ndeep learning through the prism of interpolation. arXiv:2105.14368  , May 2021. arXiv: 2105.14368. URL: http://arxiv.org/\\nabs/2105.14368 .\\n  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Man-\\ndal. Reconciling modern machine-learning practice and the classical\\nbias–variance trade-off. Proceedings of the National Academy of Sci-\\nences , 116(32):15849–15854, 2019. Publisher: National Academy of\\nSciences eprint: https://www.pnas.org/content/116/32/15849.full.pdf.\\n URL: https://www.pnas.org/content/116/32/15849 ,doi:10.1073/\\npnas.1903070116 .\\n  Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep\\nlearning we need to understand kernel learning. February 2018. URL:\\nhttps://arxiv.org/abs/1802.01396v3 .\\n  Emily M. Bender and Alexander Koller. Climbing towards NLU: On mean-\\ning, form, and understanding in the age of data. In Proceedings of the\\n58th Annual Meeting of the Association for Computational Linguistics ,\\npages 5185–5198, Online, July 2020. Association for Computational Lin-\\nguistics. URL: https://aclanthology.org/2020.acl-main.463 ,doi:\\n10.18653/v1/2020.acl-main.463 .\\n 34 Yoshua Bengio. From system 1 deep learning to system 2 deep\\nlearning, December 2019. URL: https://slideslive.com/38922304/\\nfrom-system-1-deep-learning-to-system-2-deep-learning .\\n  Yoshua Bengio, R´ ejean Ducharme, and Pascal Vincent. A neural proba-\\nbilistic language model. Advances in neural information processing sys-\\ntems, 13, 2000.\\n  Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and\\nmachine learning , volume 4. Springer, 2006.\\n  Tai-Danae Bradley, John Terilla, and Yiannis Vlassopoulos. An enriched\\ncategory theory of language: from syntax to semantics. arXiv:2106.07890\\n , June 2021. arXiv: 2106.07890. URL: http://arxiv.org/abs/\\n2106.07890 .\\n  Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Robert L\\nMercer, et al. The mathematics of statistical machine translation: Pa-\\nrameter estimation. 1993.\\n  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. Language Models are Few-Shot Learners. arXiv:2005.14165\\n, June 2020. arXiv: 2005.14165. URL: http://arxiv.org/abs/2005.\\n14165 .\\n  Cameron B. Browne, Edward Powley, Daniel Whitehouse, Simon M. Lu-\\ncas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez,\\nSpyridon Samothrakis, and Simon Colton. A survey of monte carlo tree\\nsearch methods. IEEE Transactions on Computational Intelligence and\\nAI in games , 4(1):1–43, 2012.\\n  S´ ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes\\nGehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi\\nLi, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\\nand Yi Zhang. Sparks of Artificial General Intelligence: Early experi-\\nments with GPT-4, March 2023. URL: https://arxiv.org/abs/2303.\\n12712v1 .\\n  Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering\\nLatent Knowledge in Language Models Without Supervision, December\\n2022. arXiv:2212.03827 . URL: http://arxiv.org/abs/2212.03827 .\\n 35 Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin\\nKnight. Recurrent Neural Networks as Weighted Language Recognizers,\\nMarch 2018. arXiv:1711.05408 . URL: http://arxiv.org/abs/1711.\\n05408 ,doi:10.48550/arXiv.1711.05408 .\\n  Ethan A. Chi, John Hewitt, and Christopher D. Manning. Finding Uni-\\nversal Grammatical Relations in Multilingual BERT. arXiv:2005.04511\\n, May 2020. arXiv: 2005.04511. URL: http://arxiv.org/abs/2005.\\n04511 .\\n David Chiang, Peter Cholak, and Anand Pillay. Tighter Bounds on\\nthe Expressivity of Transformer Encoders, May 2023. arXiv:2301.10743\\n. URL: http://arxiv.org/abs/2301.10743 ,doi:10.48550/arXiv.\\n 2301.10743 .\\n Ted Chiang. Chatgpt is a blurry jpeg of the web. The New Yorker ,\\nFebruary 2023.\\n  Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating\\nLong Sequences with Sparse Transformers. April 2019. URL: https:\\n//arxiv.org/abs/1904.10509v1 .\\n  Anna Choromanska, Mikael Henaff, Michael Mathieu, G´ erard Ben Arous,\\nand Yann LeCun. The Loss Surfaces of Multilayer Networks, January\\n2015. arXiv:1412.0233 . URL: http://arxiv.org/abs/1412.0233 ,\\ndoi:10.48550/arXiv.1412.0233 .\\n  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. PaLM: Scal-\\ning Language Modeling with Pathways. arXiv:2204.02311  , April 2022.\\n arXiv: 2204.02311. URL: http://arxiv.org/abs/2204.02311 .\\n  Bilal Chughtai, Lawrence Chan, and Neel Nanda. A Toy Model of Uni-\\nversality: Reverse Engineering How Networks Learn Group Operations,\\nMay 2023. arXiv:2302.03025 . URL: http://arxiv.org/abs/\\n2302.03025 ,doi:10.48550/arXiv.2302.03025 .\\n  Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical\\nFoundations for a Compositional Distributional Model of Meaning, March\\n2010. arXiv:1003.4394 . URL: http://arxiv.org/abs/1003.\\n4394 ,doi:10.48550/arXiv.1003.4394 .\\n  Taco Cohen and Max Welling. Group equivariant convolutional net-\\nworks. In International conference on machine learning , pages 2990–2999.\\nPMLR, 2016. arXiv:1602.07576.\\n  R´ emi Coulom. Efficient selectivity and backup operators in monte-carlo\\ntree search. In International conference on computers and games , pages\\n72–83. Springer, 2006.\\n 36 Francis Crick. The recent excitement about neural networks. Nature ,\\n337:129–132, 1989.\\n  George V. Cybenko. Approximation by superpositions of a sigmoidal\\nfunction. Mathematics of Control, Signals and Systems , 2:303–314, 1989.\\n  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBERT: Pre-training of Deep Bidirectional Transformers for Language Un-\\nderstanding. October 2018. arXiv: 1810.04805. URL: https://arxiv.\\norg/abs/1810.04805v1 .\\n  Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural Network\\nApproximation. arXiv:2012.14501  , December 2020. arXiv:\\n2012.14501. URL: http://arxiv.org/abs/2012.14501 .\\n  Benjamin L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.\\n Inductive Biases and Variable Creation in Self-Attention Mechanisms.\\n arXiv:2110.10090  , October 2021. arXiv: 2110.10090. URL:\\nhttp://arxiv.org/abs/2110.10090 .\\n  Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom\\nHenighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn\\nDrain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario\\nAmodei, Martin Wattenberg, and Christopher Olah. Toy Models of\\nSuperposition, September 2022. arXiv:2209.10652 . URL: http:\\n//arxiv.org/abs/2209.10652 ,doi:10.48550/arXiv.2209.10652 .\\n  Philip Feldman, James R. Foulds, and Shimei Pan. Trapping LLM Hal-\\nlucinations Using Tagged Context Prompts, June 2023. arXiv:2306.06085\\n. URL: http://arxiv.org/abs/2306.06085 ,doi:10.48550/arXiv.\\n2306.06085 .\\n  John Rupert Firth. Studies in linguistic analysis . Wiley-Blackwell, 1957.\\n  Jerry A Fodor. The modularity of mind . MIT press, 1983.\\n  Dan Friedman, Alexander Wettig, and Danqi Chen. Learning Transformer\\nPrograms, June 2023. arXiv:2306.01128 . URL: http://arxiv.org/\\nabs/2306.01128 ,doi:10.48550/arXiv.2306.01128 .\\n  Artur d’Avila Garcez and Luis C. Lamb. Neurosymbolic AI: The 3rd\\nWave, December 2020. arXiv:2012.05876 . URL: http://arxiv.org/\\nabs/2012.05876 .\\n  Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What\\nCan Transformers Learn In-Context? A Case Study of Simple Function\\nClasses, January 2023. arXiv:2208.01066 . URL: http://arxiv.org/\\nabs/2208.01066 ,doi:10.48550/arXiv.2208.01066 .\\n37 Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning . MIT\\npress, 2016.\\n  Anirudh Goyal and Yoshua Bengio. Inductive Biases for Deep Learn-\\ning of Higher-Level Cognition, August 2022. arXiv:2011.15091 . URL: http://arxiv.org/abs/2011.15091 ,doi:10.48550/arXiv.\\n 2011.15091 .\\n  Andrey Gromov. Grokking modular arithmetic, January 2023.\\narXiv:2301.02679 . URL: http://arxiv.org/abs/2301.\\n02679 ,doi:10.48550/arXiv.2301.02679 .\\n  Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini,\\nFosca Giannotti, and Dino Pedreschi. A Survey of Methods for Explain-\\ning Black Box Models. ACM Computing Surveys , 51(5):1–42, September\\n2019. URL: https://dl.acm.org/doi/10.1145/3236009 ,doi:10.1145/\\n3236009 .\\n  Thilo Hagendorff. Machine Psychology: Investigating Emergent Capabil-\\nities and Behavior in Large Language Models Using Psychological Meth-\\nods, April 2023. arXiv:2303.13988 . URL: http://arxiv.org/abs/\\n2303.13988 ,doi:10.48550/arXiv.2303.13988 .\\n  Michael Hahn and Navin Goyal. A Theory of Emergent In-Context\\nLearning as Implicit Structure Induction, March 2023. arXiv:2303.07971\\n. URL: http://arxiv.org/abs/2303.07971 ,doi:10.48550/arXiv.\\n2303.07971 .\\n  Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCan-\\ndlish. Scaling Laws for Transfer, February 2021. arXiv:2102.01293\\n. URL: http://arxiv.org/abs/2102.01293 ,doi:10.48550/arXiv.\\n2102.01293 .\\n  John Hewitt and Christopher D Manning. A Structural Probe for Finding\\nSyntax in Word Representations. page 10, 2019.\\n  Sepp Hochreiter and J¨ urgen Schmidhuber. Long short-term memory. Neu-\\nral computation , 9(8):1735–1780, 1997.\\n  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne\\nHendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\\nVinyals, and Laurent Sifre. Training Compute-Optimal Large Language\\nModels, March 2022. arXiv:2203.15556 . URL: http://arxiv.org/\\nabs/2203.15556 ,doi:10.48550/arXiv.2203.15556 .\\n 38 Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik\\nStrobelt, Duen Horng Chau, Mohammed J. Zaki, and Dmitry Krotov.\\n Energy Transformer, February 2023. arXiv:2302.07253 . URL: http://arxiv.org/abs/2302.07253 ,doi:10.48550/arXiv.\\n2302.07253 .\\n  Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated\\nthe world chess champion . Princeton University Press, 2002.\\n  Myeongjun Jang and Thomas Lukasiewicz. Consistency Analysis of Chat-\\nGPT, March 2023. arXiv:2303.06273 . URL: http://arxiv.org/abs/\\n2303.06273 ,doi:10.48550/arXiv.2303.06273 .\\n  Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu,\\nMateja Jamnik, Timoth´ ee Lacroix, Yuhuai Wu, and Guillaume Lample.\\n Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal\\nProofs, November 2022. arXiv:2210.12283 . URL: http://arxiv.org/\\nabs/2210.12283 ,doi:10.48550/arXiv.2210.12283 .\\n  Iain M. Johnstone. High Dimensional Statistical Inference and Ran-\\ndom Matrices, November 2006. URL: https://arxiv.org/abs/math/\\n0611589v1 .\\n  Dan Jurafsky and James H Martin. Speech and language processing, 2009.\\n  Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn\\nDrain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova Das-\\nSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones,\\nNelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman,\\nStanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson\\nKernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson,\\nSam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben\\nMann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language Mod-\\nels (Mostly) Know What They Know, July 2022. arXiv:2207.05221 .\\n URL: http://arxiv.org/abs/2207.05221 .\\n  Daniel Kahneman. Fast and slow thinking. Allen Lane and Penguin\\nBooks, New York , 2011.\\n  Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Ben-\\njamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and\\nDario Amodei. Scaling Laws for Neural Language Models, January 2020.\\n arXiv:2001.08361 . URL: http://arxiv.org/abs/2001.08361 ,\\ndoi:10.48550/arXiv.2001.08361 .\\n  Michael J Kearns and Umesh Vazirani. An introduction to computational\\nlearning theory . MIT press, 1994.\\n  Daphne Koller and Nir Friedman. Probabilistic graphical models: princi-\\nples and techniques . MIT press, 2009.\\n39 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi-\\nfication with deep convolutional neural networks. Communications of the\\nACM , 60(6):84–90, 2017.\\n  Florent Krzakala, Federico Ricci-Tersenghi, Lenka Zdeborova, Eric W\\nTramel, Riccardo Zecchina, and Leticia F Cugliandolo. Statistical Physics,\\nOptimization, Inference, and Message-Passing Algorithms: Lecture Notes\\nof the Les Houches School of Physics: Special Issue, October 2013 . Num-\\nber 2013. Oxford University Press, 2016.\\n  B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level\\nconcept learning through probabilistic program induction. Science ,\\n350(6266):1332–1338, December 2015. URL: https://www.sciencemag.\\norg/lookup/doi/10.1126/science.aab3050 ,doi:10.1126/science.\\naab3050 .\\n  Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J\\nGershman. Building Machines That Learn and Think Like People. April\\n2016. URL: http://arxiv.org/abs/1604.00289 .\\n  Yann LeCun. Popular talks and private discussion, 2015.\\n  Yann LeCun. A path towards autonomous machine intelligence, 2022.\\n URL: https://openreview.net/forum?id=BZ5a1r-kVsf .\\n  Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature ,\\n521:436–444, 2015.\\n  Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk\\nMichalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and\\nVedant Misra. Solving Quantitative Reasoning Problems with Lan-\\nguage Models, June 2022. Number: arXiv:2206.14858 arXiv:2206.14858\\n. URL: http://arxiv.org/abs/2206.14858 ,doi:10.48550/arXiv.\\n 2206.14858 .\\n  Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi´ egas, Hanspeter\\nPfister, and Martin Wattenberg. Emergent World Representations: Ex-\\nploring a Sequence Model Trained on a Synthetic Task, February 2023.\\narXiv:2210.13382 . URL: http://arxiv.org/abs/2210.13382 ,doi:\\n10.48550/arXiv.2210.13382 .\\n  Kenneth Li, Oam Patel, Fernanda Vi´ egas, Hanspeter Pfister, and Mar-\\ntin Wattenberg. Inference-Time Intervention: Eliciting Truthful Answers\\nfrom a Language Model, June 2023. arXiv:2306.03341 . URL: http:\\n//arxiv.org/abs/2306.03341 ,doi:10.48550/arXiv.2306.03341 .\\n  Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara\\nSoylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai\\n40Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan,\\nCe Zhang, Christian Cosgrove, Christopher D. Manning, Christopher\\nR´ e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus,\\nFaisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav\\nSanthanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,\\nNathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Hender-\\nson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\\nGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav\\nChaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and\\nYuta Koreeda. Holistic Evaluation of Language Models, November 2022.\\n arXiv:2211.09110 . URL: http://arxiv.org/abs/2211.09110 ,doi:\\n10.48550/arXiv.2211.09110 .\\n  Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen\\nBaker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and\\nKarl Cobbe. Let’s Verify Step by Step, May 2023. arXiv:2305.20050\\n. URL: http://arxiv.org/abs/2305.20050 ,doi:10.48550/arXiv.\\n2305.20050 .\\n  Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and\\nCyril Zhang. Transformers Learn Shortcuts to Automata, October 2022.\\n arXiv:2210.10749 . URL: http://arxiv.org/abs/2210.10749 .\\n  David JC MacKay. Information theory, inference and learning algorithms .\\n Cambridge university press, 2003.\\n  Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher,\\nJoshua B. Tenenbaum, and Evelina Fedorenko. Dissociating language\\nand thought in large language models: a cognitive perspective, January\\n2023. arXiv:2301.06627 . URL: http://arxiv.org/abs/2301.06627 ,\\ndoi:10.48550/arXiv.2301.06627 .\\n  Alexander Maloney, Daniel A. Roberts, and James Sully. A Solvable\\nModel of Neural Scaling Laws, October 2022. arXiv:2210.16859 . URL: http://arxiv.org/abs/2210.16859 ,doi:10.48550/arXiv.\\n 2210.16859 .\\n  Yuri Manin and Matilde Marcolli. Semantic Spaces. arXiv.org , May 2016.\\n arXiv: 1605.04238v1. URL: http://arxiv.org/abs/1605.04238v1 .\\n  Christopher Manning and Hinrich Schutze. Foundations of statistical nat-\\nural language processing . MIT press, 1999.\\n  Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal,\\nand Omer Levy. Emergent linguistic structure in artificial neural net-\\nworks trained by self-supervision. Proceedings of the National Academy of\\nSciences , 117(48):30046–30054, 2020.\\n 41 Matilde Marcolli, Noam Chomsky, and Robert Berwick. Mathe-\\nmatical Structure of Syntactic Merge, May 2023. arXiv:2305.18278\\n. URL: http://arxiv.org/abs/2305.18278 ,doi:10.48550/\\narXiv.2305.18278 .\\n  Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Build-\\ning a large annotated corpus of english: The penn treebank. 1993.\\n  David Marr. Vision: A computational investigation into the human rep-\\nresentation and processing of visual information . MIT press, 2010.\\n  Jir ˇi Matouˇ sek. Lecture notes on metric embeddings. 2013. URL: https:\\n//kam.mff.cuni.cz/ ~matousek/ba-a4.pdf .\\n  Pamela McCorduck and Cli Cfe. Machines who think: A personal inquiry\\ninto the history and prospects of artificial intelligence . CRC Press, 2004.\\n  William Merrill. On the Linguistic Capacity of Real-Time Counter Au-\\ntomata. arXiv:2004.06866  , April 2020. arXiv: 2004.06866. URL:\\nhttp://arxiv.org/abs/2004.06866 .\\n  William Merrill and Ashish Sabharwal. The Parallelism Tradeoff: Lim-\\nitations of Log-Precision Transformers, April 2023. arXiv:2207.00729\\n. URL: http://arxiv.org/abs/2207.00729 ,doi:10.48550/arXiv.\\n2207.00729 .\\n  William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated Trans-\\nformers are Constant-Depth Threshold Circuits. arXiv:2106.16213  ,\\nApril 2022. arXiv: 2106.16213. URL: http://arxiv.org/abs/2106.\\n16213 .\\n  Marc Mezard and Andrea Montanari. Information, physics, and compu-\\ntation . Oxford University Press, 2009.\\n  Eric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The Quan-\\ntization Model of Neural Scaling, March 2023. arXiv:2303.13506 . URL: http://arxiv.org/abs/2303.13506 ,doi:10.48550/arXiv.\\n2303.13506 .\\n  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient\\nEstimation of Word Representations in Vector Space, September 2013.\\n arXiv:1301.3781 . URL: http://arxiv.org/abs/1301.3781 .\\n  Marvin Minsky. Society of mind . Simon and Schuster, 1988.\\n  David Mumford. Pattern theory: the mathematics of perception. arXiv\\npreprint math/0212400 , 2002.\\n  David Mumford and Agn` es Desolneux. Pattern theory: the stochastic\\nanalysis of real-world signals . CRC Press, 2010.\\n 42 Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Stein-\\nhardt. Progress measures for grokking via mechanistic interpretability,\\nJanuary 2023. arXiv:2301.05217 . URL: http://arxiv.org/abs/\\n2301.05217 ,doi:10.48550/arXiv.2301.05217 .\\n  Allen Newell, John Clifford Shaw, and Herbert A Simon. Empirical explo-\\nrations of the logic theory machine: a case study in heuristic. In Papers\\npresented at the February 26-28, 1957, western joint computer conference:\\nTechniques for reliability , pages 218–230, 1957.\\n  Nils J Nilsson. The quest for artificial intelligence . Cambridge University\\nPress, 2009.\\n  Chris Olah. Mechanistic interpretability, variables, and the importance of\\ninterpretable bases, 2022. URL: https://transformer-circuits.pub/\\n2022/mech-interp-essay/index.html .\\n  Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das-\\nSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna\\nChen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\\nDanny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\\nLovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared\\nKaplan, Sam McCandlish, and Chris Olah. In-context Learning and\\nInduction Heads, September 2022. arXiv:2209.11895 . URL: http:\\n//arxiv.org/abs/2209.11895 ,doi:10.48550/arXiv.2209.11895 .\\n  Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe:\\nGlobal Vectors for Word Representation. In Proceedings of the 2014 Con-\\nference on Empirical Methods in Natural Language Processing (EMNLP) ,\\npages 1532–1543, Doha, Qatar, October 2014. Association for Com-\\nputational Linguistics. URL: https://www.aclweb.org/anthology/\\nD14-1162 ,doi:10.3115/v1/D14-1162 .\\n  Mary Phuong and Marcus Hutter. Formal Algorithms for Transformers,\\nJuly 2022. arXiv:2207.09238 . URL: http://arxiv.org/abs/2207.\\n09238 .\\n  Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant\\nMisra. Grokking: Generalization Beyond Overfitting on Small Algorith-\\nmic Datasets. arXiv:2201.02177  , January 2022. arXiv: 2201.02177.\\nURL: http://arxiv.org/abs/2201.02177 .\\n  Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith,\\nand Mike Lewis. Measuring and Narrowing the Compositionality Gap\\nin Language Models, October 2022. arXiv:2210.03350 . URL: http:\\n//arxiv.org/abs/2210.03350 ,doi:10.48550/arXiv.2210.03350 .\\n  Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\\n Improving language understanding by generative pre-training. 2018. Pub-\\nlisher: OpenAI.\\n43 Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and\\nIlya Sutskever. Language Models are Unsupervised Multitask Learners.\\nundefined , 2019. URL: https://www.semanticscholar.org/paper/\\nLanguage-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/\\n9405cc0d6169988371b2755e573cc28650d14dfe .\\n  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Explor-\\ning the Limits of Transfer Learning with a Unified Text-to-Text Trans-\\nformer. arXiv:1910.10683  , July 2020. arXiv: 1910.10683. URL:\\nhttp://arxiv.org/abs/1910.10683 .\\n  Hubert Ramsauer, Bernhard Sch¨ afl, Johannes Lehner, Philipp Seidl,\\nMichael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner,\\nMilena Pavlovi´ c, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael\\nKopp, G¨ unter Klambauer, Johannes Brandstetter, and Sepp Hochreiter.\\n Hopfield Networks is All You Need, April 2021. arXiv:2008.02217 . URL: http://arxiv.org/abs/2008.02217 ,doi:10.48550/arXiv.\\n2008.02217 .\\n  Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep\\nLearning Theory. arXiv:2106.10165  , August 2021. arXiv:\\n2106.10165. URL: http://arxiv.org/abs/2106.10165 .\\n Frank Rosenblatt. The perceptron: a probabilistic model for information\\nstorage and organization in the brain. Psychological review , 65(6):386,\\n1958.\\n  David E. Rumelhart, Geoffrey E. Hinton, and James L. McClelland. A\\ngeneral framework for parallel distributed processing. 1986.\\n  Stuart J Russell. Artificial intelligence a modern approach . Pearson Ed-\\nucation, Inc., 2010.\\n  Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo. Are emergent\\nabilities of large language models a mirage? ArXiv , abs/2304.15004, 2023.\\n  Terrence J Sejnowski. The deep learning revolution . MIT press, 2018.\\n  Claude E Shannon. Xxii. programming a computer for playing chess. The\\nLondon, Edinburgh, and Dublin Philosophical Magazine and Journal of\\nScience , 41(314):256–275, 1950.\\n  Hava T Siegelmann and Eduardo D Sontag. On the computational power\\nof neural nets. In Proceedings of the fifth annual workshop on Computa-\\ntional learning theory , pages 440–449, 1992.\\n  Brian Cantwell Smith. Procedural reflection in programming languages\\nvolume i. 1982.\\n 44 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya\\nGanguli. Deep unsupervised learning using nonequilibrium thermodynam-\\nics. In International Conference on Machine Learning , pages 2256–2265.\\nPMLR, 2015. arXiv:1503.03585.\\n  Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and\\nAri S. Morcos. Beyond neural scaling laws: beating power law scaling via\\ndata pruning, June 2022. Number: arXiv:2206.14486 arXiv:2206.14486\\n. URL: http://arxiv.org/abs/2206.14486 ,doi:10.48550/\\narXiv.2206.14486 .\\n  Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. Beyond the\\nImitation Game: Quantifying and extrapolating the capabilities of lan-\\nguage models. Technical Report arXiv:2206.04615, arXiv, June 2022.\\narXiv:2206.04615  type: article. URL: http://arxiv.org/abs/\\n2206.04615 .\\n  Richard Sutton. The bitter lesson, 2019. URL: http://www.\\nincompleteideas.net/IncIdeas/BitterLesson.html .\\n  Christian Szegedy. A promising path towards autoformalization and gen-\\neral artificial intelligence. In International Conference on Intelligent Com-\\nputer Mathematics , 2020.\\n  Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gim-\\npel. Chess as a Testbed for Language Model State Tracking, May\\n2022. arXiv:2102.13249 . URL: http://arxiv.org/abs/2102.13249 ,\\ndoi:10.48550/arXiv.2102.13249 .\\n  Richard E. Turner. An Introduction to Transformers, July 2023.\\narXiv:2304.10557 . URL: http://arxiv.org/abs/2304.10557 ,doi:\\n10.48550/arXiv.2304.10557 .\\n  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Atten-\\ntion Is All You Need. June 2017. arXiv: 1706.03762. URL: https:\\n//arxiv.org/abs/1706.03762 .\\n  Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebas-\\ntian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald\\nMetzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang,\\nJeff Dean, and William Fedus. Emergent Abilities of Large Language\\nModels. 2022. Publisher: arXiv Version Number: 2. URL: https:\\n//arxiv.org/abs/2206.07682 ,doi:10.48550/ARXIV.2206.07682 .\\n  Gail Weiss, Yoav Goldberg, and Eran Yahav. On the Practical Compu-\\ntational Power of Finite Precision RNNs for Language Recognition, May\\n2018. arXiv:1805.04908 . URL: http://arxiv.org/abs/1805.\\n04908 ,doi:10.48550/arXiv.1805.04908 .\\n 45 Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin\\nChoi, and Kyunghyun Cho. NaturalProofs: Mathematical Theorem Prov-\\ning in Natural Language. page 14, 2021.\\n  Noam Wies, Yoav Levine, and Amnon Shashua. The Learnability of In-\\nContext Learning, March 2023. arXiv:2303.07895 . URL: http://\\narxiv.org/abs/2303.07895 ,doi:10.48550/arXiv.2303.07895 .\\n  Avi Wigderson. Mathematics and computation: A theory revolutionizing\\ntechnology and science . Princeton University Press, 2019.\\n  Wikipedia. URL: https://en.wikipedia.org/wiki/Reflective_\\nprogramming .\\n  Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An\\nExplanation of In-context Learning as Implicit Bayesian Inference, July\\n2022. arXiv:2111.02080 . URL: http://arxiv.org/abs/2111.02080 ,\\ndoi:10.48550/arXiv.2111.02080 .\\n  Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong\\nLiu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jian-\\nfeng Gao. Tensor Programs V: Tuning Large Neural Networks via Zero-\\nShot Hyperparameter Transfer, March 2022. arXiv:2203.03466 . URL: http://arxiv.org/abs/2203.03466 ,doi:10.48550/arXiv.\\n 2203.03466 .\\n  Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,\\nYuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Prob-\\nlem Solving with Large Language Models, May 2023. arXiv:2305.10601\\n. URL: http://arxiv.org/abs/2305.10601 ,doi:10.48550/arXiv.\\n2305.10601 .\\n  Yuhui Zhang, Michihiro Yasunaga, Zhengping Zhou, Jeff Z. HaoChen,\\nJames Zou, Percy Liang, and Serena Yeung. Beyond Positive Scaling:\\nHow Negation Impacts Scaling Trends of Language Models, May 2023.\\narXiv:2305.17311 . URL: http://arxiv.org/abs/2305.17311 ,doi:\\n10.48550/arXiv.2305.17311 .\\n  Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do\\nTransformers Parse while Predicting the Masked Word?, March 2023.\\n arXiv:2303.08117 . URL: http://arxiv.org/abs/2303.08117 ,doi:\\n10.48550/arXiv.2303.08117 .\\n  Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F: a cross-\\nsystem benchmark for formal Olympiad-level mathematics, February\\n2022. arXiv:2109.00110 . URL: http://arxiv.org/abs/2109.00110 ,\\ndoi:10.48550/arXiv.2109.00110 .\\n46\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "def clean_research_paper(paper_text):\n",
    "  \"\"\"Cleans a scientific research paper for text summarization.\n",
    "\n",
    "  Args:\n",
    "    paper_text: The text of the research paper.\n",
    "\n",
    "  Returns:\n",
    "    The cleaned text of the research paper.\n",
    "  \"\"\"\n",
    "\n",
    "  # Remove all formatting.\n",
    "  paper_text = re.sub(r\"\\[[^\\]]+\\]\", \"\", paper_text)\n",
    "  paper_text = re.sub(r\"<.*?>\", \"\", paper_text)\n",
    "\n",
    "  # Remove the reference section.\n",
    "  paper_text = remove_references(paper_text)\n",
    "\n",
    "  # Tokenize the text.\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  doc = nlp(paper_text)\n",
    "  tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "  # Remove stop words.\n",
    "  stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "  tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "  # Remove punctuation.\n",
    "  tokens = [token for token in tokens if not token in string.punctuation]\n",
    "\n",
    "  # Calculate the frequency of each word.\n",
    "  word_counts = nltk.FreqDist(tokens)\n",
    "\n",
    "  # Identify the key sentences.\n",
    "  key_sentences = []\n",
    "  for sentence in doc.sents:\n",
    "    sentence_tokens = [token.lemma_ for token in sentence]\n",
    "    sentence_score = 0\n",
    "    for token in sentence_tokens:\n",
    "      sentence_score += word_counts[token]\n",
    "    if sentence_score > 0:\n",
    "      key_sentences.append(sentence)\n",
    "\n",
    "  # Generate the summary.\n",
    "  summary = \" \".join([str(sentence) for sentence in key_sentences])\n",
    "\n",
    "  return summary\n",
    "\n",
    "def remove_references(paper_text):\n",
    "  \"\"\"Removes the reference section from a research paper.\n",
    "\n",
    "  Args:\n",
    "    paper_text: The text of the research paper.\n",
    "\n",
    "  Returns:\n",
    "    The text of the research paper without the reference section.\n",
    "  \"\"\"\n",
    "\n",
    "  # Find the reference section.\n",
    "  reference_pattern = r\"References\\n(.*)\"\n",
    "  reference_match = re.search(reference_pattern, paper_text)\n",
    "\n",
    "  # If the reference section is found, remove it.\n",
    "  if reference_match:\n",
    "    paper_text = re.sub(reference_pattern, \"\", paper_text)\n",
    "\n",
    "  return paper_text\n",
    "\n",
    "clean_research_paper(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Text:\n",
      "large language model michael r douglas cmsa harvard university dept physic stony brook university mdouglascmsafasharvardedu july 2023 abstract artificial intelligence making spectacular progress one best example development large language model llm openais gpt series lecture written reader background mathematics physic give brief history survey state art describe underlying transformer architec ture detail explore current idea llm work model trained predict next word text able perform task displaying intelligence 1arxiv230705782v1 cscl 11 jul 20231 introduction end november 2022 openai released system called chatgpt interacts user natural language answer question engage dialog translate language write computer code fluency ability far exceeding previous publically available system although fall well short human ability many way still large language model technology example widely considered major advance artificial intelligence1 development science technology entered popular conscious ness quickly chatgpt mystery ability use language defining property humanity first time computer well enough make comparison human interesting hope fear developed around ai robot technology generally brought discussion opinion justified speed recent progress make urgent better understand ai forecast capability limitation make wise decision development use great opportunity come great challenge concern u lecture note give introduction subject mathe maticians physicist scientist reader mathematically knowledgeable necessarily expert machine learning artificial intel ligence begin brief overview ai 2 explain idea consider essential context basic principle symbolic connectionist approach 3 define statistical language model relate history transformerbased llm gpt4 4 discus measure llm well give precise explanation simpler language model 5 transformer architecture in6 amazing model defined short equation trained go text simply predict next word appears task seems loosely related definition intelligence task obviously require intelligence solving word problem like one figure 1 present nobody really understands work even interpretation llm controversial ranging belief simply rearranging sentence trained way belief llm learning sophisticated model world simply scaling computation produce artificial general intelligence forecast progress must take account current model shortcoming lack long term memory ability plan tendency make fact hallucinate unreliability logical reasoning etc problem technical solution also look simple significant barrier current work llm take engineering problem solving per 1a many milestone llm development 22 32 39 113 114 2question assume variance first nnatural number 10 variance first meven natural number 16 compute mn model output letσ2be variance first meven natural number andτ2be variance first nnatural number τ2n21 12 andσ2m21 3 solve system equation n21 12 10 m21 3 16 translates system n2 121 m2 48 1 49 therefore n 11 andm 7 sonm 18 figure 1 questionanswer pair solved minerva lewkowycz et al 77 1 solving quantitative reasoning problem language model 2022 spective many interesting work focus understand ing llm work one would think far easier understand ing human brain work full knowledge llm microscopic working wide variety experiment it2these effort early day 7 survey current approach understanding llm conclude 8 general discussion question potentially important development watch start let say little background trained theoretical physicist contribution science string theory interface mathematics followed ai fairly closely since 80 detail since 2016 addition spent eight year quan titative finance gained good deal handson experience chine learning given many lecture telling computer scientist physic physicist computational topic benefited conver sations many people name let thank gerald jay sussman david mcallester yann lecun sanjeev arora surya ganguli jeremy avigad vijay balasubramanian dmitri chklovskii david donoho steve skiena christian szegedy misha tsodyks tony wu 2at least case march 2023 currently weight even de sign parameter gpt4 advanced llm held confidence openai proprietary information 3many speaker cmsa new technology seminar series3and josef urban aitp community4thanks david mcallester sergiy verstyuk comment first draft note would possible input advice hope signal noise ratio approach shared 2 symbolic connectionist ai goal artificial intelligence build computational system perform task require intelligence although intelligence hard define precisely operational definition suitable llm ability task requiring language reasoning planning judged human interact system famous difficult challenge task include playing chess 122 proving mathematical theorem 104 answering natural language question using generally known fact common sense5 problem subject intense investigation since mid 50 textbook history 93 105 119 121 essentially start two broad approach set would later called sym bolic connectionist ai6the symbolic approach originated mathematical logic generative linguistic theory tracked development computer technology hardware software tool solving practical scientific problem central topic approach formal logic lan guage theory search heuristic engineering technique designing building large complex system symbolic ai system designed meaning creator develop de tailed understanding task encode understanding system programming hardware design otherwise example consider task parsing given input string word determine grammatical structure u learned diagram sentence elementary school although linguist developed far sophisticated notion grammar simple notion give right idea grammar language encoded rule belong formal framework see appendix example contextfree grammar given framework one design parsing algorithm take input rule set input string produce output state whether string grammatical sentence make structure explicit symbolic ai approach word symbolize anything grammar come mean 3httpslivehucmsa222pantheonsiteio 4httpaitpconferenceorg2023 5this challenge originates turing famous test restriction question answer ing make better defined testable using benchmark standardized questionanswer set discussion original test found httpsenwikipediaorgwikituring test 6symbolic ai sometimes called gofai good oldfashioned ai related term include rule based logic based expert system feature engineering con nectionist approach many name reflecting mixed ancestry neural deep learning parallel distributed processing differentiable representation learning 4ing grammatical rule parsing algorithm including internal data structure clear meaning designer symbolic method considerable success many task requiring intelligence famously including chess playing 60 symbolic algebra7as well prosaic central task translating high level computer language machine code compiling great deal work done broaden scope example build question answering system well known ibm watson many valuable technique came way systematize rule knowledge base knowledge graph method automated logical reasoning long ago realized one go beyond formal world chess algebra complex messy situation real life although one postulate rule capture many truth used reasoning rule valid case rare furthermore sheer number rule required cover even likely possibility large difficulty addressed implementing probabilistic reasoning getting team human develop requisite enormous rule set leading expert system approach applied example medical question answering cyc8an early well known expert system commercially available database commonly known fact 25 million assertion however dwarfed knowledge base wikidata one billion fact systematic reasoning engine clear approach depends careful human analysis large knowledge base impractical meanwhile different connectionist approach ai cham pioned researcher drew inspiration hypothesis brain work information theory statistic physic natural science applied mathematics particularly opti mization theory diverse point view came together 1990s led great deal interdisciplinary work9of part related ai lie behind llm called machine learning ml usual starting point modern treatment ml rephrase task statistical inference problem based large dataset canonical example image recognition say given array pixel light intensity value estimate probability image depicts cat rather design system one start large dataset image label cat noncat one design general statistical model train dataset predict label given image supervised learning one also selfsupervised learning system predicts part data given part say filling part image third standard ml paradigm reinforcement learning applies task involve choosing action fulfill longer term goal classic example game playing alphago alphazero case since problem formulated statistically possible 7httpswwwsigsamorg 8httpscyccom 9i first learned 83 101 5consider training dataset one item time use incrementally improve model almost always done formulating task term objective function measure quality performed example accuracy correct label assigned image one take parameterized model train optimizing function evaluated training dataset respect model parameter classic model statistic done analytically least square fit general model one us numerical method gradient descent either way central question statistic machine learning generalization meaning extent model well describes data training set sampled probability distribution well known principle speaks question occam razor simpler model generalize better often simplified rule model minimal number parameter needed fit dataset machine learning system deep learning 76 connection ist 118 term generally refer use neural network large number parameter provide effective universal function approxima tor idea old 117 2012 widely believed impractical one argument dull side occam razor mod el many parameter destined overfit would generalize evidently case leading concept benign overfitting 10 15 another argument objective function model highly nonconvex optimization would get stuck poor quality local minimum problem turn solvable reason partially understood 31 49 finally despite effectiveness trained model performing task large number parameter often make hard understand model work given input produce particular output interpretability problem remains key issue deep learning model subject much research 52 many variation hybrid approach story another important one pattern recognition approach 19 102 also based statistical inference like symbolic approach emphasizes value detailed understanding problem domain designing system example one could handcode initial layer image recognition network detect line significant feature image unlike purely symbolic approach feature would used input general statistical neural model another concept illustrates relation two approach probabilistic reasoning use rule chance rain cloudy 50 one state use rule symbolic approach see example 69 essential distinction connectionism use probability rather representation knowledge term explicit meaningful rule suspect every reader already heard symbolic approach dominant early day 2012 along many suc ce led superhuman chess player seemed inadequate 6two challenge task theorem proving question answering 2012 connectionist approach surpassed approach computer vision 70 ever since neural system gone triumph triumph 2016 deep learning system alphazero surpassed symbolic ai chess player course human last year transformer model trained large corpus natural language predict next word appears revolutionized field natural language processing write state art gpt4 demonstrates truly remarkable performance question answering code generation many task 24 simplest arguably deepest explanation history consequence exponential growth computational power training datasets continues present day given limited computing power data ability symbolic pattern recognition approach directly incorporate human understanding system significant advan tage hand given sufficiently large computing power data advantage nullified may even become disadvantageous human effort required code system becomes limiting resource point significant advance ai computation generally come hardware improvement replacing human engineering data driven method forcefully made sutton bitter lesson essay 128 in34 8 discus scaling law evidence idea continuing along current path training everlarger model everlarger datasets achieve agi artificial general intelligence whatever mean realm beyond symbolic connectionist approach generally considered tension10there another point view consid er complementary symbolic approach better suited certain problem example logical reasoning connectionist others ex ample image recognition given point view one seek synthesis neurosymbolic approach advocated many work 7 47 72 conflict another reconciliation hypothesis problem symbolic approach solved using rule algorithm also solved way neural system particular llm however rather algorithm rule coded human result training procedure llm somehow learned encoded network yet mysterious way vague hypothesis sharpened many way part proposing specific mechanism algorithm rule encoded part making general claim algorithm learned discus idea 7 8 10to better discus point one refine symbolicconnectionist dichotomy multiple ax system design versus learning data meaningful rule versus uninterpreted model combinatorial versus differentiable optimization deterministic versus probabilistic 73 language model throughout history linguistics language described term rule rule grammar phonology morphology along log ical framework describing meaning remains case chomskyan linguistics much theoretical linguistics contrast llm statistical language model meaning encode probability distribution string word call pw1 w l approximates distribution realized large body corpus text language simplest example frequency 1gram model defined taking word independently distributed pw1 w l ly i1pwipw number occurrence win corpus total number word corpus 1 course model capture little structure language involves dependency word choice llm generative models11by mean practi cal method sampling distribution explain consider word prediction task word string given input others left blank output given probability distribution pw1 w l corresponding conditional probability distribution output given input example suppose given string cat blank outside blank token mark position missing word relevant conditional probability might pthe cat went outside cat blank outside 0 5 pthe cat sat outside cat blank outside 0 2 summing total probability 1 masked word prediction task model must determine sample distribution particularly convenient case give conditional probability word follows given string denote pwn1w1w2 w n1wn 2 sampling distribution get new word wn1and appending end string extended one word time repeating process give arbitrarily long string law probability sample original probability distribution pw1 w l example pthe cat went outside pthe pcatthepwent cat poutside cat went factorization probability successive conditional probability defines class autoregressive model one could furthermore require 11many different definition term found literature 8the conditional probability eq 2 depends kmost recent word case one would markov model whose state string kwords evaluate good language model want quantify well probability distribution approximates corpus empirical distribu tion standard measure cross entropy autoregressive model sum term one word corpus12 l1 nnnx i1logpwinwiwi1 w in1 3 one also refers exp la perplexity machine learning approach use eq 3 objective function minimize function network parameter train network apply many tool ml backpropagation splitting sum batch varying learning rate get efficient effective model detail art depends particular domain model architecture13conceptually much llm machine learning model statistical approach modeling language pursued since late 80 21 64 87 many model developed recurrent neu ral network rnn describe 5 following general machine learning experience supervised task learning inputoutput pair easier unsupervised task many work addressed machine translation parsing good labeled datasets document translation sentence grammatical structure however unlabeled datasets much larger 2015 sense self supervised learning next frontier 74 leading focus masked word prediction history transformer model start 2017 proposal vaswani et al 132 model designed translation task com plicated explain 6 essential idea use attention positional encoding represent relation word text originated fully present transformer architecture taken many group particularly influential 2018 work include bert 39 gpt 112 bert trained masking arbitrary word sentence next word allows model look backward forward context lead better result however straightforward sample model eventually simpler next word prediction approach followed gpt model work period followed paradigm pretraining followed fine tuning idea first train word prediction large corpus get general purpose model would adapted specific task question answering fine tuning mean second pas supervised learning much smaller labeled 12with sign l 0 better model smaller l term loss function often used objective function property 13in c term generally refers large scale arrangement component system 9dataset replacing next word prediction objective function specific task say question answering could accuracy answer two step procedure justified notion transfer learning meaning capability general purpose model transfer related different task approach led sota14results many benchmark motivated much work importantly great deal ingenuity hard work put solving engineering problem training larger larger model larger larger datasets data lot text available web one much used archive data provided common crawl15training largely done parallel dividing data availability large cluster gpuenabled server industrial lab cloud computing meant sufficient computing resource available principle however overall cost training scale least product model size dataset size becoming expensive precise cost figure gpt series public estimated single training run largest gpt3 model cost ten million dollar motivate efficiently carry costly experiment one need ability predict advance change model dataset size affect training method example optimal choice learning rate performance important advance direction observation power law scaling language model performance 67 figure 2 plot test loss16against logarithm size compute resource used straight line correspond power law relation size perplexity scaling hold many decade model size exponent α 0076 0095 rather small strong argument larger model better performance idea also used determine optimal model dataset size tradeoff 58 scaling hyperparameters 140 result significant input decision expensive research year model number parameter dataset size token 2018 gpt 110m 1b 2018 bert 340m 3b 2019 gpt2 15b 10b 2020 gpt3 175b 500b 2022 palm 540b 780b 2023 gpt4 14t table 1 large language model mbt millionbilliontrillion many case several model size considered quote largest 14state art word improvement previously evaluated model 15httpscommoncrawlorg 16this eq 3 minus log perplexity evaluated text removed held training set get measure generalization ability 10dataset size tokensparameters nonembeddingcompute pfdays nonembeddingtest lossfigure 2 language modeling performance function model size dataset size amount compute used training kaplan et al scaling law neural language model 2020 67 realized measure improved fairly objective still strong reason think improving would lead model qualitatively new emergent capability appears happened gpt3 finetuned cousin codex able task write computer code natural language description smaller model almost worthless17we discus progress shortly speculate bit conclusion one interesting llm phenomenon incontext learning first discussed original gpt3 paper 22 refers ability llm carry task different original objective without modify ing parameter indeed without need additional training new task fine tuning rather given input text example inputoutput pair llm given another input generate suitable output say new task question answering questionanswer example llm answer next question given intuition based human ability might find unremarkable actually quite unusual ml model pretrainingfine tuning paradigm usual approach previous work course train ing set already contains many example qa pair striking task much represented training set finding anagram rearranging letter word one even incontext metalearning machine learning task linear regression see 4 established model generalize example step towards human capability try zero example instead simply explaining task natural language point becomes difficult classify task consider task writing code natural language specification form translation example explaining task something else relation input text prompt 17a quantitative version claim performance emergent capability improves rapidly threshold value word prediction loss claim disputed see 120 133 discussion 11and output many surprising feature example standard tech nique llm question answering measurably improves performance precede question prompt answer question help fully truthfully somehow biasing network towards certain text away others internet corpus hardly reliable source truth suppose theory work test model know anything truth statement 25 79 much reported one major difficulty using llm practical task propensity invent fact especially citation limited ability logical reasoning algebra symbolic task device improving called chain thought prompting give example say question answer task definiteness intermediate reasoning step spelled used minerva qa system 77 produced example figure 1 still fraction problem solved correctly around 50 later gpt4 similar even simpler question reliability gpt4 like 90 much current research devoted problem reliable reasoning discus 8 4 phenomenology language model section discus general claim noninvasive experiment oretical argument depend microscopic detail model trained weights18this includes evaluation model capabiliities qualitative observation scaling law llm huge body work question attempt review would rapidly go date let u review primary method studying benchmarking development standardized set test item model accuracy evaluated reproducible way principle straightforward input corresponds single correct output multiple choice question answering19if answer free form text one use text comparison metric rouge score one current standard evaluating llm bigbench 127 combine 204 language task first publication accept new task including translation qa puzzle solving text classification summarization test common sense reasoning leaderboard listing current best llm at20 another eleutherai language model evaluation harness21and leaderboard22the benchmark suite helm 80 measure additional metric tendency repeat copyrighted material bias toxicity like 18we calling phenomenology following physic use term use psychology philosophy describe study subjective experience 19a potential pitfall benchmark published test item find way future training data solved memorization method detect prevent discussed reference 20httpspaperswithcodecomdatasetbigbench 21httpsgithubcomeleutherailmevaluationharness 22httpshuggingfacecospaceshuggingfaceh4open llmleaderboard 12reasoning ability particular interest mathematical scientific ap plication course look forward day computer help u grade assignment referee paper research many bench mark solving logical problem expressed natural language benchmark mathematical theorem proving include naturalproofs 135 minif2f 144 proofnet 6 mid2023 llm best system find many proof 2080 still fail seemingly easy case simpler pects reasoning benchmark ability deal negation 142 consistency different phrasing question 61 compositionality ability analyze statement problem simpler part solve combine result 111 natural language task complex benchmark constructed real world data used directly theoretical consideration purpose one generally defines toy world generates synthetic data possibility endless used arithmetic prob lem decimal arithmetic modular arithmetic game play solving system equation parsing formal language particularly interesting task lin ear regression 48 since prototypical case statistical inference system learns said learning learn coming scaling law denote model size number parameter pand dataset size number token corpus two general regime hold one say p fixed take sayd infinity law large number applies l 1d hand take one parameter large study dependence nontrivial power law scaling emerge principle one get different exponent dandp suggesting ansatz lp pc pαpαd dc dαd 4 li test loss eq 3 computed optimally regularized model23this good fit figure 2 figure 2 two exponent appear differ really convincing evidence significant working hard one ask way control many choice involved define universal exponent one context studied systematically transfer learning distinguishing dependence pretraining fine tuning datasets 55 another relevant practical question whether one prune dataset improve scaling intuitively plausible shown example set data item worth diverse similar challenge find simple way quantify similarity 126 many proposal studied 23regularization standard technique statistic ml used control overfitting model many parameter one regularize one see phenomenon double descent 14 discussion see 11 13 13scaling law arise many way specific language model one hypothesis data lie low dimensional submanifold higher dimensional space24both number parameter number point required fit manifold go dimension dof manifold lead αpαd 4dthe precise coefficient 4 depends assumption smoothness 8 related hypothesis spectral density data covariance fall power law 85 eq 4 derived random feature model covariance hypothesis follows low dimensional hypothesis general example author argue additional feature derived data nonlinear model ffns generally spectrum original data one also try relate eq 4 correction hypothesis task learned 98 scaling information theoretic quantity eq 3 performance task requiring intelligence priori much one way motivate focus draw analogy particle physic 30 cosmic ray observation gave strong hint new physic higher energy interesting event rare uncontrolled draw solid conclusion thus physicist motivated build accelerator expensive fit tabletop rapidly grow size cost large accelerator need right measure sizeper se rather energy particle produce physic relating size energy trivial due effect synchrotron ra diation worked one make good prediction energy reach still one increase energy one find smooth extrapolation came one discover qualitatively new phenomenon golden age accelerator physic 50s70s much new physic discov ered mostly associated new particle produced sharp energy threshold currently highest energy accelerator large hadron collider cern higgs particle discovered 2012 still waiting important discovery potential discovery determined measurable property accelerator energy secon darily intensity luminosity judge even absence qualitative discovery analogy perplexity playing similar role objective measure language model performance defined independently interesting qualitative behavior reflect intelligence far one push analogy could perplexity central lan guage energy physic eq 3 fairly objective definition idea completely crazy relation performance actual task predictable advance even fact clear threshold signal emergence task yet identified 133 perhaps universal threshold evidence could seen humans25 likely additional variable quality nature training corpus 24in5 explain text thought embedded high dimensional space 25thanks misha tsodyks suggestion 14details task etc would need controlled see another question probably better studied simpler task using synthetic data final topic discus behavior objective function eq 3 function training time26in almost ml run plot show long plateau interspersed steep drop interpreted many way ranging evidence nature learning simple consequence randomness eigenvalue hessian loss function recent observation compare training testing accuracy plot 9 110 argued two metric improve two distinct stage training first model memorizes training example later generalizes testing example grokking phenomenon suggested evidence learning circuit 103 idea discus 7 5 simpler language model describe generative language model detail fix concept point notation let wbe set word reader prefers number index position list word denote cardinality setsass sowis number distinct word space ncomponent real vector denoted rn simplest model ngram model defined term conditional probability pwnw1w2 w n1 5 taken independent given minimalist assumption plausible way estimate corpus pwnw1w2 w n1 number occurrence w1w2 w n1wnp wnumber occurrence w1w2 w n1w 6 simple model n 3 4 work better one might think see exam ples 64 improved bit simple statistical trick smoothing exponential growth number string nmeans hope taking nlarge enough model even single paragraph entire internet contains order magnitude 1012words corpus contain vanishingly small fraction likely twenty word strings27 general principle take ngram model dis tributional hypothesis pithily summarized shall know word company keep 44 word proper use statistic neighboring word one define quantity capture prop erties even meaning word simplest expression idea 26this roughly time gradient descent operates see eq 16 llm one often considers data item training run related different dataset size 27statistical estimate perplexity 100 best current llm per plexity 20 15is cooccurrence matrix explaining let u mention detail practical system place word use token meaningful component word physic illustration word supersymmetrization even nonphysicist reader encountering first time word naturally break super symmetry ization piece appear many word called token decomposition apply many word help understand meaning process replacing single word string token tokenization first step llm pro cessing henceforth say word mean word token sense given corpus define ngram cooccurrence matrix mnto w w matrix whose w w entry count number ngrams corpus containing word matrix defines map word vector ιw rp7 dimension pw taking word corresponding column ofmn map called word embedding applying map word independently map string k word wk string vector next step tokenization llm processing one might worry high dimensional vector many zero entry seems wasteful standard statistical cure problem principal component analysis pca word instead column mnwe use column pw matrix zchosen thatztzis best rank papproximation mnin sense minimizes tr ztzmn2 one better give right idea next feed string vector machine learning model get output use predict next word want likely next word good way output vector vrp choose word wwhich maximizes inner product vιw denote relationship asvιw generally standard inverse map vector probability distribution word boltzmann distribution inner product explicitly postulate inverse temperature β 1tand take28 vpw eβvιw p weβvιw8 observation 99 support idea word embeddings contain information meaning since embeddings vector added consider following equation ιking ιman ιwoman ι 9 28tis temperature parameter set say gpt user interface also ratio exponential usually called softmax machine learning β limit argmax function producing vector whose nonzero component index value largest input 16one might hope word maximizes inner product queen indeed many example empirically one need dimension p100 work one argue 108 5 follows relation cooccurence statistics29 wmnwking king mnwqueen queenmnwman man mnwwoman woman10 given idea map ffrom list vector vector propose general class lgram autoregressive language model combination following step 1 map linput word witolvectors ιwi 2 apply fto list vector get prediction vector v 3 use inverse map eq 8 get probability distribution word furthermore map fha parameter given corpus determine optimizing function eq 3 respect parameter bring optimization also optimize respect coefficient embedding map eq 7 dispense cooccurence statistic general prescription followed llm complete need specify family map f one possibility use general fully connected feed forward neural network ffn also called mlp multilayer perceptron recall ffn composition two general type function linear map wiand nonlinear map θ fv wdθwd1θ w1θw0 11 concrete term map wiare multiplication rectangular matrix parameter usually called weight context map θact independently component input vector fixed nonlinear function tanh typically relu identity x0 zero forx 0 main fact recall ffns limit number parameter becomes large approximate given function arbitrarily well 38 refer reader interested learning 40 116 get natural deep learning version lgram model using ffn map fin prescription 18 since asked map list vector vector need convert input list single vector easy take direct sum input vector ie dimension lpvector whose component concatenated list component using today ffns one could implement l100 seem much work large fully connected ffn language model time technology advanced point far efficient transformer model taken still illustrate general 29here w denotes number occurences w corpus ratio also expressed term pairwise mutual information pnw upwpu 17idea also one obvious limitation even l100 often predicting next word requires remembering word appeared farther back solve problem need incorporate sort memory model simplest memory additional state variable updated word used like input take state vector rq brings u recurrent neural network rnn definition hardly complex saw word position say index associate state vector siwhich depend word wiand immediately previous state let map fdetermine next word next state vi1 si1 fsi vi vi1 vi2 v ik1 12 parenthesis notation left hand side mean output vector fis concatenation two direct summand output vector mathematically eq 12 discrete dynamical system grant fcan arbitrary map general class system one way characterizing generality computational complexity theory asking class computation perform 123 argued rnn universal computer granted computation f eq 11 could use infinite precision number realistic assumption right complexity class finite state machine recognize regular language 26 134 say point view 7 many variation rnn lstms 57 advantage must move 6 recipe llm ready define transformer model30it simply another class map ffrom list vector vector used prescription indeed natural generalization ffn associated permu tational symmetry direct analogy use convolutional neural network cnns image recognition ffns equivariant symmetry translation two dimension natural set image transformer composition two type function layer taken alternation mapping input list lvectors uito output list l vector vi one ffn previously discussed applied embedding vector independently vifffnui 30other review explaining definition include 109 131 18the layer type called attention defined follows ui viwix j1cijuj 13 cijexpuibujpi j1expuibuj14 bi learnable matrix whose element model parameter equiva lently ubvis bilinear form wi linear map also learnable word item viin output vector linear transformation weighted sum input ujwith ijand depend them31 weight cijare given softmax boltzmann weight eq 8 thus general learnable way output choose input vector useful input suppose product ubvis dot product attention selects input component ujmost similar current unit input ujuiin notation 5 matrix ballows comparing different part embeddings ignoring part way determined optimizing objective function eq 3 composing two type function layer produce map rpltorpl often one take instead pure ffn attention func tions sum identity function residual connection ffns generally single hidden layer different dimension call thisph32finally language model prescription asked map rp easily obtained taking last vector final output list two essential detail cover many minor detail skip first concept attention head definition eq 13 allowed general linear transformation wwhose range output vector free choose dimension call q typically one take much le embedding dimension p return one use many copy eq 1314 parallel different choice bandw produce many output one concatenate output get final output dimension p copy called attention head denote number h sophq second essential detail far nothing definition keep track order list input vector output eq 13 invariant general permutation input vector elegant property want processing language order word matter cure simple one take input word embeddings eq 7 direct sum concatenation positional embedding vector ievectors encode position index word string combination sine 31the restriction ijto previous current input done get autoregressive model one relax purpose 32explicitly viw1max0 w0uib0 b1 b01are learnable parameter 19cosines various frequency 132 e2i1 e2i cosposition 100002idpossinposition 100002idpos 1 dpos 215 one could instead treat vector learnable parameter still trig function basis position may significant generalized rep resent graph structure using eigenfunctions graph laplacian positional embeddings invariance transformer model permutation symmetry reminiscent point mentioned earlier translation symmetry mo tivates cnn however permutation symmetry badly broken language even simplest formal languages33and obvious useful property model one might argue although particular language break permutation symmetry act naturally en semble language thus simple representation example besides usual infix arithmetic notation ab one could instead use prefix b postfix b translating notation arguably easier permutation invariant map using position embeddings oppos ing view would permutation symmetry secondary property simplest model using attention main point explain value attention addition ability select similar item provides simple way take product embedding vector computational com plexity term attention enlarges class circuit simulated constant depth transformer 28 41 95 96 physic analogy eq 1314 especially hopfield model may important 59 115 major practical advantage transformer rnn previous architecture computation attention mechanism done parallel given sufficiently many processor time required increase window length l contrast rnn information propagates one word next window length lrequires time lto process hand ability unit pay attention every previous unit mean total computation required transformer scale l2 limiting factor increasing land widely seen problem lot work improve scaling removing connection sparse attention 30 introducing multiscale structure way let u summarize listing hyperparameters34and value largest 175b gpt3 22 embedding dimension p 12288 hidden layer dimension ph 4p window length l 4096 8192 depth 96 counting ffn eq 11 attention eq 13 layers35 33compare logical implication abandba 34this term refers model choice learned gradient descent 35some attention layer gpt3 sparse 20number head h 96 equality dis coincidence far know total number parameter roughly 12 dp2 mentioned earlier parameter parameter em bedding map eq 7 determined follows one generally start ran dom initial condition usually meaning parameter drawn normal distribution mean zero variance chosen linear map expected norm independent hyperparameters random matrix theory typically mean var wij1p though refinement 140 one sequence training corpus performs step gradient descent eq 3 batch word group 106words step parameter θare modified θθηlb θ16 lbis eq 3 restricted batch conditional probability pcomes eq 8 applied output transformer ηis positive real number learning rate hyperparameter around 104 result following procedure dataset natural language text36 supplemented many enhancement described literature model source code may le important conceptual understanding llm capabiliities described 7 studying internal working success procedure raise many question asked le ml model example question optimization objective function eq 3 achieves good local minimum value near global minimum model generalize well origin scaling law like eq 4 subject general theory machine learning refer 19 97 116 much work question understanding many striking ability discussed earlier sound specific llm would mean understand chatgpt writes poetry based prompt solves physic word problem present mean clear may entirely new concept needed still share belief go far towards understanding llm building previous work computer science chine learning ai many field well established field statistical physic ml 97 surely contribute physic idea also relevant task spatial symmetry image genus tion 125 recognition 35 unexpected mathematical simplicity 36as always ml important dataset clean consistently tokenized much garbage text repetition etc many later llm also use programming language code dataset besides making code generation possible reported improves performance natural language reasoning task 21transformer model mean mathematical insight could valuable also follow approach used neuroscience psychology cognitive science evident observation paradigm neuroscience careful study microscopic working system following reductionist philosophy far practical ml model human brain micro scopic working fully explicit say easy still face difficulty extracting meaning system billion component parameter could llm one familiar starting point neuroscience measure activity neuron try correlate property system input output grandmother cell fire subject see grandmother extreme controversial example better established place cell hippocampus fire animal pass specific part environment generally reason representation direct might neural code map stimulus onto specific combination pattern activity detail neural code could even different one individual next analogous concept llm map input string intermediate result activation first embedding map eq 7 considering layer succession output sometimes called contextualized embeddings also define map detail map depend detail model training dataset choice made training procedure besides hyperparameters include random initialization parameter order data item considered training grouping batch even small difference amplified nonlinear nature loss landscape one way deal indeterminacy look structure map depend choice linear relation eq 9 word embeddings elegant example telling u presumably model something meaning word represent moving later layer one ask whether contextualized embeddings carry information grammatical role word word associated referent pronoun etc one go ask whether many structure one would think need represented understand real world visible embeddings many structure intricate show linear relation general approach postulate target training data item train probe model usually ffn predict embeddings work one go modify internal representation minimal way change probe prediction check lead corresponding effect output see 12 reference procedure simpler explain example pretty example probing world model recent work li et al 78 see also 130 representation transformer model trained play board game othello37 37for reader familiar game two player alternate placing black white 22they train model othellogpt38to take input sequence 60 legal move example e3 d3 standard algebraic notation step predict next move trained model output legal move high accuracy question whether done using internal representation reflect state game board say presence given color tile given position following probe paradigm obtain ffns given intermediate activation predict whether board position occupied color tile furthermore modifying activation ffns output flipped tile color model predicts legal move modified board state confirming identification neuroscientist dream targeted experiment numerous probe study done llm one basic ques tion understand grammatical role relation subject object like question sharpened probing internal representation parse tree concept review appendix get target probe one use large dataset sentence labeled parse tree penn treebank 90 done bert 27 56 88 following procedure denote embedding fixed layer word ia ui model learns projection pon space distance di j puiujin inner product well approximate distance tween word iandjdefined length shortest path connecting parse tree bert d1000 worked well projection pof rank 50 one know something information represented model one go try understand computation done one approach also analogous neuroscience look specific circuit perform specific computation example circuit appears trained transformer model induction head 42 107 performs following task given sequence b predicts repetition example b matching token two example done attention number work proposed studied circuit various motivation using various theoretical lens interpretability llm 106 incontext learning 107 2 formal language theory 94 28 computational complexity theory 41 82 etc reverse engineering large network ab initio iewith minimal assumption seems challenging maybe automated method developed 33 46 another approach first develop detailed computational model cm perform task without looking much system study look evidence hypothesis system study us approach also long history neuroscience 91 way test hypothesis much discussed example tile 8 8 board move result flipping opponent piece player color main point u function move board state easily computable yet nonlocal nonlinear 38while model share gpt architecture trained language data othello game 23of research tactic require opening black box one consider illusion fool system way response often depend contingent nonoptimal aspect model one distinguish different model solve task new class prediction becomes testable llm look performance function model size depth number parameter particular cm might require certain model size dataset property order perform well course one open black box assuming particular cm one make prediction probe experiment work simple task studied approach include modular addition 103 linear regression 2 several cm gradient descent ridge regression exact least square compared turning language processing cm parsing transformer llm developed zhou et al 143 lengthy explain detail let u give basic idea starting pcfg framework discussed appendix rather try represent parse tree term node edge represented giving position iin list word set variable αitj tindexes nonterminal left hand side rule ji another position αitjis turned mean rule ton lh used generate part tree stretching position ito position j generalized let αitjbe probability rule used variable additional variable βdescribing rule used higher tree satisfy simple recursion relation insideoutside parsing algorithm 87 rule two symbol rhs39these recursion relation quadratic variable encoding αvariables component embedding implemented using attention naively model predicts embedding dimension pmust large order number nonterminals time length sentence since realistic grammar english many hundred nonterminals seems contradict good performance transformer p1000 problem resolved two observation first one get fairly good parsing many fewer 20 nonterminals second compression embeddings circuit simple interpretable mapped randomlooking lower dimensional form well understood concept metric space 92 implicit discussion word embeddings 5 simplest construction cooccurence matrix produced vector one component word projecting subspace one could greatly reduce dimension little loss accuracy generalization idea neural network seems important one belief llm carrying task using particular circuit cm one go ask learned implementation data one get theoretical result limit infinite training data andor simple task dataset constructed random process learning 39one rewrite grammar property chomsky normal form introduc ing nonterminals 24in transformer model trained realistic amount data mostly studied empirically using synthetic data recent interesting work 2 51 103 intuitively one expects simpler instance task learned first allowing model learn feature needed analyze complex instance lot evidence idea many submodels learned simultaneously including straight memorization submodels rely structure also seems important ultimately learnability crucial keep mind analogous question physic evolution much easier understand optimal critical point landscape understand dynamic brings u incontext learning ability llm perform diverse task given example inputoutput pair simplest hypothesis model learned individual task example selecting particular task repertoire argued guaranteed happen infinite data limit model trained mixture task 139 136 many task common aspect example parsing might used linguistic task one ask model take advantage question discussed 54 understanding llm active research area much could say let u finish summarizing two main approach described one postulate representation computation designed perform task look evidence llm actually us postu lated structure alternatively one look function simpler class digital circuit well approximates function computed transformer model reverse engineer simpler function find either procedure could lead inter pretable system answer question llm learned guarantee work might turn one understand llm without new idea deserve tried 8 question discussion large language model revolutionized computational linguistics opened many new application ai understanding work straight forward explained 6 time outstanding scientific challenge question work multiple mean ings one hand llm relatively simple solution task predicting likely next word text hand also seem perform many task require intelligence solving physic word problem figure 1 strong understanding system perform task must vast body work cognitive science ai support one first naive intuition system must sophisticated analysis language must contain model real world must able fairly general logical reasoning demonstrated idea could learned byproduct 25word prediction would seemed hopelessly optimistic anyone dared suggest extraordinary claim greeted skepticism one must guard possibility successful ml system actually picking superficial aspect statistical regularity input clever han effect addressing important function benchmark evaluation discussed 4 course llm get good performing task practical value skeptical position becomes hard maintain intelligence language incredibly complex diverse according minsky40this diversity defining feature intelligence goal standing llm general ai accomplished understanding content training data entire internet rather trick need understand single system learn diverse corpus perform wide range task theory learnable central part computer science 68 although theoretical understanding long way go catch llm capability simpler better understood task much known note mostly looked question lens computer science took gold standard explaining llm learns performs task computational model expressed algorithm circuit together argument trained llm realizes model point view many insight offer discus let u consider point view 7 drew analogy detailed study transformer circuit neuroscience others consider another analogy cognitive psychology llm sufficiently human like make interesting growing literature applies test experimental protocol psychology llm see example 53 many reference discussing keep mind vast difference human llm function human brain believed use backpropagation learning algorithm indeed argued biological neural system use 37 perhaps related brain feedforward network many bidirectional connection whatever brain work well llm like current deep learning system need far training data human furthermore llm discussed interact world argue philo sophical ground model trained language prediction never learn meaning 16 find particular claim convincing agree assume llm perform task way human still similarity difference interesting make analogy cognitive psychology precise one analogy 17 50 well known concept fast slow think ing behavioral psychology 66 summarize human postulated two mode thought system 1 make fast intuitive judgment 40what magical trick make u intelligent trick trick power intelligence stem vast diversity single perfect principle 100 26and system 2 focus attention calculation logic plan ning system 2 general le errorprone using requires conscious attention effort according analogy llm implement sys tem 1 thinking weak system 2 thinking 84 argued llm formal linguistic competence functional competence plainer term solving problem manip ulating language using rule lack mechanism human thought may surprising purely rulebased system could llm good intuition rulebased system billion rule mechanism longstanding hypothesis cog nitive science modularity mind 45 according human brain many mental module different capability include language module sort chomsky famously advocated many others includ ing one geometric physical reasoning another social reasoning theory mind perhaps others notably formal logic mathematical reasoning seem call upon different brain region specialize language 3 suggesting function performed different men tal module one thus hypothesize llm commonality human language module might useful scientific model it41but progress towards human level capability eventually stall without analog module 73 related claim current llm even perform well bench mark construct model world consider reasoning spatial relation example front b front c front c reasoning greatly facilitated representing location object space perhaps term coordinate perhaps using place cell nonlinguistic way distance observer explicitly represented used reasoning becomes hard get type question wrong conversely extent llm get wrong might evidence lack type world model effectively use many paper exhibiting llm error suggesting inter pretations often one find next year model make error present rate progress seems premature draw strong conclusion opinion barrier principle llm constructing internal nonlinguistic model world work 78 othellogpt discussed 7 nice demonstration possible say model learned rather might better focus significant difference llm human reasoning many come back llm connectionist system work way brain guidance 7 discussed one answer hypothesis work much like algorithm circuit studied 41chomsky reject idea saying child operating system completely differ ent machine learning program new york time march 8 2023 27computer science perhaps trained llm implement algorithm like de signed computational linguist perhaps new algorithm previously thought understood similar term either version still hypothesis grant draw insight theoretical computer science apply algorithm computational complexity theory 4 137 make many statement con jectures time space required particular computation depends size problem usually meaning length input famous pnpconjecture state loosely problem involve satisfying general logical statement finding solution much harder checking solution correct point view central question complexity class circuit realized constant depth transformer meaning number layer grow window size roughly complexity class tc0of constant depth circuit threshold gate 28 41 95 96 course autoregressive llm one repeat operation compute sequence word thus circuit defines transition function finite state machine fsm state window llm learned simulate fsm natural algorithm perform task difficult complexity class fsm handle reason think task learned type llm conversely one might conjecture task algorithm class learned least limit infinite amount training data lens pure mathematics theoretical physic allied field besides personal interest field made sub stantial contribution statistic machine learning especially interface statistical physic machine learning vibrant field research 71 97 spin glass theory made deep impact starting hopfield model developing farreaching theory optimization landscape complexity random matrix theory central high dimensional statistic 63 many approach understanding deep learning 116 mathematical approach language 20 34 86 89 reveal new structure provide deeper understanding another reason think pure mathematics theoretical physic contribute neural network transformer many model neuroscience formulated term real variable continuous mathe matics contrast computer science largely based discrete mathematics appropriate question perhaps word embeddings im portant geometric property perhaps dynamic gradient descent best understood intuition continuous mathematics physic argument 7 reduce neural network digital circuit even explain functioning may adequate explain learned least mentioned many point view let combine insight speculate bit going let focus three capability seem lacking current llm planning confidence 28judgments reflection planning solving problem whose solution requires choosing series ac tions andor consideration future action agent one core problem ai making plan generally requires search general search hard assuming pnp familiar example chess program search game tree judge longer term value candi date move hypothesizing possible future move much success alphago alphazero attributed reinforcement learning selfplay also search game tree indeed monte carlo tree search algo rithm built 36 23 considered key enabling breakthrough contrast llm component dedicated search seem impossible search tree structure could learned inter nally like world model seems intuitively clear autoregressive model predicts one word time go back revise prediction light come later seriously handicapped planning observation motivating fair amount current work way incorporate search lecun suggested adding dynamic programming component search multiword prediction part path towards autonomous machine intelligence 75 another proposal tree thought model 141 work search tree llm response system us hierarchical planning mathematical theorem proving developed 62 next capability list making working confidence judg ments well known hallucination problem llm often simply invent statement including untrue fact imaginary citation advantageous poetry generator bearable system make suggestion expert human user verify huge obstacle many practical application thus subject great deal research month paper 43 79 81 perhaps time read word already major progress llm producing hallucination one intuition sort compression analogous jpeg image compression introduces error 29 point view suggests problem eventually solved larger model perhaps better training protocol focus informative data item 126 related intuition problem follow inability properly generalize come back point world model correct model example internal encoding place information definition correctly treat property modeled suppose grant llm solving class problem constructing model rulebased reasoning word llm somehow learns rule corpus us make particular inference agree model rule cover number case clear reason rule set ever cover case another intuition training data contains error reflected result certainly internet known completely reliable source truth intuition also fit observation adding code 29computer program training set improves natural language reasoning code good source rule almost debugged leading rule correct original context course might correctly applied longstanding question whether internal representation ai human shared different natural language would truly fascinating know much also shared code intuition right llm reasoning capability might improved training far code content guaranteed correct content could generated synthetically tautology even better formal verified mathematics proposed 129 different point view problem system make thing creativity value rather provide much indication confidence place particular output way adapt reasoning statement known different level confidence much reasoning involves uncertain claim claim turn false point distinguish justified claim keep track confidence belief possible extract confidence score llm 65 also philosophical point make fact epistemological status fact grounded evidence others true definition llm course statistical model even completely deterministic task say arithmetic statistical approach learning powerful learning based input consist finitely many training example given random order naturally formulated statistical term without making additional nonstatistical assumption one never go almost 100 confidence 100 confidence difference crucial many aspect human thought course logical reasoning mathematics stand prime example long chain reasoning possible individual link reliable also crucial social reasoning essential difference statistical evidencebased statement say michael popular name tauto logical definitional descriptive statement name michael first statement might subject discussion model get confused second statement clearly missing defining aspect human thought lose confidence interlocutor perhaps episte mological status tautological correctness need somehow represented model need designed model need given additional signal beyond next word prediction learn third point list reflection seem much discussed seems important computer science reflection ca pability system work program form data 124 138 naturally possible computer programmed assembly language instruction encoded integer extent also possible lisp program encoded universal list data structure type system programming language refinement introduced flection becomes difficult provide necessary systemslevel 30programming make various standard task easier implement since llm operates language reflection llm ability work internal model linguistic term related ml inter pretability ability translate model understandable term 7 discussed interpretability llm term circuit computational mod el implicitly leaving human interpret understand one imagine interpretation engine given model automatically produce interpretable description term circuit rule even description model functioning natural language given interpretation engine applying llm sending output input llm implement form reflection basic human capability corresponds process translation procedural implicit form memory linguistic explicit mem ory often learn riding bicycle solving math problem interacting socially reflect learned uncon scious way occasionally come verbal observation summary word reflection fascinating combining idea discussed brings u contact topic conclude worth forty year followed ai far exciting period agree think llm major milestone believe idea behind including trans former architecture remain important even light future progress question raise interesting important enough even specialist make remarkable progress need leave field scientist thinker engage try contribute grammar parsing reader encountered idea sentence diagram graph ically represents decomposition sentence clause subject verb object assignment adjective prepositional phrase noun verb modify formal version concept foundational linguistics computer science short introduction review good way bring general idea discussing life formal grammar given set production rule used generate grammatical string simple example figure 3 line rule made two string symbol separated left hand side lh right hand side rh symbol terminal appear language x 0 example nonterminals term rule used follows start string scontaining dis tinguished start symbol expr iterate following process choose rule whose lh occurs apply substituting one occurrence lh swith rh every string swhich obtained finite sequence operation considered grammatical keeping track 31expr term expr 17 expr expr expr term term value term term expr term value value x value value 1 figure 3 grammar arithmetic expression rule application get parse tree graph whose node symbol whose edge connect symbol appears lh rule application node symbol appear rhs42 good exercise work parse tree expression 1xand check multiplication take precedence addition example grammar contextfree grammar meaning left hand side rule consists single symbol put restriction resulting class language universal computer thus suffer potential undecidability also restricted class grammar called regular grammar hierarchy found chomsky describe nested structure parenthesis eq 17 contextfree grammar right degree complexity many purpose particular programming language formal language mathematical logic described using cfgs thus algorithm working associated theory well developed besides recognizing parsing language one describe linguistic task similar term trivial example would word replacement rule old inew realistic task benefit framework structure example use grammar eq 17 arithmetic would much better framework token value carry associated numerical symbolic value done framework attribute grammar suggest 8 llm perform natural language task using system large number rule sort extended grammatical framework mind cfgs really adequate natural language inherent ambiguity many special case exception general malism probabilistic cfg obtained associating probability distribution symbol appears left hand side rule nonterminals example might stipulate value 75 chance 42one see example english sentence wikipedia article parse tree 32to number 25 chance variable information pcfg defines probability distribution string give zero probability nongrammatical string symbolic approach parsing would propose two primary algorithm one parser given grammar input produce parse tree would algorithm learning grammar corpus since finite corpus described many grammar pcfgs better suited cfgs problem case learning parsing algorithm necessarily related connectionist approach followed llm two algorithm subsumed definition model parse pcfg whose rule encoded weight training corpus model learns particular pcfg generates corpus interpretability discussed 7 mean reversing relation extracting parser pcfg trained model reference 1 reproduced cc 40 license httpscreativecommonsorg licensesby40 2 ekin aky urek dale schuurmans jacob andreas tengyu denny zhou learning algorithm incontext learning investigation linear model november 2022 arxiv221115661 c url http arxivorgabs221115661 doi1048550arxiv221115661 3 marie amalric stanislas dehaene distinct cortical network mathematical knowledge human brain neuroimage 1891931 april 2019 url httpswwwsciencedirectcomsciencearticle piis1053811919300011 doi101016jneuroimage201901001 4 sanjeev arora boaz barak computational complexity modern approach cambridge university press 2009 5 sanjeev arora yuanzhi li yingyu liang tengyu andrej ri teski latent variable model approach pmibased word embed ding arxiv150203520 c stat june 2019 arxiv 150203520 url httparxivorgabs150203520 6 zhangir azerbayev bartosz piotrowski hailey schoelkopf edward w ayers dragomir radev jeremy avigad proofnet autoformalizing formally proving undergraduatelevel mathematics february 2023 arxiv230212433 c url httparxivorgabs230212433 doi 1048550arxiv230212433 7 sebastian bader pascal hitzler dimension neuralsymbolic tegration structured survey november 2005 arxivcs0511042 33url httparxivorgabscs0511042 doi1048550arxivcs 0511042 8 yasaman bahri ethan dyer jared kaplan jaehoon lee utkarsh sharma explaining neural scaling law february 2021 url http arxivorgabs210206701v1 9 boaz barak benjamin l edelman surbhi goel sham kakade eran malach cyril zhang hidden progress deep learning sgd learns parity near computational limit july 2022 arxiv220708799 c math stat url httparxivorgabs220708799 doi1048550 arxiv220708799 10 peter l bartlett philip long g abor lugosi alexander tsigler benign overfitting linear regression proceeding national academy science 117483006330070 2020 publisher national acad science 11 peter l bartlett andrea montanari alexander rakhlin deep learn ing statistical viewpoint arxiv210309177 c math stat march 2021 arxiv 210309177 url httparxivorgabs210309177 12 yonatan belinkov probing classifier promise shortcoming advance computational linguistics 48207219 2021 url http arxivorgabs210212452 13 mikhail belkin fit without fear remarkable mathematical phenomenon deep learning prism interpolation arxiv210514368 c math stat may 2021 arxiv 210514368 url httparxivorg abs210514368 14 mikhail belkin daniel hsu siyuan soumik man dal reconciling modern machinelearning practice classical biasvariance tradeoff proceeding national academy sci ences 116321584915854 2019 publisher national academy science eprint httpswwwpnasorgcontent1163215849fullpdf url httpswwwpnasorgcontent1163215849 doi101073 pnas1903070116 15 mikhail belkin siyuan soumik mandal understand deep learning need understand kernel learning february 2018 url httpsarxivorgabs180201396v3 16 emily bender alexander koller climbing towards nlu mean ing form understanding age data proceeding 58th annual meeting association computational linguistics page 51855198 online july 2020 association computational lin guistics url httpsaclanthologyorg2020aclmain463 doi 1018653v12020aclmain463 3417 yoshua bengio system 1 deep learning system 2 deep learning december 2019 url httpsslideslivecom38922304 fromsystem1deeplearningtosystem2deeplearning 18 yoshua bengio r ejean ducharme pascal vincent neural proba bilistic language model advance neural information processing sys tems 13 2000 19 christopher bishop nasser nasrabadi pattern recognition machine learning volume 4 springer 2006 20 taidanae bradley john terilla yiannis vlassopoulos enriched category theory language syntax semantics arxiv210607890 c math june 2021 arxiv 210607890 url httparxivorgabs 210607890 21 peter f brown stephen della pietra vincent j della pietra robert l mercer et al mathematics statistical machine translation pa rameter estimation 1993 22 tom b brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel herbertvoss gretchen krueger tom henighan rewon child aditya ramesh daniel ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christo pher berner sam mccandlish alec radford ilya sutskever dario amodei language model fewshot learner arxiv200514165 c june 2020 arxiv 200514165 url httparxivorgabs2005 14165 23 cameron b browne edward powley daniel whitehouse simon lu ca peter cowling philipp rohlfshagen stephen tavener diego perez spyridon samothrakis simon colton survey monte carlo tree search method ieee transaction computational intelligence ai game 41143 2012 24 ebastien bubeck varun chandrasekaran ronen eldan johannes gehrke eric horvitz ece kamar peter lee yin tat lee yuanzhi li scott lundberg harsha nori hamid palangi marco tulio ribeiro yi zhang spark artificial general intelligence early experi ments gpt4 march 2023 url httpsarxivorgabs2303 12712v1 25 collin burn haotian ye dan klein jacob steinhardt discovering latent knowledge language model without supervision december 2022 arxiv221203827 c url httparxivorgabs221203827 3526 yining chen sorcha gilroy andreas maletti jonathan may kevin knight recurrent neural network weighted language recognizers march 2018 arxiv171105408 c url httparxivorgabs1711 05408 doi1048550arxiv171105408 27 ethan chi john hewitt christopher manning finding uni versal grammatical relation multilingual bert arxiv200504511 c may 2020 arxiv 200504511 url httparxivorgabs2005 04511 28 david chiang peter cholak anand pillay tighter bound expressivity transformer encoders may 2023 arxiv230110743 c url httparxivorgabs230110743 doi1048550arxiv 230110743 29 ted chiang chatgpt blurry jpeg web new yorker february 2023 30 rewon child scott gray alec radford ilya sutskever generating long sequence sparse transformer april 2019 url http arxivorgabs190410509v1 31 anna choromanska mikael henaff michael mathieu g erard ben arous yann lecun loss surface multilayer network january 2015 arxiv14120233 c url httparxivorgabs14120233 doi1048550arxiv14120233 32 aakanksha chowdhery sharan narang jacob devlin et al palm scal ing language modeling pathway arxiv220402311 c april 2022 arxiv 220402311 url httparxivorgabs220402311 33 bilal chughtai lawrence chan neel nanda toy model uni versality reverse engineering network learn group operation may 2023 arxiv230203025 c math url httparxivorgabs 230203025 doi1048550arxiv230203025 34 bob coecke mehrnoosh sadrzadeh stephen clark mathematical foundation compositional distributional model meaning march 2010 arxiv10034394 c math url httparxivorgabs1003 4394 doi1048550arxiv10034394 35 taco cohen max welling group equivariant convolutional net work international conference machine learning page 29902999 pmlr 2016 arxiv160207576 36 r emi coulom efficient selectivity backup operator montecarlo tree search international conference computer game page 7283 springer 2006 3637 francis crick recent excitement neural network nature 337129132 1989 38 george v cybenko approximation superposition sigmoidal function mathematics control signal system 2303314 1989 39 jacob devlin mingwei chang kenton lee kristina toutanova bert pretraining deep bidirectional transformer language un derstanding october 2018 arxiv 181004805 url httpsarxiv orgabs181004805v1 40 ronald devore boris hanin guergana petrova neural network approximation arxiv201214501 c math december 2020 arxiv 201214501 url httparxivorgabs201214501 41 benjamin l edelman surbhi goel sham kakade cyril zhang inductive bias variable creation selfattention mechanism arxiv211010090 c stat october 2021 arxiv 211010090 url httparxivorgabs211010090 42 nelson elhage tristan hume catherine olsson nicholas schiefer tom henighan shauna kravec zac hatfielddodds robert lasenby dawn drain carol chen roger grosse sam mccandlish jared kaplan dario amodei martin wattenberg christopher olah toy model superposition september 2022 arxiv220910652 c url http arxivorgabs220910652 doi1048550arxiv220910652 43 philip feldman james r foulds shimei pan trapping llm hal lucinations using tagged context prompt june 2023 arxiv230606085 c url httparxivorgabs230606085 doi1048550arxiv 230606085 44 john rupert firth study linguistic analysis wileyblackwell 1957 45 jerry fodor modularity mind mit press 1983 46 dan friedman alexander wettig danqi chen learning transformer program june 2023 arxiv230601128 c url httparxivorg abs230601128 doi1048550arxiv230601128 47 artur davila garcez luis c lamb neurosymbolic ai 3rd wave december 2020 arxiv201205876 c url httparxivorg abs201205876 48 shivam garg dimitris tsipras percy liang gregory valiant transformer learn incontext case study simple function class january 2023 arxiv220801066 c url httparxivorg abs220801066 doi1048550arxiv220801066 3749 ian goodfellow yoshua bengio aaron courville deep learning mit press 2016 50 anirudh goyal yoshua bengio inductive bias deep learn ing higherlevel cognition august 2022 arxiv201115091 c stat url httparxivorgabs201115091 doi1048550arxiv 201115091 51 andrey gromov grokking modular arithmetic january 2023 arxiv230102679 condmat url httparxivorgabs2301 02679 doi1048550arxiv230102679 52 riccardo guidotti anna monreale salvatore ruggieri franco turini fosca giannotti dino pedreschi survey method explain ing black box model acm computing survey 515142 september 2019 url httpsdlacmorgdoi1011453236009 doi101145 3236009 53 thilo hagendorff machine psychology investigating emergent capabil ities behavior large language model using psychological meth od april 2023 arxiv230313988 c url httparxivorgabs 230313988 doi1048550arxiv230313988 54 michael hahn navin goyal theory emergent incontext learning implicit structure induction march 2023 arxiv230307971 c url httparxivorgabs230307971 doi1048550arxiv 230307971 55 danny hernandez jared kaplan tom henighan sam mccan dlish scaling law transfer february 2021 arxiv210201293 c url httparxivorgabs210201293 doi1048550arxiv 210201293 56 john hewitt christopher manning structural probe finding syntax word representation page 10 2019 57 sepp hochreiter j urgen schmidhuber long shortterm memory neu ral computation 9817351780 1997 58 jordan hoffmann sebastian borgeaud arthur mensch elena buchatskaya trevor cai eliza rutherford diego de la casas lisa anne hendricks johannes welbl aidan clark tom hennigan eric noland katie millican george van den driessche bogdan damoc aurelia guy simon osindero karen simonyan erich elsen jack w rae oriol vinyals laurent sifre training computeoptimal large language model march 2022 arxiv220315556 c url httparxivorg abs220315556 doi1048550arxiv220315556 3859 benjamin hoover yuchen liang bao pham rameswar panda hendrik strobelt duen horng chau mohammed j zaki dmitry krotov energy transformer february 2023 arxiv230207253 condmat qbio stat url httparxivorgabs230207253 doi1048550arxiv 230207253 60 fenghsiung hsu behind deep blue building computer defeated world chess champion princeton university press 2002 61 myeongjun jang thomas lukasiewicz consistency analysis chat gpt march 2023 arxiv230306273 c url httparxivorgabs 230306273 doi1048550arxiv230306273 62 albert q jiang sean welleck jin peng zhou wenda li jiacheng liu mateja jamnik timoth ee lacroix yuhuai wu guillaume lample draft sketch prove guiding formal theorem provers informal proof november 2022 arxiv221012283 c url httparxivorg abs221012283 doi1048550arxiv221012283 63 iain johnstone high dimensional statistical inference ran dom matrix november 2006 url httpsarxivorgabsmath 0611589v1 64 dan jurafsky james h martin speech language processing 2009 65 saurav kadavath tom conerly amanda askell tom henighan dawn drain ethan perez nicholas schiefer zac hatfield dodds nova da sarma eli tranjohnson scott johnston sheer elshowk andy jones nelson elhage tristan hume anna chen yuntao bai sam bowman stanislav fort deep ganguli danny hernandez josh jacobson jackson kernion shauna kravec liane lovitt kamal ndousse catherine olsson sam ringer dario amodei tom brown jack clark nicholas joseph ben mann sam mccandlish chris olah jared kaplan language mod el mostly know know july 2022 arxiv220705221 c url httparxivorgabs220705221 66 daniel kahneman fast slow thinking allen lane penguin book new york 2011 67 jared kaplan sam mccandlish tom henighan tom b brown ben jamin chess rewon child scott gray alec radford jeffrey wu dario amodei scaling law neural language model january 2020 arxiv200108361 c stat url httparxivorgabs200108361 doi1048550arxiv200108361 68 michael j kearns umesh vazirani introduction computational learning theory mit press 1994 69 daphne koller nir friedman probabilistic graphical model princi ples technique mit press 2009 3970 alex krizhevsky ilya sutskever geoffrey e hinton imagenet classi fication deep convolutional neural network communication acm 6068490 2017 71 florent krzakala federico riccitersenghi lenka zdeborova eric w tramel riccardo zecchina leticia f cugliandolo statistical physic optimization inference messagepassing algorithm lecture note le houches school physic special issue october 2013 num ber 2013 oxford university press 2016 72 b lake r salakhutdinov j b tenenbaum humanlevel concept learning probabilistic program induction science 350626613321338 december 2015 url httpswwwsciencemag orglookupdoi101126scienceaab3050 doi101126science aab3050 73 brenden lake tomer ullman joshua b tenenbaum samuel j gershman building machine learn think like people april 2016 url httparxivorgabs160400289 74 yann lecun popular talk private discussion 2015 75 yann lecun path towards autonomous machine intelligence 2022 url httpsopenreviewnetforumidbz5a1rkvsf 76 yann lecun yoshua bengio geoffrey hinton deep learning nature 521436444 2015 77 aitor lewkowycz anders andreassen david dohan ethan dyer henryk michalewski vinay ramasesh ambrose slone cem anil imanol schlag theo gutmansolo yuhuai wu behnam neyshabur guy gurari vedant misra solving quantitative reasoning problem lan guage model june 2022 number arxiv220614858 arxiv220614858 c url httparxivorgabs220614858 doi1048550arxiv 220614858 78 kenneth li aspen k hopkins david bau fernanda vi egas hanspeter pfister martin wattenberg emergent world representation ex ploring sequence model trained synthetic task february 2023 arxiv221013382 c url httparxivorgabs221013382 doi 1048550arxiv221013382 79 kenneth li oam patel fernanda vi egas hanspeter pfister mar tin wattenberg inferencetime intervention eliciting truthful answer language model june 2023 arxiv230603341 c url http arxivorgabs230603341 doi1048550arxiv230603341 80 percy liang rishi bommasani tony lee dimitris tsipras dilara soylu michihiro yasunaga yian zhang deepak narayanan yuhuai 40wu ananya kumar benjamin newman binhang yuan bobby yan ce zhang christian cosgrove christopher manning christopher r e diana acostanavas drew hudson eric zelikman esin durmus faisal ladhak frieda rong hongyu ren huaxiu yao jue wang keshav santhanam laurel orr lucia zheng mert yuksekgonul mirac suzgun nathan kim neel guha niladri chatterji omar khattab peter hender son qian huang ryan chi sang michael xie shibani santurkar surya ganguli tatsunori hashimoto thomas icard tianyi zhang vishrav chaudhary william wang xuechen li yifan mai yuhui zhang yuta koreeda holistic evaluation language model november 2022 arxiv221109110 c url httparxivorgabs221109110 doi 1048550arxiv221109110 81 hunter lightman vineet kosaraju yura burda harri edward bowen baker teddy lee jan leike john schulman ilya sutskever karl cobbe let verify step step may 2023 arxiv230520050 c url httparxivorgabs230520050 doi1048550arxiv 230520050 82 bingbin liu jordan ash surbhi goel akshay krishnamurthy cyril zhang transformer learn shortcut automaton october 2022 arxiv221010749 c stat url httparxivorgabs221010749 83 david jc mackay information theory inference learning algorithm cambridge university press 2003 84 kyle mahowald anna ivanova idan blank nancy kanwisher joshua b tenenbaum evelina fedorenko dissociating language thought large language model cognitive perspective january 2023 arxiv230106627 c url httparxivorgabs230106627 doi1048550arxiv230106627 85 alexander maloney daniel robert james sully solvable model neural scaling law october 2022 arxiv221016859 hepth stat url httparxivorgabs221016859 doi1048550arxiv 221016859 86 yuri manin matilde marcolli semantic space arxivorg may 2016 arxiv 160504238v1 url httparxivorgabs160504238v1 87 christopher manning hinrich schutze foundation statistical nat ural language processing mit press 1999 88 christopher manning kevin clark john hewitt urvashi khandelwal omer levy emergent linguistic structure artificial neural net work trained selfsupervision proceeding national academy science 117483004630054 2020 4189 matilde marcolli noam chomsky robert berwick mathe matical structure syntactic merge may 2023 arxiv230518278 c math url httparxivorgabs230518278 doi1048550 arxiv230518278 90 mitchell marcus beatrice santorini mary ann marcinkiewicz build ing large annotated corpus english penn treebank 1993 91 david marr vision computational investigation human rep resentation processing visual information mit press 2010 92 jir ˇi matouˇ sek lecture note metric embeddings 2013 url http kammffcunicz matousekbaa4pdf 93 pamela mccorduck cli cfe machine think personal inquiry history prospect artificial intelligence crc press 2004 94 william merrill linguistic capacity realtime counter au tomata arxiv200406866 c april 2020 arxiv 200406866 url httparxivorgabs200406866 95 william merrill ashish sabharwal parallelism tradeoff lim itations logprecision transformer april 2023 arxiv220700729 c url httparxivorgabs220700729 doi1048550arxiv 220700729 96 william merrill ashish sabharwal noah smith saturated trans former constantdepth threshold circuit arxiv210616213 c april 2022 arxiv 210616213 url httparxivorgabs2106 16213 97 marc mezard andrea montanari information physic compu tation oxford university press 2009 98 eric j michaud ziming liu uzay girit max tegmark quan tization model neural scaling march 2023 arxiv230313506 cond mat url httparxivorgabs230313506 doi1048550arxiv 230313506 99 tomas mikolov kai chen greg corrado jeffrey dean efficient estimation word representation vector space september 2013 arxiv13013781 c url httparxivorgabs13013781 100 marvin minsky society mind simon schuster 1988 101 david mumford pattern theory mathematics perception arxiv preprint math0212400 2002 102 david mumford agn e desolneux pattern theory stochastic analysis realworld signal crc press 2010 42103 neel nanda lawrence chan tom lieberum jess smith jacob stein hardt progress measure grokking via mechanistic interpretability january 2023 arxiv230105217 c url httparxivorgabs 230105217 doi1048550arxiv230105217 104 allen newell john clifford shaw herbert simon empirical explo ration logic theory machine case study heuristic paper presented february 2628 1957 western joint computer conference technique reliability page 218230 1957 105 nil j nilsson quest artificial intelligence cambridge university press 2009 106 chris olah mechanistic interpretability variable importance interpretable base 2022 url httpstransformercircuitspub 2022mechinterpessayindexhtml 107 catherine olsson nelson elhage neel nanda nicholas joseph nova da sarma tom henighan ben mann amanda askell yuntao bai anna chen tom conerly dawn drain deep ganguli zac hatfielddodds danny hernandez scott johnston andy jones jackson kernion liane lovitt kamal ndousse dario amodei tom brown jack clark jared kaplan sam mccandlish chris olah incontext learning induction head september 2022 arxiv220911895 c url http arxivorgabs220911895 doi1048550arxiv220911895 108 jeffrey pennington richard socher christopher manning glove global vector word representation proceeding 2014 con ference empirical method natural language processing emnlp page 15321543 doha qatar october 2014 association com putational linguistics url httpswwwaclweborganthology d141162 doi103115v1d141162 109 mary phuong marcus hutter formal algorithm transformer july 2022 arxiv220709238 c url httparxivorgabs2207 09238 110 alethea power yuri burda harri edward igor babuschkin vedant misra grokking generalization beyond overfitting small algorith mic datasets arxiv220102177 c january 2022 arxiv 220102177 url httparxivorgabs220102177 111 ofir press muru zhang sewon min ludwig schmidt noah smith mike lewis measuring narrowing compositionality gap language model october 2022 arxiv221003350 c url http arxivorgabs221003350 doi1048550arxiv221003350 112 alec radford karthik narasimhan tim salimans ilya sutskever improving language understanding generative pretraining 2018 pub lisher openai 43113 alec radford jeff wu rewon child luan dario amodei ilya sutskever language model unsupervised multitask learner undefined 2019 url httpswwwsemanticscholarorgpaper languagemodelsareunsupervisedmultitasklearnersradfordwu 9405cc0d6169988371b2755e573cc28650d14dfe 114 colin raffel noam shazeer adam robert katherine lee sharan narang michael matena yanqi zhou wei li peter j liu explor ing limit transfer learning unified texttotext trans former arxiv191010683 c stat july 2020 arxiv 191010683 url httparxivorgabs191010683 115 hubert ramsauer bernhard sch afl johannes lehner philipp seidl michael widrich thomas adler lukas gruber markus holzleitner milena pavlovi c geir kjetil sandve victor greiff david kreil michael kopp g unter klambauer johannes brandstetter sepp hochreiter hopfield network need april 2021 arxiv200802217 c stat url httparxivorgabs200802217 doi1048550arxiv 200802217 116 daniel robert sho yaida boris hanin principle deep learning theory arxiv210610165 hepth stat august 2021 arxiv 210610165 url httparxivorgabs210610165 117 frank rosenblatt perceptron probabilistic model information storage organization brain psychological review 656386 1958 118 david e rumelhart geoffrey e hinton james l mcclelland general framework parallel distributed processing 1986 119 stuart j russell artificial intelligence modern approach pearson ed ucation inc 2010 120 rylan schaeffer brando miranda oluwasanmi koyejo emergent ability large language model mirage arxiv abs230415004 2023 121 terrence j sejnowski deep learning revolution mit press 2018 122 claude e shannon xxii programming computer playing chess london edinburgh dublin philosophical magazine journal science 41314256275 1950 123 hava siegelmann eduardo sontag computational power neural net proceeding fifth annual workshop computa tional learning theory page 440449 1992 124 brian cantwell smith procedural reflection programming language volume 1982 44125 jascha sohldickstein eric wei niru maheswaranathan surya ganguli deep unsupervised learning using nonequilibrium thermodynam ic international conference machine learning page 22562265 pmlr 2015 arxiv150303585 126 ben sorscher robert geirhos shashank shekhar surya ganguli ari morcos beyond neural scaling law beating power law scaling via data pruning june 2022 number arxiv220614486 arxiv220614486 c stat url httparxivorgabs220614486 doi1048550 arxiv220614486 127 aarohi srivastava abhinav rastogi abhishek rao et al beyond imitation game quantifying extrapolating capability lan guage model technical report arxiv220604615 arxiv june 2022 arxiv220604615 c stat type article url httparxivorgabs 220604615 128 richard sutton bitter lesson 2019 url httpwww incompleteideasnetincideasbitterlessonhtml 129 christian szegedy promising path towards autoformalization gen eral artificial intelligence international conference intelligent com puter mathematics 2020 130 shubham toshniwal sam wiseman karen livescu kevin gim pel chess testbed language model state tracking may 2022 arxiv210213249 c url httparxivorgabs210213249 doi1048550arxiv210213249 131 richard e turner introduction transformer july 2023 arxiv230410557 c url httparxivorgabs230410557 doi 1048550arxiv230410557 132 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin atten tion need june 2017 arxiv 170603762 url http arxivorgabs170603762 133 jason wei yi tay rishi bommasani colin raffel barret zoph sebas tian borgeaud dani yogatama maarten bosma denny zhou donald metzler ed h chi tatsunori hashimoto oriol vinyals percy liang jeff dean william fedus emergent ability large language model 2022 publisher arxiv version number 2 url http arxivorgabs220607682 doi1048550arxiv220607682 134 gail wei yoav goldberg eran yahav practical compu tational power finite precision rnns language recognition may 2018 arxiv180504908 c stat url httparxivorgabs1805 04908 doi1048550arxiv180504908 45135 sean welleck jiacheng liu ronan le bra hannaneh hajishirzi yejin choi kyunghyun cho naturalproofs mathematical theorem prov ing natural language page 14 2021 136 noam wy yoav levine amnon shashua learnability context learning march 2023 arxiv230307895 c url http arxivorgabs230307895 doi1048550arxiv230307895 137 avi wigderson mathematics computation theory revolutionizing technology science princeton university press 2019 138 wikipedia url httpsenwikipediaorgwikireflective_ programming 139 sang michael xie aditi raghunathan percy liang tengyu explanation incontext learning implicit bayesian inference july 2022 arxiv211102080 c url httparxivorgabs211102080 doi1048550arxiv211102080 140 greg yang edward j hu igor babuschkin szymon sidor xiaodong liu david farhi nick ryder jakub pachocki weizhu chen jian feng gao tensor program v tuning large neural network via zero shot hyperparameter transfer march 2022 arxiv220303466 cond mat url httparxivorgabs220303466 doi1048550arxiv 220303466 141 shunyu yao dian yu jeffrey zhao izhak shafran thomas l griffith yuan cao karthik narasimhan tree thought deliberate prob lem solving large language model may 2023 arxiv230510601 c url httparxivorgabs230510601 doi1048550arxiv 230510601 142 yuhui zhang michihiro yasunaga zhengping zhou jeff z haochen james zou percy liang serena yeung beyond positive scaling negation impact scaling trend language model may 2023 arxiv230517311 c url httparxivorgabs230517311 doi 1048550arxiv230517311 143 haoyu zhao abhishek panigrahi rong ge sanjeev arora transformer parse predicting masked word march 2023 arxiv230308117 c url httparxivorgabs230308117 doi 1048550arxiv230308117 144 kunhao zheng jesse michael han stanislas polu minif2f cross system benchmark formal olympiadlevel mathematics february 2022 arxiv210900110 c url httparxivorgabs210900110 doi1048550arxiv210900110 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\soulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the cleaned words back into a sentence\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text)\n",
    "# print(\"Original Text:\")\n",
    "# print(original_text)\n",
    "print(\"\\nCleaned Text:\")\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\soulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15271a050f064de48a950096b214dea6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2305bbf6353d400db7db64d522359ecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2eab49022484b4a983bd35e7767a30f",
      "max": 2169,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6391971bfa8a43c28e51af0a5c65bb17",
      "value": 2169
     }
    },
    "266cde849a2147c2b546c41b5bdc32e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba620322118c4583bfc296331ad66e5b",
      "placeholder": "​",
      "style": "IPY_MODEL_47d33f4f0f2841d0892fc3dab185e7d1",
      "value": "Downloading builder script: "
     }
    },
    "26f780177576466692fa9adb33315d85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37237174db9746ce936837e3e0497167": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dbd980a30b34221b4edcf1ba336910c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46cfab302bff434ab00f0129ebf6bd9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7a1b419379846efa61e63aa085cff9b",
      "max": 14732,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0653a62c15842fa927db9b03eaefdd1",
      "value": 14732
     }
    },
    "47d33f4f0f2841d0892fc3dab185e7d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5cdee988303d4c51943d5b5d2ecd9b34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_266cde849a2147c2b546c41b5bdc32e1",
       "IPY_MODEL_2305bbf6353d400db7db64d522359ecc",
       "IPY_MODEL_841fa1c9ffa548baabc6cf8d795380d4"
      ],
      "layout": "IPY_MODEL_8444df609b094a3eab311992b679ecd7"
     }
    },
    "6391971bfa8a43c28e51af0a5c65bb17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7cef02372fad494ab9d983d57a748540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7dd8482301664be296dfdeecd0d5f417": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "841fa1c9ffa548baabc6cf8d795380d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15271a050f064de48a950096b214dea6",
      "placeholder": "​",
      "style": "IPY_MODEL_7dd8482301664be296dfdeecd0d5f417",
      "value": " 5.65k/? [00:00&lt;00:00, 260kB/s]"
     }
    },
    "8444df609b094a3eab311992b679ecd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d1f58fad61748729bb19e31f165d3b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37237174db9746ce936837e3e0497167",
      "placeholder": "​",
      "style": "IPY_MODEL_7cef02372fad494ab9d983d57a748540",
      "value": " 14732/14732 [00:07&lt;00:00, 2149.22 examples/s]"
     }
    },
    "91aa5ad0699a435e815b9a522722fdab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26f780177576466692fa9adb33315d85",
      "placeholder": "​",
      "style": "IPY_MODEL_3dbd980a30b34221b4edcf1ba336910c",
      "value": "Map: 100%"
     }
    },
    "a0653a62c15842fa927db9b03eaefdd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba620322118c4583bfc296331ad66e5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2eab49022484b4a983bd35e7767a30f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de297ffe3a784fed9d29135393ceed65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91aa5ad0699a435e815b9a522722fdab",
       "IPY_MODEL_46cfab302bff434ab00f0129ebf6bd9a",
       "IPY_MODEL_8d1f58fad61748729bb19e31f165d3b1"
      ],
      "layout": "IPY_MODEL_eea4e9901cc14628940789a21931abcb"
     }
    },
    "eea4e9901cc14628940789a21931abcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7a1b419379846efa61e63aa085cff9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
