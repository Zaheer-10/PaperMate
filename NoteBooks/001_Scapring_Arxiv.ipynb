{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97999784-6a7f-469c-bb9a-66f24e3ac430",
   "metadata": {},
   "source": [
    "<center>Notebook - 001 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13414a2-9f9c-4dd7-9fb4-dd381008c2bb",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>SCRAPING THE ARXIV DATA</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b0c4b-4b45-4ddc-95ac-aeb7c7f0dc4d",
   "metadata": {},
   "source": [
    "<center><h4>This notebook scrapes the arXiv website for papers in the category \"cs.CV\" (Computer Vision) ,\"stat.ML\" / \"cs.LG\" (Machine Learning) and \"cs.AI\" (Artificial Intelligence). The papers are then saved in a csv file.</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d57b43-fae4-4e1b-902f-801286de014c",
   "metadata": {},
   "source": [
    "<center>\n",
    "        <img src=\"https://1.bp.blogspot.com/-qNgnU6Fb4mQ/YNYg4YdWyaI/AAAAAAAAV04/Bbx5Ez0Iz_4PFOpFxuL2bPMrfLqFHF_rgCLcBGAsYHQ/s791/Data%2BScraping%2Bseminar%2Btopics.jpg\" alt=\"Your Image\">\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045bb9a5-78ce-46c8-b14f-387aca99c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.8)\n",
      "Requirement already satisfied: feedparser in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from feedparser->arxiv) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82329cd8-6450-4a30-a84c-3fd56c3a2690",
   "metadata": {},
   "source": [
    "<h2>Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5913368-6453-4189-b037-9fc3786734c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cbeb3c-b649-4dd0-b5ac-5dca71f1b602",
   "metadata": {},
   "source": [
    "##### Assigning the path to the data directory that is one level above the current working directory to the variable PATH_DATA_BASE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479e78a9-6df6-4d0f-8394-fe8781489326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soulo\\MACHINE_LEARNING\\PaperMate\\data\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA_BASE = Path.cwd().parent / \"data\"\n",
    "print(PATH_DATA_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33865cf2-4f79-471f-bd0b-4075ab0ae9e6",
   "metadata": {},
   "source": [
    "## Scraping the arXiv website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b0a70-4fd2-44ee-845b-da8a8f3c7464",
   "metadata": {},
   "source": [
    "<p>Defining a list of keywords that we will use to query the arXiv API.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e633ea-998a-4c5e-aad2-808ef70a84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_keywords = [\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"self-supervised learning\\\"\",\n",
    "    \"\\\"representation learning\\\"\",\n",
    "    \"\\\"image generation\\\"\",\n",
    "    \"\\\"object detection\\\"\",\n",
    "    \"\\\"transfer learning\\\"\",\n",
    "    \"\\\"transformers\\\"\",\n",
    "    \"\\\"adversarial training\\\"\",\n",
    "    \"\\\"generative adversarial networks\\\"\",\n",
    "    \"\\\"model compression\\\"\",\n",
    "    \"\\\"few-shot learning\\\"\",\n",
    "    \"\\\"natural language processing\\\"\",\n",
    "    \"\\\"graph neural networks\\\"\",\n",
    "    \"\\\"colorization\\\"\",\n",
    "    \"\\\"depth estimation\\\"\",\n",
    "    \"\\\"point cloud\\\"\",\n",
    "    \"\\\"structured data\\\"\",\n",
    "    \"\\\"optical flow\\\"\",\n",
    "    \"\\\"reinforcement learning\\\"\",\n",
    "    \"\\\"super resolution\\\"\",\n",
    "    \"\\\"attention mechanisms\\\"\",\n",
    "    \"\\\"tabular data\\\"\",\n",
    "    \"\\\"unsupervised learning\\\"\",\n",
    "    \"\\\"semi-supervised learning\\\"\",\n",
    "    \"\\\"explainable AI\\\"\",\n",
    "    \"\\\"radiance field\\\"\",\n",
    "    \"\\\"decision tree\\\"\",\n",
    "    \"\\\"time series analysis\\\"\",\n",
    "    \"\\\"molecule generation\\\"\",\n",
    "    \"\\\"large language models\\\"\",\n",
    "    \"\\\"LLMs\\\"\",\n",
    "    \"\\\"language models\\\"\",\n",
    "    \"\\\"image classification\\\"\",\n",
    "    \"\\\"document image classification\\\"\",\n",
    "    \"\\\"encoder-decoder\\\"\",\n",
    "    \"\\\"multimodal learning\\\"\",\n",
    "    \"\\\"multimodal deep learning\\\"\",\n",
    "    \"\\\"speech recognition\\\"\",\n",
    "    \"\\\"generative models\\\"\",\n",
    "    \"\\\"anomaly detection\\\"\",\n",
    "    \"\\\"recommender systems\\\"\",\n",
    "    \"\\\"robotics\\\"\",\n",
    "    \"\\\"knowledge graphs\\\"\",\n",
    "    \"\\\"cross-modal learning\\\"\",\n",
    "    \"\\\"attention mechanisms\\\"\",\n",
    "    \"\\\"unsupervised translation\\\"\",\n",
    "    \"\\\"machine translation\\\"\",\n",
    "    \"\\\"dialogue systems\\\"\",\n",
    "    \"\\\"sentiment analysis\\\"\",\n",
    "    \"\\\"question answering\\\"\",\n",
    "    \"\\\"text summarization\\\"\",\n",
    "    \"\\\"sequential modeling\\\"\",\n",
    "    \"\\\"neurosymbolic AI\\\"\",\n",
    "    \"\\\"fairness in AI\\\"\",\n",
    "    \"\\\"transferable skills\\\"\",\n",
    "    \"\\\"data augmentation\\\"\",\n",
    "    \"\\\"neural architecture search\\\"\",\n",
    "    \"\\\"active learning\\\"\",\n",
    "    \"\\\"automated machine learning\\\"\",\n",
    "    \"\\\"meta-learning\\\"\",\n",
    "    \"\\\"domain adaptation\\\"\",\n",
    "    \"\\\"time series forecasting\\\"\",\n",
    "    \"\\\"weakly supervised learning\\\"\",\n",
    "    \"\\\"self-supervised vision\\\"\",\n",
    "    \"\\\"visual reasoning\\\"\",\n",
    "    \"\\\"knowledge distillation\\\"\",\n",
    "    \"\\\"hyperparameter optimization\\\"\",\n",
    "    \"\\\"cross-validation\\\"\",\n",
    "    \"\\\"explainable reinforcement learning\\\"\",\n",
    "    \"\\\"meta-reinforcement learning\\\"\",\n",
    "    \"\\\"generative models in NLP\\\"\",\n",
    "    \"\\\"knowledge representation and reasoning\\\"\",\n",
    "    \"\\\"zero-shot learning\\\"\",\n",
    "    \"\\\"self-attention mechanisms\\\"\",\n",
    "    \"\\\"ensemble learning\\\"\",\n",
    "    \"\\\"online learning\\\"\",\n",
    "    \"\\\"cognitive computing\\\"\",\n",
    "    \"\\\"self-driving cars\\\"\",\n",
    "    \"\\\"emerging AI trends\\\"\",\n",
    "    \"\\\"Attention is all you need\\\"\",\n",
    "    \"\\\"GPT\\\"\",\n",
    "    \"\\\"BERT\\\"\",\n",
    "    \"\\\"Transformers\\\"\",\n",
    "    \"\\\"yolo\\\"\",\n",
    "    \"\\\"speech recognisation\\\"\",\n",
    "    \"\\\"LSTM\\\"\",\n",
    "    \"\\\"GRU\\\"\",\n",
    "    \"\\\"BERT - Bidirectinal Encoder Representation of Transformes\\\"\",\n",
    "    \"\\\"Large Language Model\\\" \",\n",
    "    \"\\\"Stabel diffusion\\\"\",\n",
    "    \"\\\"Attention is all you need\\\"\",\n",
    "    \"\\\"Encoder-Decoder\\\"\",\n",
    "     \"\\\"Paper Recommendatin systems\\\"\",\n",
    "     \"\\\" Latent Dirichlet Allocation (LDA)\\\"\",\n",
    "     \"\\\"Transformers\\\"\",\n",
    "     \"\\\"Generative Pre-trained Transforme\\\"\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae4b88-9a12-4d78-a504-3fcff5d2aa70",
   "metadata": {},
   "source": [
    "<p>Afterwards, we define a function that creates a search object using the given query. It sets the maximum number of results for each category to 6000 and sorts them by the last updated date. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa152eb-5598-41d4-883a-4accad375efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client(num_retries=20, page_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8726231-bdbc-4d47-b7ed-a40f56c53013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def query_with_keywords(query) -> tuple:\n",
    "    \"\"\"\n",
    "    Query the arXiv API for research papers based on a specific query and filter results by selected categories.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to be used for fetching research papers from arXiv.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists - terms, titles, and abstracts of the filtered research papers.\n",
    "        \n",
    "            terms (list): A list of lists, where each inner list contains the categories associated with a research paper.\n",
    "            titles (list): A list of titles of the research papers.\n",
    "            abstracts (list): A list of abstracts (summaries) of the research papers.\n",
    "            urls (list): A list of URLs for the papers' detail page on the arXiv website.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a search object with the query and sorting parameters.\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=6000,\n",
    "        sort_by=arxiv.SortCriterion.LastUpdatedDate\n",
    "    )\n",
    "    \n",
    "    # Initialize empty lists for terms, titles, abstracts, and urls.\n",
    "    terms = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    urls = []\n",
    "    ids = []\n",
    "    # For each result in the search...\n",
    "    for res in tqdm(client.results(search), desc=query):\n",
    "        # Check if the primary category of the result is in the specified list.\n",
    "        if res.primary_category in [\"cs.CV\", \"stat.ML\", \"cs.LG\", \"cs.AI\" ,\"cs.CL\"]:\n",
    "            # If it is, append the result's categories, title, summary, and url to their respective lists.\n",
    "            terms.append(res.categories)\n",
    "            titles.append(res.title)\n",
    "            abstracts.append(res.summary)\n",
    "            urls.append(res.entry_id)\n",
    "            ids.append(res.entry_id.split('/')[-1])\n",
    "\n",
    "    # Return the four lists.\n",
    "    return terms, titles, abstracts, urls , ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803be591-ec40-4f68-96d7-0466eedaaf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"image segmentation\": 3322it [01:27, 37.75it/s]\n",
      "\"self-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"representation learning\": 6000it [02:42, 36.85it/s]\n",
      "\"image generation\": 2580it [01:09, 36.98it/s]\n",
      "\"object detection\": 6000it [02:56, 34.08it/s]\n",
      "\"transfer learning\": 5642it [02:38, 35.55it/s]\n",
      "\"transformers\": 6000it [02:39, 37.65it/s]\n",
      "\"adversarial training\": 2855it [01:12, 39.43it/s]\n",
      "\"generative adversarial networks\": 6000it [02:13, 45.09it/s]\n",
      "\"model compression\": 800it [00:26, 30.71it/s]\n",
      "\"few-shot learning\": 0it [00:03, ?it/s]\n",
      "\"natural language processing\": 6000it [02:50, 35.25it/s]\n",
      "\"graph neural networks\": 5095it [02:33, 33.27it/s]\n",
      "\"colorization\": 6000it [02:22, 42.04it/s]\n",
      "\"depth estimation\": 1390it [00:35, 38.89it/s]\n",
      "\"point cloud\": 5070it [02:17, 36.77it/s]\n",
      "\"structured data\": 2110it [01:16, 27.47it/s]\n",
      "\"optical flow\": 1656it [00:45, 36.54it/s]\n",
      "\"reinforcement learning\": 6000it [02:28, 40.30it/s]\n",
      "\"super resolution\": 3237it [01:13, 44.33it/s]\n",
      "\"attention mechanisms\": 5512it [02:27, 37.32it/s]\n",
      "\"tabular data\": 619it [00:20, 29.57it/s]\n",
      "\"unsupervised learning\": 2955it [01:05, 44.95it/s]\n",
      "\"semi-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"explainable AI\": 798it [00:17, 45.82it/s]\n",
      "\"radiance field\": 736it [00:24, 30.16it/s]\n",
      "\"decision tree\": 2720it [01:14, 36.73it/s]\n",
      "\"time series analysis\": 1111it [00:23, 46.89it/s]\n",
      "\"molecule generation\": 147it [00:05, 28.18it/s]\n",
      "\"large language models\": 3036it [01:25, 35.43it/s]\n",
      "\"LLMs\": 2021it [00:57, 34.95it/s]\n",
      "\"language models\": 6000it [02:40, 37.45it/s]\n",
      "\"image classification\": 6000it [02:38, 37.97it/s]\n",
      "\"document image classification\": 20it [00:03,  5.11it/s]\n",
      "\"encoder-decoder\": 0it [00:03, ?it/s]\n",
      "\"multimodal learning\": 283it [00:10, 28.03it/s]\n",
      "\"multimodal deep learning\": 85it [00:04, 19.94it/s]\n",
      "\"speech recognition\": 4223it [01:43, 40.64it/s]\n",
      "\"generative models\": 6000it [04:13, 23.71it/s]\n",
      "\"anomaly detection\": 3368it [01:12, 46.27it/s]\n",
      "\"recommender systems\": 4550it [02:13, 34.00it/s]\n",
      "\"robotics\": 6000it [02:40, 37.42it/s]\n",
      "\"knowledge graphs\": 2858it [01:22, 34.59it/s]\n",
      "\"cross-modal learning\": 0it [00:03, ?it/s]\n",
      "\"attention mechanisms\": 5512it [03:02, 30.13it/s]\n",
      "\"unsupervised translation\": 34it [00:04,  8.01it/s]\n",
      "\"machine translation\": 4088it [01:59, 34.23it/s]\n",
      "\"dialogue systems\": 1054it [00:24, 42.54it/s]\n",
      "\"sentiment analysis\": 2116it [01:12, 29.31it/s]\n",
      "\"question answering\": 4493it [02:09, 34.82it/s]\n",
      "\"text summarization\": 572it [00:21, 27.06it/s]\n",
      "\"sequential modeling\": 355it [00:10, 33.12it/s]\n",
      "\"neurosymbolic AI\": 13it [00:03,  3.35it/s]\n",
      "\"fairness in AI\": 31it [00:04,  6.67it/s]\n",
      "\"transferable skills\": 41it [00:04,  8.29it/s]\n",
      "\"data augmentation\": 5508it [03:32, 25.97it/s]\n",
      "\"neural architecture search\": 1298it [00:39, 32.82it/s]\n",
      "\"active learning\": 2412it [01:19, 30.46it/s]\n",
      "\"automated machine learning\": 324it [00:08, 40.19it/s]\n",
      "\"meta-learning\": 30it [00:04,  6.80it/s]\n",
      "\"domain adaptation\": 4117it [02:22, 28.98it/s]\n",
      "\"time series forecasting\": 762it [00:24, 31.52it/s]\n",
      "\"weakly supervised learning\": 424it [00:17, 24.05it/s]\n",
      "\"self-supervised vision\": 0it [00:04, ?it/s]\n",
      "\"visual reasoning\": 235it [00:10, 23.31it/s]\n",
      "\"knowledge distillation\": 2177it [01:14, 29.15it/s]\n",
      "\"hyperparameter optimization\": 520it [00:21, 23.89it/s]\n",
      "\"cross-validation\": 6it [00:03,  1.56it/s]\n",
      "\"explainable reinforcement learning\": 34it [00:04,  7.63it/s]\n",
      "\"meta-reinforcement learning\": 0it [00:03, ?it/s]\n",
      "\"generative models in NLP\": 0it [00:03, ?it/s]\n",
      "\"knowledge representation and reasoning\": 169it [00:14, 11.27it/s]\n",
      "\"zero-shot learning\": 0it [00:03, ?it/s]\n",
      "\"self-attention mechanisms\": 0it [00:03, ?it/s]\n",
      "\"ensemble learning\": 826it [00:30, 26.99it/s]\n",
      "\"online learning\": 2290it [01:17, 29.47it/s]\n",
      "\"cognitive computing\": 383it [00:10, 37.62it/s]\n",
      "\"self-driving cars\": 0it [00:03, ?it/s]\n",
      "\"emerging AI trends\": 0it [00:04, ?it/s]\n",
      "\"Attention is all you need\": 15it [00:04,  3.71it/s]\n",
      "\"GPT\": 613it [00:24, 25.17it/s]\n",
      "\"BERT\": 5308it [02:44, 32.34it/s]\n",
      "\"Transformers\": 6000it [03:28, 28.74it/s]\n",
      "\"yolo\": 395it [00:21, 18.75it/s]\n",
      "\"speech recognisation\": 8it [00:03,  2.07it/s]\n",
      "\"LSTM\": 5282it [02:54, 30.35it/s]\n",
      "\"GRU\": 859it [00:31, 27.06it/s]\n",
      "\"BERT - Bidirectinal Encoder Representation of Transformes\": 0it [00:03, ?it/s]\n",
      "\"Large Language Model\" : 3036it [01:34, 32.16it/s]\n",
      "\"Stabel diffusion\": 0it [00:03, ?it/s]\n",
      "\"Attention is all you need\": 15it [00:03,  3.87it/s]\n",
      "\"Encoder-Decoder\": 0it [00:03, ?it/s]\n",
      "\"Paper Recommendatin systems\": 0it [00:03, ?it/s]\n",
      "\" Latent Dirichlet Allocation (LDA)\": 246it [00:07, 34.63it/s]\n",
      "\"Transformers\": 6000it [03:03, 32.66it/s]\n",
      "\"Generative Pre-trained Transforme\": 0it [00:03, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_titles = []\n",
    "all_abstracts = []\n",
    "all_terms = []\n",
    "all_urls = []\n",
    "all_ids = []\n",
    "\n",
    "for query in query_keywords:\n",
    "    terms, titles, abstracts, urls , ids = query_with_keywords(query)\n",
    "    all_titles.extend(titles)\n",
    "    all_abstracts.extend(abstracts)\n",
    "    all_terms.extend(terms)\n",
    "    all_urls.extend(urls)\n",
    "    all_ids.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a502f2c-1a78-4e76-a10b-36dd1bca424f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee55de48-52d6-4096-8c9f-d2e0d6a2d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2307.10123v2\n"
     ]
    }
   ],
   "source": [
    "print(all_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b7d0e1-1c11-4e2f-8d0f-3cddadb5a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cs.CV'], ['cs.CV'], ['cs.CV'], ['cs.CV', 'cs.AI'], ['cs.CV'], ['cs.CV'], ['cs.LG', 'cs.CR', 'eess.IV'], ['cs.CV'], ['cs.CV'], ['cs.CV']]\n"
     ]
    }
   ],
   "source": [
    "print(all_terms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48ab9f65-abf4-4015-a9fd-5cf1bfa4b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Approaches to Supervised Image Segmentation\n"
     ]
    }
   ],
   "source": [
    "print(all_titles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bd5f9-e059-4ff5-8dc1-368e30cad5ec",
   "metadata": {},
   "source": [
    "### lets see the data scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7675dbd0-9ead-45e2-8da2-315c5013509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.DataFrame({\n",
    "    'titles': all_titles,\n",
    "    'abstracts': all_abstracts,\n",
    "    'terms': all_terms,\n",
    "    'urls': all_urls,\n",
    "    'ids':all_ids,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "274eda83-16c6-427a-8f1c-9fbfb0702db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Point2Mask: Point-supervised Panoptic Segmenta...</td>\n",
       "      <td>Weakly-supervised image segmentation has recen...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2308.01779v1</td>\n",
       "      <td>2308.01779v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two Approaches to Supervised Image Segmentation</td>\n",
       "      <td>Though performed almost effortlessly by humans...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2307.10123v2</td>\n",
       "      <td>2307.10123v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semi-Siamese Network for Robust Change Detecti...</td>\n",
       "      <td>Automatic defect detection for 3D printing pro...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2212.08583v2</td>\n",
       "      <td>2212.08583v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data-Centric Diet: Effective Multi-center Data...</td>\n",
       "      <td>This paper seeks to address the dense labeling...</td>\n",
       "      <td>[cs.CV, cs.AI]</td>\n",
       "      <td>http://arxiv.org/abs/2308.01189v1</td>\n",
       "      <td>2308.01189v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prompt-Based Tuning of Transformer Models for ...</td>\n",
       "      <td>Medical image segmentation is a vital healthca...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2305.18948v2</td>\n",
       "      <td>2305.18948v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Point2Mask: Point-supervised Panoptic Segmenta...   \n",
       "1    Two Approaches to Supervised Image Segmentation   \n",
       "2  Semi-Siamese Network for Robust Change Detecti...   \n",
       "3  Data-Centric Diet: Effective Multi-center Data...   \n",
       "4  Prompt-Based Tuning of Transformer Models for ...   \n",
       "\n",
       "                                           abstracts           terms  \\\n",
       "0  Weakly-supervised image segmentation has recen...         [cs.CV]   \n",
       "1  Though performed almost effortlessly by humans...         [cs.CV]   \n",
       "2  Automatic defect detection for 3D printing pro...         [cs.CV]   \n",
       "3  This paper seeks to address the dense labeling...  [cs.CV, cs.AI]   \n",
       "4  Medical image segmentation is a vital healthca...         [cs.CV]   \n",
       "\n",
       "                                urls           ids  \n",
       "0  http://arxiv.org/abs/2308.01779v1  2308.01779v1  \n",
       "1  http://arxiv.org/abs/2307.10123v2  2307.10123v2  \n",
       "2  http://arxiv.org/abs/2212.08583v2  2212.08583v2  \n",
       "3  http://arxiv.org/abs/2308.01189v1  2308.01189v1  \n",
       "4  http://arxiv.org/abs/2305.18948v2  2305.18948v2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71b066d0-b49c-4d95-a20a-373dce746106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135321"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35539db-ad56-4896-b492-f97550bb0ea8",
   "metadata": {},
   "source": [
    "### Save the data - Finally, we export the DataFrame to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b73a52b-a87b-4006-a6d0-b257f8052b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.to_csv(PATH_DATA_BASE / 'data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
