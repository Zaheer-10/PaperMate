{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97999784-6a7f-469c-bb9a-66f24e3ac430",
   "metadata": {},
   "source": [
    "<center>Notebook - 001 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13414a2-9f9c-4dd7-9fb4-dd381008c2bb",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>SCRAPING THE ARXIV DATA</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b0c4b-4b45-4ddc-95ac-aeb7c7f0dc4d",
   "metadata": {},
   "source": [
    "<center><h4>This notebook scrapes the arXiv website for papers in the category \"cs.CV\" (Computer Vision) ,\"stat.ML\" / \"cs.LG\" (Machine Learning) and \"cs.AI\" (Artificial Intelligence). The papers are then saved in a csv file.</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d57b43-fae4-4e1b-902f-801286de014c",
   "metadata": {},
   "source": [
    "<center>\n",
    "        <img src=\"https://1.bp.blogspot.com/-qNgnU6Fb4mQ/YNYg4YdWyaI/AAAAAAAAV04/Bbx5Ez0Iz_4PFOpFxuL2bPMrfLqFHF_rgCLcBGAsYHQ/s791/Data%2BScraping%2Bseminar%2Btopics.jpg\" alt=\"Your Image\">\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045bb9a5-78ce-46c8-b14f-387aca99c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.8)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: feedparser in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from feedparser->arxiv) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82329cd8-6450-4a30-a84c-3fd56c3a2690",
   "metadata": {},
   "source": [
    "<h2>Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5913368-6453-4189-b037-9fc3786734c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cbeb3c-b649-4dd0-b5ac-5dca71f1b602",
   "metadata": {},
   "source": [
    "##### Assigning the path to the data directory that is one level above the current working directory to the variable PATH_DATA_BASE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479e78a9-6df6-4d0f-8394-fe8781489326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soulo\\PaperMate\\data\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA_BASE = Path.cwd().parent / \"data\"\n",
    "print(PATH_DATA_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33865cf2-4f79-471f-bd0b-4075ab0ae9e6",
   "metadata": {},
   "source": [
    "## Scraping the arXiv website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b0a70-4fd2-44ee-845b-da8a8f3c7464",
   "metadata": {},
   "source": [
    "<p>Defining a list of keywords that we will use to query the arXiv API.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e633ea-998a-4c5e-aad2-808ef70a84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_keywords = [\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"self-supervised learning\\\"\",\n",
    "    \"\\\"representation learning\\\"\",\n",
    "    \"\\\"image generation\\\"\",\n",
    "    \"\\\"object detection\\\"\",\n",
    "    \"\\\"transfer learning\\\"\",\n",
    "    \"\\\"transformers\\\"\",\n",
    "    \"\\\"adversarial training\\\"\",\n",
    "    \"\\\"generative adversarial networks\\\"\",\n",
    "    \"\\\"model compression\\\"\",\n",
    "    \"\\\"few-shot learning\\\"\",\n",
    "    \"\\\"natural language processing\\\"\",\n",
    "    \"\\\"graph neural networks\\\"\",\n",
    "    \"\\\"colorization\\\"\",\n",
    "    \"\\\"depth estimation\\\"\",\n",
    "    \"\\\"point cloud\\\"\",\n",
    "    \"\\\"structured data\\\"\",\n",
    "    \"\\\"optical flow\\\"\",\n",
    "    \"\\\"reinforcement learning\\\"\",\n",
    "    \"\\\"super resolution\\\"\",\n",
    "    \"\\\"attention mechanisms\\\"\",\n",
    "    \"\\\"tabular data\\\"\",\n",
    "    \"\\\"unsupervised learning\\\"\",\n",
    "    \"\\\"semi-supervised learning\\\"\",\n",
    "    \"\\\"explainable AI\\\"\",\n",
    "    \"\\\"radiance field\\\"\",\n",
    "    \"\\\"decision tree\\\"\",\n",
    "    \"\\\"time series analysis\\\"\",\n",
    "    \"\\\"molecule generation\\\"\",\n",
    "    \"\\\"large language models\\\"\",\n",
    "    \"\\\"LLMs\\\"\",\n",
    "    \"\\\"language models\\\"\",\n",
    "    \"\\\"image classification\\\"\",\n",
    "    \"\\\"document image classification\\\"\",\n",
    "    \"\\\"encoder-decoder\\\"\",\n",
    "    \"\\\"multimodal learning\\\"\",\n",
    "    \"\\\"multimodal deep learning\\\"\",\n",
    "    \"\\\"speech recognition\\\"\",\n",
    "    \"\\\"generative models\\\"\",\n",
    "    \"\\\"anomaly detection\\\"\",\n",
    "    \"\\\"recommender systems\\\"\",\n",
    "    \"\\\"robotics\\\"\",\n",
    "    \"\\\"knowledge graphs\\\"\",\n",
    "    \"\\\"cross-modal learning\\\"\",\n",
    "    \"\\\"attention mechanisms\\\"\",\n",
    "    \"\\\"unsupervised translation\\\"\",\n",
    "    \"\\\"machine translation\\\"\",\n",
    "    \"\\\"dialogue systems\\\"\",\n",
    "    \"\\\"sentiment analysis\\\"\",\n",
    "    \"\\\"question answering\\\"\",\n",
    "    \"\\\"text summarization\\\"\",\n",
    "    \"\\\"sequential modeling\\\"\",\n",
    "    \"\\\"neurosymbolic AI\\\"\",\n",
    "    \"\\\"fairness in AI\\\"\",\n",
    "    \"\\\"transferable skills\\\"\",\n",
    "    \"\\\"data augmentation\\\"\",\n",
    "    \"\\\"neural architecture search\\\"\",\n",
    "    \"\\\"active learning\\\"\",\n",
    "    \"\\\"automated machine learning\\\"\",\n",
    "    \"\\\"meta-learning\\\"\",\n",
    "    \"\\\"domain adaptation\\\"\",\n",
    "    \"\\\"time series forecasting\\\"\",\n",
    "    \"\\\"weakly supervised learning\\\"\",\n",
    "    \"\\\"self-supervised vision\\\"\",\n",
    "    \"\\\"visual reasoning\\\"\",\n",
    "    \"\\\"knowledge distillation\\\"\",\n",
    "    \"\\\"hyperparameter optimization\\\"\",\n",
    "    \"\\\"cross-validation\\\"\",\n",
    "    \"\\\"explainable reinforcement learning\\\"\",\n",
    "    \"\\\"meta-reinforcement learning\\\"\",\n",
    "    \"\\\"generative models in NLP\\\"\",\n",
    "    \"\\\"knowledge representation and reasoning\\\"\",\n",
    "    \"\\\"zero-shot learning\\\"\",\n",
    "    \"\\\"self-attention mechanisms\\\"\",\n",
    "    \"\\\"ensemble learning\\\"\",\n",
    "    \"\\\"online learning\\\"\",\n",
    "    \"\\\"cognitive computing\\\"\",\n",
    "    \"\\\"self-driving cars\\\"\",\n",
    "    \"\\\"emerging AI trends\\\"\",\n",
    "    \"\\\"Attention is all you need\\\"\",\n",
    "    \"\\\"GPT\\\"\",\n",
    "    \"\\\"BERT\\\"\",\n",
    "    \"\\\"Transformers\\\"\",\n",
    "    \"\\\"yolo\\\"\",\n",
    "    \"\\\"speech recognisation\\\"\",\n",
    "    \"\\\"LSTM\\\"\",\n",
    "    \"\\\"GRU\\\"\",\n",
    "    \"\\\"BERT - Bidirectinal Encoder Representation of Transformes\\\"\",\n",
    "    \"\\\"Large Language Model\\\" \",\n",
    "    \"\\\"Stabel diffusion\\\"\",\n",
    "    \"\\\"Attention is all you need\\\"\",\n",
    "    \"\\\"Encoder-Decoder\\\"\",\n",
    "     \"\\\"Paper Recommendatin systems\\\"\",\n",
    "     \"\\\" Latent Dirichlet Allocation (LDA)\\\"\",\n",
    "     \"\\\"Transformers\\\"\",\n",
    "     \"\\\"Generative Pre-trained Transforme\\\"\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae4b88-9a12-4d78-a504-3fcff5d2aa70",
   "metadata": {},
   "source": [
    "<p>Afterwards, we define a function that creates a search object using the given query. It sets the maximum number of results for each category to 6000 and sorts them by the last updated date. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa152eb-5598-41d4-883a-4accad375efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client(num_retries=20, page_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8726231-bdbc-4d47-b7ed-a40f56c53013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def query_with_keywords(query) -> tuple:\n",
    "    \"\"\"\n",
    "    Query the arXiv API for research papers based on a specific query and filter results by selected categories.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to be used for fetching research papers from arXiv.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists - terms, titles, and abstracts of the filtered research papers.\n",
    "        \n",
    "            terms (list): A list of lists, where each inner list contains the categories associated with a research paper.\n",
    "            titles (list): A list of titles of the research papers.\n",
    "            abstracts (list): A list of abstracts (summaries) of the research papers.\n",
    "            urls (list): A list of URLs for the papers' detail page on the arXiv website.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a search object with the query and sorting parameters.\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=6000,\n",
    "        sort_by=arxiv.SortCriterion.LastUpdatedDate\n",
    "    )\n",
    "    \n",
    "    # Initialize empty lists for terms, titles, abstracts, and urls.\n",
    "    terms = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    urls = []\n",
    "    ids = []\n",
    "    # For each result in the search...\n",
    "    for res in tqdm(client.results(search), desc=query):\n",
    "        # Check if the primary category of the result is in the specified list.\n",
    "        if res.primary_category in [\"cs.CV\", \"stat.ML\", \"cs.LG\", \"cs.AI\" ,\"cs.CL\"]:\n",
    "            # If it is, append the result's categories, title, summary, and url to their respective lists.\n",
    "            terms.append(res.categories)\n",
    "            titles.append(res.title)\n",
    "            abstracts.append(res.summary)\n",
    "            urls.append(res.entry_id)\n",
    "            ids.append(res.entry_id.split('/')[-1])\n",
    "\n",
    "    # Return the four lists.\n",
    "    return terms, titles, abstracts, urls , ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "803be591-ec40-4f68-96d7-0466eedaaf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"image segmentation\": 3303it [01:33, 35.47it/s]\n",
      "\"self-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"representation learning\": 6000it [03:13, 30.98it/s]\n",
      "\"image generation\": 2557it [01:36, 26.56it/s]\n",
      "\"object detection\": 6000it [03:26, 29.04it/s]\n",
      "\"transfer learning\": 5623it [03:09, 29.65it/s]\n",
      "\"transformers\": 6000it [03:26, 29.00it/s]\n",
      "\"adversarial training\": 2837it [01:16, 37.26it/s]\n",
      "\"generative adversarial networks\": 5984it [03:38, 27.44it/s]\n",
      "\"model compression\": 801it [00:21, 36.57it/s]\n",
      "\"few-shot learning\": 0it [00:04, ?it/s]\n",
      "\"natural language processing\": 6000it [03:38, 27.49it/s]\n",
      "\"graph neural networks\": 5074it [04:20, 19.45it/s]\n",
      "\"colorization\": 6000it [03:25, 29.18it/s]\n",
      "\"depth estimation\": 1388it [00:45, 30.43it/s]\n",
      "\"point cloud\": 5052it [02:32, 33.10it/s]\n",
      "\"structured data\": 2104it [01:19, 26.62it/s]\n",
      "\"optical flow\": 1651it [00:55, 29.82it/s]\n",
      "\"reinforcement learning\": 6000it [03:02, 32.89it/s]\n",
      "\"super resolution\": 3228it [01:34, 34.02it/s]\n",
      "\"attention mechanisms\": 5488it [02:28, 36.83it/s]\n",
      "\"tabular data\": 614it [00:15, 39.39it/s]\n",
      "\"unsupervised learning\": 2945it [01:21, 36.06it/s]\n",
      "\"semi-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"explainable AI\": 794it [00:26, 29.46it/s]\n",
      "\"radiance field\": 730it [00:19, 36.63it/s]\n",
      "\"decision tree\": 2711it [01:37, 27.93it/s]\n",
      "\"time series analysis\": 1107it [00:30, 36.41it/s]\n",
      "\"molecule generation\": 147it [00:07, 19.96it/s]\n",
      "\"large language models\": 2935it [01:24, 34.56it/s]\n",
      "\"LLMs\": 1938it [00:51, 37.60it/s]\n",
      "\"language models\": 6000it [02:45, 36.20it/s]\n",
      "\"image classification\": 6000it [02:35, 38.56it/s]\n",
      "\"document image classification\": 20it [00:03,  5.24it/s]\n",
      "\"encoder-decoder\": 0it [00:03, ?it/s]\n",
      "\"multimodal learning\": 283it [00:11, 25.60it/s]\n",
      "\"multimodal deep learning\": 85it [00:05, 14.91it/s]\n",
      "\"speech recognition\": 4212it [02:13, 31.53it/s]\n",
      "\"generative models\": 6000it [03:49, 26.10it/s]\n",
      "\"anomaly detection\": 3354it [01:42, 32.86it/s]\n",
      "\"recommender systems\": 4531it [02:08, 35.25it/s]\n",
      "\"robotics\": 6000it [02:18, 43.22it/s]\n",
      "\"knowledge graphs\": 2843it [01:21, 34.85it/s]\n",
      "\"cross-modal learning\": 0it [00:03, ?it/s]\n",
      "\"attention mechanisms\": 5488it [02:47, 32.84it/s]\n",
      "\"unsupervised translation\": 34it [00:04,  8.07it/s]\n",
      "\"machine translation\": 4078it [02:04, 32.77it/s]\n",
      "\"dialogue systems\": 1047it [00:31, 32.94it/s]\n",
      "\"sentiment analysis\": 2114it [00:59, 35.28it/s]\n",
      "\"question answering\": 4470it [02:16, 32.86it/s]\n",
      "\"text summarization\": 572it [00:19, 29.31it/s]\n",
      "\"sequential modeling\": 354it [00:07, 48.54it/s]\n",
      "\"neurosymbolic AI\": 13it [00:03,  3.42it/s]\n",
      "\"fairness in AI\": 31it [00:04,  7.35it/s]\n",
      "\"transferable skills\": 41it [00:04, 10.23it/s]\n",
      "\"data augmentation\": 5482it [02:39, 34.27it/s]\n",
      "\"neural architecture search\": 1298it [00:41, 31.13it/s]\n",
      "\"active learning\": 2404it [01:15, 32.02it/s]\n",
      "\"automated machine learning\": 323it [00:11, 27.33it/s]\n",
      "\"meta-learning\": 30it [00:03,  7.69it/s]\n",
      "\"domain adaptation\": 4096it [02:02, 33.52it/s]\n",
      "\"time series forecasting\": 760it [00:17, 42.30it/s]\n",
      "\"weakly supervised learning\": 424it [00:15, 27.81it/s]\n",
      "\"self-supervised vision\": 0it [00:03, ?it/s]\n",
      "\"visual reasoning\": 234it [00:09, 24.28it/s]\n",
      "\"knowledge distillation\": 2160it [00:55, 38.70it/s]\n",
      "\"hyperparameter optimization\": 516it [00:13, 38.43it/s]\n",
      "\"cross-validation\": 6it [00:03,  1.59it/s]\n",
      "\"explainable reinforcement learning\": 34it [00:04,  7.50it/s]\n",
      "\"meta-reinforcement learning\": 0it [00:03, ?it/s]\n",
      "\"generative models in NLP\": 0it [00:03, ?it/s]\n",
      "\"knowledge representation and reasoning\": 169it [00:07, 23.41it/s]\n",
      "\"zero-shot learning\": 0it [00:03, ?it/s]\n",
      "\"self-attention mechanisms\": 0it [00:03, ?it/s]\n",
      "\"ensemble learning\": 821it [00:25, 32.68it/s]\n",
      "\"online learning\": 2282it [00:51, 44.16it/s]\n",
      "\"cognitive computing\": 382it [00:07, 50.41it/s]\n",
      "\"self-driving cars\": 0it [00:03, ?it/s]\n",
      "\"emerging AI trends\": 0it [00:03, ?it/s]\n",
      "\"Attention is all you need\": 14it [00:03,  3.63it/s]\n",
      "\"GPT\": 604it [00:15, 37.95it/s]\n",
      "\"BERT\": 5287it [02:22, 37.20it/s]\n",
      "\"Transformers\": 6000it [02:46, 36.06it/s]\n",
      "\"yolo\": 393it [00:13, 30.18it/s]\n",
      "\"speech recognisation\": 8it [00:03,  2.09it/s]\n",
      "\"LSTM\": 5262it [02:33, 34.38it/s]\n",
      "\"GRU\": 853it [00:25, 33.65it/s]\n"
     ]
    }
   ],
   "source": [
    "all_titles = []\n",
    "all_abstracts = []\n",
    "all_terms = []\n",
    "all_urls = []\n",
    "all_ids = []\n",
    "\n",
    "for query in query_keywords:\n",
    "    terms, titles, abstracts, urls , ids = query_with_keywords(query)\n",
    "    all_titles.extend(titles)\n",
    "    all_abstracts.extend(abstracts)\n",
    "    all_terms.extend(terms)\n",
    "    all_urls.extend(urls)\n",
    "    all_ids.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a502f2c-1a78-4e76-a10b-36dd1bca424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210.06475v2\n"
     ]
    }
   ],
   "source": [
    "print(urls[50].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee55de48-52d6-4096-8c9f-d2e0d6a2d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2305.16165v2\n"
     ]
    }
   ],
   "source": [
    "print(ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b7d0e1-1c11-4e2f-8d0f-3cddadb5a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs.LG', 'cs.CY']\n"
     ]
    }
   ],
   "source": [
    "print(terms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ab9f65-abf4-4015-a9fd-5cf1bfa4b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing\n"
     ]
    }
   ],
   "source": [
    "print(titles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bd5f9-e059-4ff5-8dc1-368e30cad5ec",
   "metadata": {},
   "source": [
    "### lets see the data scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7675dbd0-9ead-45e2-8da2-315c5013509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.DataFrame({\n",
    "    'titles': all_titles,\n",
    "    'abstracts': all_abstracts,\n",
    "    'terms': all_terms,\n",
    "    'urls': all_urls,\n",
    "    'ids':all_ids,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274eda83-16c6-427a-8f1c-9fbfb0702db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAE-Former: Dual Attention-guided Efficient Tr...</td>\n",
       "      <td>Transformers have recently gained attention in...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2212.13504v3</td>\n",
       "      <td>2212.13504v3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multi-modal Learning with Missing Modality via...</td>\n",
       "      <td>The missing modality issue is critical but non...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2307.14126v1</td>\n",
       "      <td>2307.14126v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unite-Divide-Unite: Joint Boosting Trunk and S...</td>\n",
       "      <td>High-accuracy Dichotomous Image Segmentation (...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2307.14052v1</td>\n",
       "      <td>2307.14052v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDViT: Multi-domain Vision Transformer for Sma...</td>\n",
       "      <td>Despite its clinical utility, medical image se...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2307.02100v2</td>\n",
       "      <td>2307.02100v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning Transferable Object-Centric Diffeomor...</td>\n",
       "      <td>Obtaining labelled data in medical image segme...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2307.13645v1</td>\n",
       "      <td>2307.13645v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93800</th>\n",
       "      <td>Ask the GRU: Multi-Task Learning for Deep Text...</td>\n",
       "      <td>In a variety of application domains the conten...</td>\n",
       "      <td>[stat.ML, cs.CL, cs.LG, I.2.7; I.2.6]</td>\n",
       "      <td>http://arxiv.org/abs/1609.02116v2</td>\n",
       "      <td>1609.02116v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93801</th>\n",
       "      <td>Faster Training of Very Deep Networks Via p-No...</td>\n",
       "      <td>A major contributing factor to the recent adva...</td>\n",
       "      <td>[stat.ML, cs.LG, cs.NE]</td>\n",
       "      <td>http://arxiv.org/abs/1608.03639v1</td>\n",
       "      <td>1608.03639v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93802</th>\n",
       "      <td>Drawing and Recognizing Chinese Characters wit...</td>\n",
       "      <td>Recent deep learning based approaches have ach...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/1606.06539v1</td>\n",
       "      <td>1606.06539v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93803</th>\n",
       "      <td>Delving Deeper into Convolutional Networks for...</td>\n",
       "      <td>We propose an approach to learn spatio-tempora...</td>\n",
       "      <td>[cs.CV, cs.LG, cs.NE]</td>\n",
       "      <td>http://arxiv.org/abs/1511.06432v4</td>\n",
       "      <td>1511.06432v4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93804</th>\n",
       "      <td>Image Question Answering using Convolutional N...</td>\n",
       "      <td>We tackle image question answering (ImageQA) p...</td>\n",
       "      <td>[cs.CV, cs.CL, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/1511.05756v1</td>\n",
       "      <td>1511.05756v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93805 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles  \\\n",
       "0      DAE-Former: Dual Attention-guided Efficient Tr...   \n",
       "1      Multi-modal Learning with Missing Modality via...   \n",
       "2      Unite-Divide-Unite: Joint Boosting Trunk and S...   \n",
       "3      MDViT: Multi-domain Vision Transformer for Sma...   \n",
       "4      Learning Transferable Object-Centric Diffeomor...   \n",
       "...                                                  ...   \n",
       "93800  Ask the GRU: Multi-Task Learning for Deep Text...   \n",
       "93801  Faster Training of Very Deep Networks Via p-No...   \n",
       "93802  Drawing and Recognizing Chinese Characters wit...   \n",
       "93803  Delving Deeper into Convolutional Networks for...   \n",
       "93804  Image Question Answering using Convolutional N...   \n",
       "\n",
       "                                               abstracts  \\\n",
       "0      Transformers have recently gained attention in...   \n",
       "1      The missing modality issue is critical but non...   \n",
       "2      High-accuracy Dichotomous Image Segmentation (...   \n",
       "3      Despite its clinical utility, medical image se...   \n",
       "4      Obtaining labelled data in medical image segme...   \n",
       "...                                                  ...   \n",
       "93800  In a variety of application domains the conten...   \n",
       "93801  A major contributing factor to the recent adva...   \n",
       "93802  Recent deep learning based approaches have ach...   \n",
       "93803  We propose an approach to learn spatio-tempora...   \n",
       "93804  We tackle image question answering (ImageQA) p...   \n",
       "\n",
       "                                       terms  \\\n",
       "0                                    [cs.CV]   \n",
       "1                                    [cs.CV]   \n",
       "2                                    [cs.CV]   \n",
       "3                                    [cs.CV]   \n",
       "4                                    [cs.CV]   \n",
       "...                                      ...   \n",
       "93800  [stat.ML, cs.CL, cs.LG, I.2.7; I.2.6]   \n",
       "93801                [stat.ML, cs.LG, cs.NE]   \n",
       "93802                                [cs.CV]   \n",
       "93803                  [cs.CV, cs.LG, cs.NE]   \n",
       "93804                  [cs.CV, cs.CL, cs.LG]   \n",
       "\n",
       "                                    urls           ids  \n",
       "0      http://arxiv.org/abs/2212.13504v3  2212.13504v3  \n",
       "1      http://arxiv.org/abs/2307.14126v1  2307.14126v1  \n",
       "2      http://arxiv.org/abs/2307.14052v1  2307.14052v1  \n",
       "3      http://arxiv.org/abs/2307.02100v2  2307.02100v2  \n",
       "4      http://arxiv.org/abs/2307.13645v1  2307.13645v1  \n",
       "...                                  ...           ...  \n",
       "93800  http://arxiv.org/abs/1609.02116v2  1609.02116v2  \n",
       "93801  http://arxiv.org/abs/1608.03639v1  1608.03639v1  \n",
       "93802  http://arxiv.org/abs/1606.06539v1  1606.06539v1  \n",
       "93803  http://arxiv.org/abs/1511.06432v4  1511.06432v4  \n",
       "93804  http://arxiv.org/abs/1511.05756v1  1511.05756v1  \n",
       "\n",
       "[93805 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35539db-ad56-4896-b492-f97550bb0ea8",
   "metadata": {},
   "source": [
    "### Save the data - Finally, we export the DataFrame to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b73a52b-a87b-4006-a6d0-b257f8052b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.to_csv(PATH_DATA_BASE / 'data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
