{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb470e-2cd9-47be-8bac-e9f404d24f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccc036-4ac0-409c-b1f8-ace300a02d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa9f4c-95fa-4ffd-9d7d-963c7e0c5680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185014c5-6be6-463e-97fe-387af8b67374",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xmltodict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxmltodict\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringPipeline\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xmltodict'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import xmltodict\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from transformers.pipelines.question_answering import QuestionAnsweringPipeline\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "QA_MODEL_NAME = \"ixa-ehu/SciBERT-SQuAD-QuAC\"\n",
    "TEMP_PDF_PATH = \"/tmp/arxiv_paper.pdf\"\n",
    "\n",
    "@dataclass\n",
    "class PaperMetaData:\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    text: str\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_field(text: str) -> str:\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, paper_id: str, title: str, abstract: str) -> PaperMetaData:\n",
    "        return PaperMetaData(\n",
    "            paper_id=paper_id,\n",
    "            title=cls._clean_field(title),\n",
    "            abstract=cls._clean_field(abstract),\n",
    "            text=\"\",  # Placeholder for text, will be filled later\n",
    "        )\n",
    "\n",
    "class ScrapedPaper:\n",
    "    def __init__(self, paper_id: str, title: str, abstract: str, url: str):\n",
    "        self.paper_id = paper_id\n",
    "        self.title = title\n",
    "        self.abstract = abstract\n",
    "        self.url = url\n",
    "\n",
    "    def _download_pdf(self, download_path: str) -> None:\n",
    "        pdf_r = requests.get(self.url)\n",
    "        pdf_r.raise_for_status()\n",
    "        with open(download_path, \"wb\") as pdf_file:\n",
    "            pdf_file.write(pdf_r.content)\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        self._download_pdf(TEMP_PDF_PATH)\n",
    "        reader = PdfReader(TEMP_PDF_PATH)\n",
    "        pdf_text = \" \".join([page.extract_text() for page in reader.pages])\n",
    "        return pdf_text\n",
    "\n",
    "    def get_paper_full_data(self) -> PaperMetaData:\n",
    "        text = self.extract_text()\n",
    "        return PaperMetaData.from_dataset(paper_id=self.paper_id, title=self.title, abstract=self.abstract, text=text)\n",
    "\n",
    "def load_dataset(file_path: str) -> List[ScrapedPaper]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    papers = []\n",
    "    for _, row in df.iterrows():\n",
    "        paper = ScrapedPaper(paper_id=row['paper_id'], title=row['title'], abstract=row['abstract'], url=row['url'])\n",
    "        papers.append(paper)\n",
    "    return papers\n",
    "\n",
    "def get_paper_data(papers: List[ScrapedPaper], paper_id: str) -> Tuple[str, str, str]:\n",
    "    for paper in papers:\n",
    "        if paper.paper_id == paper_id:\n",
    "            return paper.title, paper.abstract, paper.extract_text()\n",
    "    return \"\", \"\", \"\"\n",
    "\n",
    "def get_qa_pipeline(qa_model_name: str = QA_MODEL_NAME) -> QuestionAnsweringPipeline:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "    return qa_pipeline\n",
    "\n",
    "def get_answer(question: str, context: str) -> str:\n",
    "    qa_pipeline = get_qa_pipeline()\n",
    "    prediction = qa_pipeline(question=question, context=context)\n",
    "    return prediction[\"answer\"]\n",
    "\n",
    "# Load the dataset\n",
    "dataset_file_path = \"your_dataset.csv\"  # Replace with the actual file path\n",
    "papers = load_dataset(dataset_file_path)\n",
    "\n",
    "# Example usage\n",
    "paper_id = \"your_paper_id_here\"\n",
    "title, abstract, text = get_paper_data(papers, paper_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
