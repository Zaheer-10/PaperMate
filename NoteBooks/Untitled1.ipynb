{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63757b6d-dd41-4802-93e5-18fa40e24176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cb50ba-0431-40dd-a786-3b2b12956b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ython-crfsuite (c:\\users\\soulo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d096761c-a63a-4976-a87a-b1a720148337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown utils\n",
    "from transformers.utils.logging import set_verbosity\n",
    "\n",
    "set_verbosity(40)\n",
    "\n",
    "import warnings\n",
    "# ignore hf pipeline complaints\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf45013-387d-4fd4-a738-ac50e8d56912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b511aee1c7498b9e8d4e064cefedf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soulo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\soulo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dd638b176f4cb283a105df878a2db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    \"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a0341f-e6ff-4f8e-a6f4-294af4de4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_length\": 256,\n",
    "    \"min_length\": 8,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    \"repetition_penalty\": 3.5,\n",
    "    \"length_penalty\": 0.3,\n",
    "    \"encoder_no_repeat_ngram_size\": 3,\n",
    "    \"num_beams\": 4,\n",
    "} # parameters for text generation out of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e8c7fd-c949-4c1a-a4ce-a34164bc67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Recommender systems are widely used these days in e-commerce, for the purpose of personalized\n",
    "recommendation. Based on each user’s profile, previous purchase history, and online behavior, they\n",
    "suggest products which they are likely to prefer. For example, Amazon.com is using recommender\n",
    "systems for books. When a user logs-in to the system, it suggests books similar to previously bought\n",
    "ones by the user.\n",
    "Personalized recommendation can be applied to outside of commercial applications. These days,\n",
    "many academic papers are coming out from a lot of conferences and journals. Academic researchers\n",
    "should go through all the conferences and journals which are related to their field of research and\n",
    "find out if there is any new articles that may relate to their current works. Sometimes they search\n",
    "the articles from Google scholars or Citeseer with the key words that might show interesting articles\n",
    "to them. However, these two methods require users to commit their time to search articles, which is\n",
    "labor-intensive, and also do not guarantee that they will find the exact articles related to their field\n",
    "of research.\n",
    "In order to reduce their workload, we suggest developing the scholarly paper recommendation system for academic researchers, which will automatically detect their research topics they are interested in and recommend the related articles they may be interested in based on similarity of the\n",
    "works. We believe this system will save the researchers’ time to search the articles and increase the\n",
    "accuracy of finding the articles they are interested in.\n",
    "In this section we briefly present some of the research literature related to recommender systems in\n",
    "general, academic paper recommendation system, and evaluation of recommender systems.\n",
    "Recommender systems are broadly classified into three categories[7]: collaborative filtering,\n",
    "contents-based methods, and hybrid methods. First, collaborative filtering uses only user-item rating\n",
    "matrix for predicting unseen preference[21, 1]. It can be categorized into memory-based CF, which\n",
    "contains the whole matrix on memory, and model-based CF, building a model for estimation[2].\n",
    "The most effective memory-based algorithms known so far is item-based CF[19]. Recently, making use of matrix factorization, a kind of model-based approach[14, 16, 18, 3, 24], is known as\n",
    "the most efficient and accurate, especially after those approaches won the Netflix prize in 2009.\n",
    "Content-based methods, on the other hand, recommend items based on their characteristics as well\n",
    "as specific preferences of a user[7]. Pazzani[15] studied this approach in depth, including how to\n",
    "build user and item profiles. Last category, hybrid approach, tries to combine both collaborative and\n",
    "content-based recommendation. Koren[8] suggested effectively combining rating information and\n",
    "user, item profiles for more accurate recommendation.\n",
    "Recommender systems have concentrated on recommending media items such as movies, but recently they are extending to academy. Most popular application is citation recommendation[5, 12,\n",
    "23, 20]. Recently, Matsatsinis [11] introduced scientific paper recommendation using decision theory. Sugiyama[22] extended scholarly paper recommendation with citation and reference information.\n",
    "Although recommender systems are very popular in commercial applications these days, it is still\n",
    "difficult to evaluate them due to the lack of standard methods. Traditional recommender systems [4, 17, 6] were usually introduced in Human-Computer Interaction community, so they have\n",
    "been evaluated by user study. This approach is still used, especially for verifying improvement in\n",
    "terms of user experience.\n",
    "3.2 Data Model\n",
    "3.2.1 Bag-of-word model\n",
    "With the gathered data, we modeled them by a bag-of-word model. In this model, each word appeared in the whole document corpora becomes an attribute. Then, each document is represented by\n",
    "a bit vector, indicating whether each word appears or not. This model is based on two assumptions;\n",
    "1) word probabilities for one text position are independent of the words that occur in other positions\n",
    "(Naive Bayes Assumption) and 2) the probability of encountering a specific word is independent of\n",
    "its position. (Independent Identical Distribution Assumption) This assumption is incorrect, but it is\n",
    "known that this does not seriously affect classification or learning task. [13] We combined title, key\n",
    "words, and abstract to construct a set of words representing a paper.\n",
    "3.2.2 Heuristics\n",
    "For more efficient processing, we applied some heuristics. First, we removed stop words such as\n",
    "”the” or ”of”. These words appear in almost every document in English, so they are not useful for\n",
    "classifying or filtering some specific documents, but just slow down computation speed by increasing\n",
    "the text length. We removed about 140 words which were selected manually. This process reduces\n",
    "the length of dictionary, resulting in reduced dimension of the clustering work, so we expect speed\n",
    "improvement.\n",
    "The other heuristic applied is stemming. In English, same word can be used as different parts, usually in a slightly different form. For example, ”clear”, ”clearly”, and ”cleared” have same meaning,\n",
    "but used in different forms for its position or role in the sentence. It is much better to deal with these\n",
    "minor changes of forms as same words, as it can dramatically reduce the dimension. However, this\n",
    "work is not straightforward. As a first step, we just removed last ”ed”, ”ly”, and ”ing” from the\n",
    "word, whenever encountered.\n",
    "3.3 Learner (Recommender)\n",
    "Using the crawled documents and data model discussed so far, we are ready to proceed to our main\n",
    "goal: personalized recommendation of academic papers. As a perspective of recommendation system, we can consider authors as users and papers as items. We will use these terms interchangeably\n",
    "henceforth. We can think of recommendation system as a task to fill out missing preference data on\n",
    "a user-item matrix, based on observed values. There can be lots of schemes to decide proper values\n",
    "for missing preference. Filling with the user’s average or item’s average can be a simple baseline. In\n",
    "this section, we discuss fundamental characteristics of our problem, and then describe our algorithm.\n",
    "3.3.1 Inherent Characteristics of Problem\n",
    "The information we gather contains each paper’s title, list of authors, key words, and abstract. In\n",
    "order to build a user-item matrix with this data, we assume that users are interested in their own\n",
    "papers. Thus, we set high score (in this paper, 5) to every <author, paper> pair that the paper is\n",
    "written by the author. We use 1-5 scale as it is widely used in recommendation systems in literature.\n",
    "We claim that this user-item matrix we use is extremely sparse, which means most of values are\n",
    "missing while only small portion of them are observed. This situation is common in recommendation, though. According to Netflix Prize data, only 1% of cells of the user-item matrix are observed\n",
    "3\n",
    "values. Nonetheless, it has been shown that it is possible to accurately estimate missing data only\n",
    "using small amount of observed data. In our situation, however, the sparsity can be worse. Regularly, one author writes only one or two papers in one conference proceeding. There are only at\n",
    "most two or three top-level conferences in each field, the maximum number of papers one author\n",
    "can publish a year is about 10. This is an ideal case, and most researchers may have only one or two\n",
    "papers. Thus, our matrix have only a few number of preference data.\n",
    "More serious problem is that we do not have ”dislike” information. When we request users to\n",
    "explicitly rate items in a common recommendation system, we can get both positive and negative\n",
    "feedback from the user. For example, we can get ”very like” feedback for the movie ”Titanic” as\n",
    "well as ”very hate” one for the ”Shrek 2.” Based on this variety, we can infer that the user may\n",
    "prefer romantic movies to animations. In our data, however, we do not have negative feedback. This\n",
    "problem makes difficult us to use widely-used collaborative filtering algorithms.\n",
    "3.3.2 Naive Recommender\n",
    "We basically assume that authors will like papers similar to ones they wrote before. In this context,\n",
    "we note that similar papers mean ones dealing with similar topic. In our Naive Recommender,\n",
    "we just apply this assumption. When we try to recommend a set of papers to a specific user, we\n",
    "first calculate similarity between every paper and the user’s own papers. Then, we take the highest\n",
    "similarity as the score of that paper. This process is similar to k-Nearest Neighbors (kNN) algorithm.\n",
    "That is, we can easily select and recommend most similar n papers to the target user’s previous paper.\n",
    "We used vector cosine of our data model (bit vector) as the similarity measure.\n",
    "However, the real situation is a little bit more complicated, as the user may have written more than\n",
    "one paper. It is still kNN, but we can have more than one queried point. Thus, we applied clustering\n",
    "first. All candidate papers are assigned to only one of the most similar paper written by the target\n",
    "user. This process is similar to K-means, but the centroids are also papers, so their geometric location\n",
    "in the space cannot change. Thus, we do not need to iterate in our case. After assigned to a cluster,\n",
    "the score is calculated based on the distance between the candidate paper and its centroid. For\n",
    "example, as shown in Figure 2, each big circle represents a centroid of a cluster and small circles\n",
    "connected to the centroid are members of its cluster. Using the calculated score as a distance metric\n",
    "for kNN, we select k papers for recommendation to the target user.\n",
    "interested in more accurately. Also, through the focus group interview we discovered the interesting\n",
    "fact that even though the topics are not as much as relevant to their research topic, they showed great\n",
    "interest to the papers that their peer researchers, i.e., their former students or the researchers they\n",
    "have done research together before, wrote. In this way, it will be important to include the information about relevant researchers to users and recommend papers that they found interesting or they\n",
    "have wrote. Also, the subjects replied, if we provide information about which researcher liked this\n",
    "papers, it would also give them great reason and motivation to read that paper.\n",
    "For the perspective of machine learning, we may need to consider about scalability. Although our\n",
    "current system runs within a few minutes, it may take more time when we crawl more data. First, we\n",
    "can improve accuracy of similarity measure by allowing counting the frequency of each word in a\n",
    "document, instead of bit vector model. TF-IDF model [10] can be a great candidate to implement. In\n",
    "this model, we give more weight for frequently used words in a specific document, but not in other\n",
    "ones. Also, we may need to speed up the calculation. For this, dimension reduction will be helpful.\n",
    "Specifically, it would be better to add more stemming logic because this can deal with more words\n",
    "as same ones, so we can successfully reduce dimension. We may use L-Distance algorithm [9] for\n",
    "calculating similarity of each word pair, and decide whether they are same or not.\n",
    " Conclusion\n",
    "In this paper, we have presented a Personalized Academic Research Paper Recommendation System, which recommends related articles for each researcher. Thanks to our system, researchers can\n",
    "get their related papers without searching keywords on Google or browsing top conferences’ proceedings. Our system makes three contributions. First, we have developed a web crawler to retrieve\n",
    "a huge number of research papers from the web. Second, we define a similarity measure for research\n",
    "papers. Third, we have developed our recommender system using collaboration filtering methods.\n",
    "Evaluation results show the usefulness of our system.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a67d8d6-7f8b-4c68-a0a3-fd1804727807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'This paper introduces a new recommendation system that uses machine learning to predict which articles are most likely to be useful for academics. It uses a combination of crowd-mapping, clustering, and sentimental analysis to develop a recommendation system. The main goal of the proposed system is to automatically find papers that relate to the interests of each individual.'}]\n",
      "CPU times: total: 5min 48s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = summarizer(input_text, **params)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b22039c-3ea1-45c7-ab4e-59d89e74f9ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'This paper introduces a new recommendation system that uses machine learning to predict which articles are most likely to be useful for academics. It uses a combination of crowd-mapping, clustering, and sentimental analysis to develop a recommendation system. The main goal of the proposed system is to automatically find papers that relate to the interests of each individual.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate dynamic max_length as a proportion of input_length\n",
    "input_length = len(input_text)\n",
    "proportion = 0.6  # You can adjust this value\n",
    "dynamic_max_length = int(input_length * proportion)\n",
    "\n",
    "params = {\n",
    "    \"max_length\": 512,\n",
    "    \"min_length\": 8,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    \"repetition_penalty\": 3.5,\n",
    "    \"length_penalty\": 0.3,\n",
    "    \"encoder_no_repeat_ngram_size\": 3,\n",
    "    \"num_beams\": 4,\n",
    "} # parameters for text generation out of model\n",
    "\n",
    "\n",
    "# Perform summarization with dynamic max_length\n",
    "result = summarizer(input_text, **params)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e91ac98-0643-4e8e-aa45-920aaec19962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, we discuss the use of recommendation systems to help customers find the right books and articles for their needs. In this paper, we present a recommendation system that will help academics find the most relevant articles for their fields of study. In this paper, we discuss the different types of recommendation approaches that can be used to help us make better decisions about what to buy and how to purchase it. Some of the most effective approaches are described in this paper. The first is a \"memory-based\" approach, which uses memory to build a prediction model. The second is an \"item-based,\" or \"collaborative,\" approach. These approaches work best because they combine both information about the item and its characteristics. In this paper, we present a new recommendation system that uses machine learning to predict the likelihood of each word appearing in a scientific paper. In this paper, we explore the use of a combination of key words and abstracts to build a dictionary. We first remove words that are commonly used in English such as the word \"the\" or \"of.\" This will speed up the process of clustering by reducing the amount of text that needs to be written. In this chapter, the learner is introduced to the concept of stemmering. It's used in English to mean different things at different speeds. The narrator uses an example of a sentence where the word \"clear\" is used for two different purposes: as one part of the sentence and as another part of it. This works fine, but we still need to figure out how to make sense of these different kinds of words. This paper uses a combination of machine learning and natural language understanding to develop a recommendation system. The main problem is to find the most important values for each item in a given paper. In this paper, we discuss the problem of missing information. For one author, it is impossible to know how many papers he has written in one year. This means that there are only few preference-sensitive items in our model. Furthermore, we don't have enough information to distinguish between \"very like\" and \"very hate\" items. This paper introduces a new recommendation method, which uses clustering to rank the best papers according to similarity. In this paper, we present a new approach to assigning papers to different audiences by calculating a score. This new approach takes advantage of the fact that papers tend to be more closely related to their peers than they are to themselves. In this paper, we discuss how we can use machine learning to learn about people and papers that are relevant to them. For example, we want to be able to track which of the papers are most popular amongst our readers. This paper presents a novel approach to predicting similarity between two words. It uses a combination of machine learning and natural language understanding to predict word pairs that are similar or different. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the input text into chunks of a manageable length\n",
    "chunk_size = 1000\n",
    "chunks = [input_text[i:i+chunk_size] for i in range(0, len(input_text), chunk_size)]\n",
    "    \n",
    "# Generate summaries for each chunk and combine them\n",
    "combined_summary = \"\"\n",
    "for chunk in chunks:\n",
    "    summary = summarizer(chunk, **params)\n",
    "    combined_summary += summary[0]['summary_text'] + \" \"\n",
    "print(combined_summary)\n",
    "len(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035122d8-0a6e-4c19-8b2a-ec6b3fa0030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 2919\n",
      "2. 1\n",
      "2. 1\n"
     ]
    }
   ],
   "source": [
    "print(\"1.\",len(combined_summary))\n",
    "print(\"2.\",len(result))\n",
    "print(\"2.\",len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "244647b2-4ee8-4a31-916b-92ef7f5607cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000002063BE19510>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\soulo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\soulo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py\", line 276, in close\n",
      "    def close(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m combined_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m----> 8\u001b[0m     summary \u001b[38;5;241m=\u001b[39m summarizer(chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m      9\u001b[0m     combined_summary \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m summary[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_summary)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:265\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m          ids of the summary.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[0;32m    170\u001b[0m     ):\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1122\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1115\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1116\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         )\n\u001b[0;32m   1120\u001b[0m     )\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1129\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1128\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1129\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1130\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1028\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1027\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1028\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1029\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m], generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 187\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    188\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1627\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1620\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1621\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1622\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1623\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1625\u001b[0m     )\n\u001b[0;32m   1626\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[0;32m   1628\u001b[0m         input_ids,\n\u001b[0;32m   1629\u001b[0m         beam_scorer,\n\u001b[0;32m   1630\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1631\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1632\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1633\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1634\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1635\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1636\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1637\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1638\u001b[0m     )\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1641\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1642\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:3005\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3001\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3002\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[0;32m   3003\u001b[0m )\n\u001b[0;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3005\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_key_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[0;32m   3008\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\longt5\\modeling_longt5.py:2139\u001b[0m, in \u001b[0;36mLongT5ForConditionalGeneration._reorder_cache\u001b[1;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[0;32m   2135\u001b[0m reordered_layer_past_states \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past_state \u001b[38;5;129;01min\u001b[39;00m layer_past_states:\n\u001b[0;32m   2137\u001b[0m     \u001b[38;5;66;03m# need to set correct `past` for each of the four key / value states\u001b[39;00m\n\u001b[0;32m   2138\u001b[0m     reordered_layer_past_states \u001b[38;5;241m=\u001b[39m reordered_layer_past_states \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m-> 2139\u001b[0m         \u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2140\u001b[0m     )\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m reordered_layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reordered_layer_past_states) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(layer_past_states)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the input text into chunks of a manageable length\n",
    "chunk_size = 4\n",
    "chunks = [input_text[i:i+chunk_size] for i in range(0, len(input_text), chunk_size)]\n",
    "    \n",
    "# Generate summaries for each chunk and combine them\n",
    "combined_summary = \"\"\n",
    "for chunk in chunks:\n",
    "    summary = summarizer(chunk, **params)\n",
    "    combined_summary += summary[0]['summary_text'] + \" \"\n",
    "print(combined_summary)\n",
    "len(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d3d8ab0-5da7-40f4-b8c8-03b1a3ba2c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, Wuthering Heights discusses the use of recommendation systems for recommending books to customers. He uses an example of an e commerce system that recommends books based upon past purchases and previous behavior. This paper also discusses how academic journals and other sources of information can be used to predict which articles will be most useful for students. In this paper, Wuthering Heights uses a combination of machine learning and natural language understanding to develop a recommendation system for academic papers In this chapter, we describe the problem of predicting whether a given paper will be useful for a particular audience. The goal of our model is to find out how many papers each author has written in one year. This information can be used to predict which papers will be most popular with the target audience. We then use a similarity-separation technique to find the most similar papers to each other. This paper presents a novel recommendation system that uses clustering to track the similarity between papers written by different academics. The results show that although the topics of the papers are not very relevant to the actual research being done, they do show great interest in the papers which their peer scholars have written. In order to make the system more scalable, we will need to improve the quality of the score. For instance, we could use a \"TF-Idf\" model to reduce the weight of words frequently used in one word pair. Finally, we develop a recommender for each scholar using a combination of crowd-resource methods. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1581"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split the input text into chunks of a manageable length\n",
    "chunk_size = 3000\n",
    "chunks = [input_text[i:i+chunk_size] for i in range(0, len(input_text), chunk_size)]\n",
    "    \n",
    "# Generate summaries for each chunk and combine them\n",
    "combined_summary = \"\"\n",
    "for chunk in chunks:\n",
    "    summary = summarizer(chunk, **params)\n",
    "    combined_summary += summary[0]['summary_text'] + \" \"\n",
    "print(combined_summary)\n",
    "len(combined_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
